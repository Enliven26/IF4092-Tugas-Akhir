{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "load_dotenv(dotenv_path=\".env.test\", verbose=True, override=True)\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autocommit_evaluation.core.enums import EnvironmentKey\n",
    "from autocommit_evaluation.cmg.evaluators import CommitMessageGenerator\n",
    "from autocommit_evaluation.cmg import evaluator\n",
    "from autocommit_evaluation.core import (\n",
    "    openrouter_few_shot_high_level_context_cmg_chain,\n",
    "    openrouter_low_level_context_cmg_chain,\n",
    "    openrouter_zero_shot_high_level_context_cmg_chain,\n",
    "    openrouter_high_level_context_chain\n",
    ")\n",
    "from autocommit.core.models import CommitDataModel\n",
    "from autocommit_evaluation.datapreparation import context_generator, example_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMIT_DATA_JSON_FILE_PATH = os.path.join(\"autocommit_evaluation\", \"data\", \"cmg\", \"commits.test.json\")\n",
    "CONTEXT_DATA_PATH = os.path.join(\"autocommit_evaluation\",\"data\", \"context\")\n",
    "\n",
    "DEFAULT_CONTEXT_GENERATION_OUTPUT_PATH = os.path.join(\n",
    "    \"autocommit_evaluation\", \"data\", \"context\"\n",
    ")\n",
    "DEFAULT_HIGH_LEVEL_CONTEXT_OUTPUT_PATH = os.path.join(\n",
    "    \"out\", \"test\", \"highlevelcontext\"\n",
    ")\n",
    "DEFAULT_CMG_OUTPUT_PATH = os.path.join(\"out\", \"test\", \"cmg\")\n",
    "DEFAULT_DIFF_CLASSIFICATION_OUTPUT_PATH = os.path.join(\n",
    "    \"out\", \"test\", \"diffclassification\"\n",
    ")\n",
    "\n",
    "DIFF_CLASSIFIER_CHAINS = [\n",
    "    openrouter_low_level_context_cmg_chain,\n",
    "    openrouter_zero_shot_high_level_context_cmg_chain\n",
    "]\n",
    "\n",
    "HIGH_LEVEL_CONTEXT_CHAINS = [\n",
    "    openrouter_high_level_context_chain,\n",
    "]\n",
    "\n",
    "GENERATORS = [\n",
    "    CommitMessageGenerator(\"OpenRouter Low-Level Context Generator\", openrouter_low_level_context_cmg_chain),\n",
    "    CommitMessageGenerator(\n",
    "        \"OpenRouter Zero-Shot High-Level Context Generator\", openrouter_zero_shot_high_level_context_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\n",
    "        \"OpenRouter Few-Shot High-Level Context Generator\", openrouter_few_shot_high_level_context_cmg_chain\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_GENERATION_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.CONTEXT_GENERATION_OUTPUT_PATH.value,\n",
    "        DEFAULT_CONTEXT_GENERATION_OUTPUT_PATH,\n",
    "    )\n",
    "\n",
    "HIGH_LEVEL_CONTEXT_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.HIGH_LEVEL_CONTEXT_OUTPUT_PATH.value,\n",
    "        DEFAULT_HIGH_LEVEL_CONTEXT_OUTPUT_PATH,\n",
    "    )\n",
    "\n",
    "CMG_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.CMG_OUTPUT_PATH.value, DEFAULT_CMG_OUTPUT_PATH\n",
    "    )\n",
    "\n",
    "DIFF_CLASSIFICATION_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.DIFF_CLASSIFICATION_OUTPUT_PATH.value,\n",
    "        DEFAULT_DIFF_CLASSIFICATION_OUTPUT_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commits(path: str) -> list[CommitDataModel]:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            json_string = file.read()\n",
    "\n",
    "        return CommitDataModel.from_json(json_string)\n",
    "\n",
    "COMMITS = get_commits(COMMIT_DATA_JSON_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDED_DIFF_CLASSIFIER_CHAIN_INDEXES = [0, 1]\n",
    "\n",
    "for index in INCLUDED_DIFF_CLASSIFIER_CHAIN_INDEXES:\n",
    "    evaluator.classify_diffs(DIFF_CLASSIFIER_CHAINS[index], COMMITS, CONTEXT_DATA_PATH, DIFF_CLASSIFICATION_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_generator.generate_context(COMMITS, CONTEXT_GENERATION_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get High Level Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDED_HIGH_LEVEL_CONTEXT_CHAIN_INDEXES = [0]\n",
    "\n",
    "for index in INCLUDED_HIGH_LEVEL_CONTEXT_CHAIN_INDEXES:\n",
    "    evaluator.get_high_level_contexts(\n",
    "        HIGH_LEVEL_CONTEXT_CHAINS[index],\n",
    "        COMMITS, \n",
    "        CONTEXT_DATA_PATH, \n",
    "        HIGH_LEVEL_CONTEXT_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Commit Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.smith.langchain.com:443\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Given a Git diff and the relevant source code, write a concise summary of the code changes in a way that a non-technical person can understand. The query text must summarize the code changes in two very brief sentences.\\n\\nGit diff:\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n\\nSource code:\\ncontrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java (Before)\\npublic class HiveStoragePlugin extends AbstractStoragePlugin {\\n@Override\\npublic Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n    switch(phase) {\\n        case LOGICAL:\\n            final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n            ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnScan(optimizerContext, defaultPartitionValue));\\n            return ruleBuilder.build();\\n        case PHYSICAL:\\n            {\\n                ruleBuilder = ImmutableSet.builder();\\n                OptionManager options = optimizerContext.getPlannerSettings().getOptions();\\n                // TODO: Remove implicit using of convert_fromTIMESTAMP_IMPALA function\\n                // once \"store.parquet.reader.int96_as_timestamp\" will be true by default\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_SCAN_WITH_NATIVE_READERS) || options.getBoolean(ExecConstants.HIVE_OPTIMIZE_PARQUET_SCAN_WITH_NATIVE_READER)) {\\n                    ruleBuilder.add(ConvertHiveParquetScanToDrillParquetScan.INSTANCE);\\n                }\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER)) {\\n                    try {\\n                        Class<?> hiveToDrillMapRDBJsonRuleClass = Class.forName(\"org.apache.drill.exec.planner.sql.logical.ConvertHiveMapRDBJsonScanToDrillMapRDBJsonScan\");\\n                        ruleBuilder.add((StoragePluginOptimizerRule) hiveToDrillMapRDBJsonRuleClass.getField(\"INSTANCE\").get(null));\\n                    } catch (ReflectiveOperationException e) {\\n                        logger.warn(\"Current Drill build is not designed for working with Hive MapR-DB tables. \" + \"Please disable {} option\", ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER);\\n                    }\\n                }\\n                return ruleBuilder.build();\\n            }\\n        default:\\n            return ImmutableSet.of();\\n    }\\n}\\n}\\n\\ncontrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java (After)\\npublic class HiveStoragePlugin extends AbstractStoragePlugin {\\n@Override\\npublic Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n    switch(phase) {\\n        case PARTITION_PRUNING:\\n            final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n            ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnScan(optimizerContext, defaultPartitionValue));\\n            return ruleBuilder.build();\\n        case PHYSICAL:\\n            {\\n                ruleBuilder = ImmutableSet.builder();\\n                OptionManager options = optimizerContext.getPlannerSettings().getOptions();\\n                // TODO: Remove implicit using of convert_fromTIMESTAMP_IMPALA function\\n                // once \"store.parquet.reader.int96_as_timestamp\" will be true by default\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_SCAN_WITH_NATIVE_READERS) || options.getBoolean(ExecConstants.HIVE_OPTIMIZE_PARQUET_SCAN_WITH_NATIVE_READER)) {\\n                    ruleBuilder.add(ConvertHiveParquetScanToDrillParquetScan.INSTANCE);\\n                }\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER)) {\\n                    try {\\n                        Class<?> hiveToDrillMapRDBJsonRuleClass = Class.forName(\"org.apache.drill.exec.planner.sql.logical.ConvertHiveMapRDBJsonScanToDrillMapRDBJsonScan\");\\n                        ruleBuilder.add((StoragePluginOptimizerRule) hiveToDrillMapRDBJsonRuleClass.getField(\"INSTANCE\").get(null));\\n                    } catch (ReflectiveOperationException e) {\\n                        logger.warn(\"Current Drill build is not designed for working with Hive MapR-DB tables. \" + \"Please disable {} option\", ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER);\\n                    }\\n                }\\n                return ruleBuilder.build();\\n            }\\n        default:\\n            return ImmutableSet.of();\\n    }\\n}\\n}\\n\\ncontrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java (Before)\\nimport org.apache.drill.exec.rpc.user.QueryDataBatch;\\nimport org.junit.AfterClass;\\nimport org.junit.BeforeClass;\\nimport org.junit.Ignore;\\nimport org.junit.Test;\\nimport org.junit.experimental.categories.Category;\\npublic class TestHivePartitionPruning extends HiveTestBase {\\n// DRILL-5032\\n@Test\\npublic void testPartitionColumnsCaching() throws Exception {\\n    final String query = \"EXPLAIN PLAN FOR SELECT * FROM hive.partition_with_few_schemas\";\\n    List<QueryDataBatch> queryDataBatches = testSqlWithResults(query);\\n    String resultString = getResultString(queryDataBatches, \"|\");\\n    // different for both partitions column strings from physical plan\\n    String columnString = \"\\\\\"name\\\\\" : \\\\\"a\\\\\"\";\\n    String secondColumnString = \"\\\\\"name\\\\\" : \\\\\"a1\\\\\"\";\\n    int columnIndex = resultString.indexOf(columnString);\\n    assertTrue(columnIndex >= 0);\\n    columnIndex = resultString.indexOf(columnString, columnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, columnIndex);\\n    int secondColumnIndex = resultString.indexOf(secondColumnString);\\n    assertTrue(secondColumnIndex >= 0);\\n    secondColumnIndex = resultString.indexOf(secondColumnString, secondColumnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, secondColumnIndex);\\n}\\n// DRILL-6173\\n@Test\\n@Ignore(\"DRILL-8400\")\\npublic void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" + \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" + \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n    int actualRowCount = testSql(query);\\n    int expectedRowCount = 450;\\n    assertEquals(\"Expected and actual row count should match\", expectedRowCount, actualRowCount);\\n    final String[] expectedPlan = { \"partition_with_few_schemas.*numPartitions=6\", \"partition_pruning_test.*numPartitions=6\" };\\n    testPlanMatchingPatterns(query, expectedPlan);\\n}\\n}\\n\\ncontrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java (After)\\nimport org.apache.drill.exec.rpc.user.QueryDataBatch;\\nimport org.junit.AfterClass;\\nimport org.junit.BeforeClass;\\nimport org.junit.Test;\\nimport org.junit.experimental.categories.Category;\\npublic class TestHivePartitionPruning extends HiveTestBase {\\n// DRILL-5032\\n@Test\\npublic void testPartitionColumnsCaching() throws Exception {\\n    final String query = \"EXPLAIN PLAN FOR SELECT * FROM hive.partition_with_few_schemas\";\\n    List<QueryDataBatch> queryDataBatches = testSqlWithResults(query);\\n    String resultString = getResultString(queryDataBatches, \"|\");\\n    // different for both partitions column strings from physical plan\\n    String columnString = \"\\\\\"name\\\\\" : \\\\\"a\\\\\"\";\\n    String secondColumnString = \"\\\\\"name\\\\\" : \\\\\"a1\\\\\"\";\\n    int columnIndex = resultString.indexOf(columnString);\\n    assertTrue(columnIndex >= 0);\\n    columnIndex = resultString.indexOf(columnString, columnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, columnIndex);\\n    int secondColumnIndex = resultString.indexOf(secondColumnString);\\n    assertTrue(secondColumnIndex >= 0);\\n    secondColumnIndex = resultString.indexOf(secondColumnString, secondColumnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, secondColumnIndex);\\n}\\n// DRILL-6173\\n@Test\\npublic void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" + \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" + \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n    int actualRowCount = testSql(query);\\n    int expectedRowCount = 450;\\n    assertEquals(\"Expected and actual row count should match\", expectedRowCount, actualRowCount);\\n    final String[] expectedPlan = { \"partition_with_few_schemas.*numPartitions=6\", \"partition_pruning_test.*numPartitions=6\" };\\n    testPlanMatchingPatterns(query, expectedPlan);\\n}\\n}\\n\\n\\nQuery text:', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023463C35D30>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023462945F40> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023463B7B250>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"GET /info HTTP/11\" 200 672\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 20 Feb 2025 09:45:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'1608'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'197098'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'870ms'), (b'x-request-id', b'req_efda1f1554c931cb2fd0847c13e708bb'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=f9m7r_jEqzmD4HkCtsEqBuE8v17RgOUgQRA9dXtGEEE-1740044745-1.0.1.1-mNqI5Tj9vUDXpjCdTIwrrhXrklU0OlR1kmycdu.YYsXlGDMs8cJefnJ9Yh4SfX7DZs8m9NWGhgmUl6q5B9PltQ; path=/; expires=Thu, 20-Feb-25 10:15:45 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=eWyg2KJ0Rmz89bdSOTk1IekuVkoYEcm06ZKPIBT1QSA-1740044745492-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'914d90bb6a8d4103-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Thu, 20 Feb 2025 09:45:45 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '1608'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '197098'), ('x-ratelimit-reset-requests', '8.64s'), ('x-ratelimit-reset-tokens', '870ms'), ('x-request-id', 'req_efda1f1554c931cb2fd0847c13e708bb'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=f9m7r_jEqzmD4HkCtsEqBuE8v17RgOUgQRA9dXtGEEE-1740044745-1.0.1.1-mNqI5Tj9vUDXpjCdTIwrrhXrklU0OlR1kmycdu.YYsXlGDMs8cJefnJ9Yh4SfX7DZs8m9NWGhgmUl6q5B9PltQ; path=/; expires=Thu, 20-Feb-25 10:15:45 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=eWyg2KJ0Rmz89bdSOTk1IekuVkoYEcm06ZKPIBT1QSA-1740044745492-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '914d90bb6a8d4103-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_efda1f1554c931cb2fd0847c13e708bb\n",
      "DEBUG:faiss.loader:Environment variable FAISS_OPT_LEVEL is not set, so let's pick the instruction set according to the current CPU\n",
      "INFO:faiss.loader:Loading faiss with AVX512 support.\n",
      "INFO:faiss.loader:Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x000002347DB64B80>, 'json_data': {'input': [[791, 2082, 4442, 7652, 1268, 264, 3230, 1920, 2663, 330, 42098, 86292, 1, 374, 18073, 11, 28865, 279, 10474, 505, 330, 7391, 15942, 1, 311, 330, 34590, 7237, 10794, 1899, 1753, 1, 311, 7417, 15374, 13, 23212, 11, 264, 1296, 430, 574, 8767, 12305, 374, 1457, 4642, 11, 23391, 279, 2082, 4375, 12722, 449, 279, 6177, 17071, 86292, 15293, 13]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023463CD1450>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023462946210> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023463D3D940>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 20 Feb 2025 09:45:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'133'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-56dffcfb88-gplkn'), (b'x-envoy-upstream-service-time', b'121'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999939'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'req_5452631bb8f03da1c65ebbbc74b88d6e'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=T2nCK7ndDl3T08NCpWoa7U5m.ZC1CU_FUbNj5.tI9f0-1740044746-1.0.1.1-ECUJgs2Z4qj7gNxAmiX0p731UESHsTJlSyIq5csV3lgidH4MNnVqdDKTWybZLcAdrFi3L49ZUXdVOUihcXj0_A; path=/; expires=Thu, 20-Feb-25 10:15:46 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=EpcJFlTtjv62GV5F6porpXfrS3J_247QTe.9geQUhm0-1740044746355-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'914d90ce0c819ff1-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Thu, 20 Feb 2025 09:45:46 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '133'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-56dffcfb88-gplkn'), ('x-envoy-upstream-service-time', '121'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999939'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '3ms'), ('x-request-id', 'req_5452631bb8f03da1c65ebbbc74b88d6e'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=T2nCK7ndDl3T08NCpWoa7U5m.ZC1CU_FUbNj5.tI9f0-1740044746-1.0.1.1-ECUJgs2Z4qj7gNxAmiX0p731UESHsTJlSyIq5csV3lgidH4MNnVqdDKTWybZLcAdrFi3L49ZUXdVOUihcXj0_A; path=/; expires=Thu, 20-Feb-25 10:15:46 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=EpcJFlTtjv62GV5F6porpXfrS3J_247QTe.9geQUhm0-1740044746355-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '914d90ce0c819ff1-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_5452631bb8f03da1c65ebbbc74b88d6e\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8400\\nIssue Summary: Fix pruning partitions with pushed transitive predicates\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nSee {{TestHivePartitionPruning.prunePartitionsBasedOnTransitivePredicates()}} test for details.\\n\\n\\n\\nThe issue occurs for queries like these:\\n\\n{code:sql}\\n\\nSELECT * FROM hive.partition_pruning_test t1 \\n\\nJOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \\n\\nWHERE t2.`e` IS NOT NULL AND t1.`d` = 1\\n\\n{code}\\n\\n\\n\\nThe expected behavior is to create additional filters based on the existing filters and join conditions. We have a {{TRANSITIVE_CLOSURE}} planning phase, which is responsible for such query transformations, but Drill pushes down filters from the WHERE condition before that phase, so the optimization is not performed.\\n\\n\\n\\nIdeally, we should move rules from the {{TRANSITIVE_CLOSURE}} phase to the {{LOGICAL}} phase so that the planner will choose the most optimal plan, but it wouldn\\'t help until CALCITE-1048 is fixed (it is required to pull predicates when three has {{RelSubset}} nodes).\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8381\\nIssue Summary: Add support for filtered aggregate calls\\nIssue Type: New Feature\\nPriority: Major\\n\\nDescription:\\nCurrently, Drill ignores filters for filtered aggregate calls and returns incorrect results.\\n\\nHere is the example query for which Drill will return incorrect results:\\n\\n{code:sql}\\n\\nSELECT count(n_name) FILTER(WHERE n_regionkey = 1) AS nations_count_in_1_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 2) AS nations_count_in_2_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 3) AS nations_count_in_3_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 4) AS nations_count_in_4_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 0) AS nations_count_in_0_region\\n\\nFROM cp.`tpch/nation.parquet`\\n\\n{code}\\n\\n{noformat}\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| nations_count_in_1_region | nations_count_in_2_region | nations_count_in_3_region | nations_count_in_4_region | nations_count_in_0_region |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| 25                        | 25                        | 25                        | 25                        | 25                        |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n{noformat}\\n\\nBut the correct result is\\n\\n{noformat}\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| nations_count_in_1_region | nations_count_in_2_region | nations_count_in_3_region | nations_count_in_4_region | nations_count_in_0_region |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| 5                         | 5                         | 5                         | 5                         | 5                         |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n{noformat}\\n\\nSide note:\\n\\nThe query above could be rewritten using PIVOT:\\n\\n{code:sql}\\n\\nSELECT `1` nations_count_in_1_region, `2` nations_count_in_2_region, `3` nations_count_in_3_region, `4` nations_count_in_4_region, `0` nations_count_in_0_region\\n\\nFROM (SELECT n_name, n_regionkey FROM cp.`tpch/nation.parquet`) \\n\\nPIVOT(count(n_name) FOR n_regionkey IN (0, 1, 2, 3, 4))\\n\\n{code}\\n\\nAnd will return correct results when this issue is fixed and Calcite is updated to 1.33.0\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8513\\nIssue Summary: Right Hash Join with empty Left table ruturns 0 result\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nDrill returns no results on the right Hash Join if the probe(left) table is empty.\\n\\n\\n\\nThe simplest way to reproduce the issue:\\n\\n\\n\\n1.To force Drill not to use merge join and use the hash join operator instead:\\n\\n{code:java}\\n\\nalter session set planner.enable_mergejoin = false;\\n\\nalter session set planner.enable_nestedloopjoin= false; {code}\\n\\n2. Disable join order optimization to prevent Drill from flipping join tables:\\n\\n{code:java}\\n\\nalter session set planner.enable_join_optimization = false;  {code}\\n\\n3. Execute a query with empty left table outcome:\\n\\n{code:java}\\n\\nSELECT *\\n\\nFROMÂ\\xa0\\n\\nÂ\\xa0 Â\\xa0 (SELECT * FROM (VALUES (1, \\'Max\\', 28),Â\\xa0\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0(2, \\'Jane\\', 32),\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0(3, \\'Saymon\\', 29)\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0) AS users(id, name, age)\\n\\nÂ\\xa0 Â\\xa0 WHERE false\\n\\nÂ\\xa0 Â\\xa0 ) AS users\\n\\nRIGHT JOINÂ\\xa0\\n\\nÂ\\xa0 Â\\xa0 (VALUES (1, \\'Engineer\\'),Â\\xa0\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 (2, \\'Doctor\\'),Â\\xa0\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 (3, \\'Teacher\\')\\n\\nÂ\\xa0 Â\\xa0 ) AS job(id, title)\\n\\nON users.id = job.idÂ\\xa0{code}\\n\\nExpected result is:\\n\\n||id||name||age||id0||title||\\n\\n|null|null|null|1|Engineer|\\n\\n|null|null|null|2|Doctor|\\n\\n|null|null|null|3|Teacher|\\n\\n\\n\\nBut we get 0 rows.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8503\\nIssue Summary: Add Configuration Option to Skip Host Validation for Splunk\\nIssue Type: Improvement\\nPriority: Major\\n\\nDescription:\\nThis PR adds an option to skip host validation for SSL connections to Splunk.Â\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-4935\\nIssue Summary: Allow drillbits to advertise a configurable host address to Zookeeper\\nIssue Type: New Feature\\nPriority: Minor\\n\\nDescription:\\nThere are certain situations, such as running Drill in distributed Docker containers, in which it is desirable to advertise a different hostname to Zookeeper than would be output by INetAddress.getLocalHost().  I propose adding a configuration variable \\'drill.exec.rpc.bit.advertised.host\\' and passing this address to Zookeeper when the configuration variable is populated, otherwise falling back to the present behavior.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023447FC8D60>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023447F4F410>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023447F39D00>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023447F39E10>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000234629460F0> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023447F8FC50>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000234629460F0> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000234629460F0> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000234629460F0> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000234629460F0> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023447FA9D50>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023463D38500>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023463D38AA0>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002347E2004B0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002347E200130>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 20 Feb 2025 09:45:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'209'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9997'), (b'x-ratelimit-remaining-tokens', b'198169'), (b'x-ratelimit-reset-requests', b'22.87s'), (b'x-ratelimit-reset-tokens', b'549ms'), (b'x-request-id', b'req_8b02920ef25bfdeb997eb1b5a51ba595'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=9jMo5Zq2KiLrDPHpoTkLKArksqgyMA0wyDPPizHyoC8-1740044747-1.0.1.1-emhQ0aq4hN2Y3hpEnUbOjNPJ_hFpvLmv387aiXyX7Kw3SNOGzLokMvQoKLwj3ysFAXoeRpSUzl1btKFMdNbzMA; path=/; expires=Thu, 20-Feb-25 10:15:47 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=lwjY838mEhDzX2z8HbQIy.aUEMlmAEtH3fm.diozPcg-1740044747109-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'914d90d249a34103-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Thu, 20 Feb 2025 09:45:47 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '209'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9997'), ('x-ratelimit-remaining-tokens', '198169'), ('x-ratelimit-reset-requests', '22.87s'), ('x-ratelimit-reset-tokens', '549ms'), ('x-request-id', 'req_8b02920ef25bfdeb997eb1b5a51ba595'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=9jMo5Zq2KiLrDPHpoTkLKArksqgyMA0wyDPPizHyoC8-1740044747-1.0.1.1-emhQ0aq4hN2Y3hpEnUbOjNPJ_hFpvLmv387aiXyX7Kw3SNOGzLokMvQoKLwj3ysFAXoeRpSUzl1btKFMdNbzMA; path=/; expires=Thu, 20-Feb-25 10:15:47 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=lwjY838mEhDzX2z8HbQIy.aUEMlmAEtH3fm.diozPcg-1740044747109-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '914d90d249a34103-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_8b02920ef25bfdeb997eb1b5a51ba595\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 20 Feb 2025 09:45:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'259'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9995'), (b'x-ratelimit-remaining-tokens', b'195754'), (b'x-ratelimit-reset-requests', b'40.134s'), (b'x-ratelimit-reset-tokens', b'1.273s'), (b'x-request-id', b'req_86a3599480d4af61711e51e9a0703fa1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=8ogL7ypJPnmrmza2fbashXJXbwdLZUiFNLj7suWWl04-1740044747-1.0.1.1-QKbylDtv7b25stbkBkEMdRcHXKd5u8_mjgxMApqFnV3Ie29Cc3x15OJBAO1OadyqMGm.oQAUw4jmy..ARfjwhg; path=/; expires=Thu, 20-Feb-25 10:15:47 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=mwYY042VGxqmvR5m6OXW0WFoloU5evmZHBquMBZ4eQI-1740044747179-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'914d90d24e635fd5-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Thu, 20 Feb 2025 09:45:47 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '259'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9995'), ('x-ratelimit-remaining-tokens', '195754'), ('x-ratelimit-reset-requests', '40.134s'), ('x-ratelimit-reset-tokens', '1.273s'), ('x-request-id', 'req_86a3599480d4af61711e51e9a0703fa1'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=8ogL7ypJPnmrmza2fbashXJXbwdLZUiFNLj7suWWl04-1740044747-1.0.1.1-QKbylDtv7b25stbkBkEMdRcHXKd5u8_mjgxMApqFnV3Ie29Cc3x15OJBAO1OadyqMGm.oQAUw4jmy..ARfjwhg; path=/; expires=Thu, 20-Feb-25 10:15:47 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=mwYY042VGxqmvR5m6OXW0WFoloU5evmZHBquMBZ4eQI-1740044747179-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '914d90d24e635fd5-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_86a3599480d4af61711e51e9a0703fa1\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 20 Feb 2025 09:45:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'313'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'199017'), (b'x-ratelimit-reset-requests', b'14.234s'), (b'x-ratelimit-reset-tokens', b'294ms'), (b'x-request-id', b'req_902e6c48390dac4aae5bd6acf0aaf18a'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=qw54dbdBnW9C.cnG3AlD6TDX8zf7l87EHX8wGvIxiG8-1740044747-1.0.1.1-kWfKMuYsGYNupswanhvUqrk6YHGHdnsDyB1drzWc.Wqj68eSX8i_h6tTS0k8PbfgWJ4s47EW.6DRdadrWsszkQ; path=/; expires=Thu, 20-Feb-25 10:15:47 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=mxhD.buWZSz_uhxif0I5uiizb_GVqk7YN7hxGLIRfCs-1740044747223-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'914d90d24e91894a-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Thu, 20 Feb 2025 09:45:47 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '313'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9998'), ('x-ratelimit-remaining-tokens', '199017'), ('x-ratelimit-reset-requests', '14.234s'), ('x-ratelimit-reset-tokens', '294ms'), ('x-request-id', 'req_902e6c48390dac4aae5bd6acf0aaf18a'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=qw54dbdBnW9C.cnG3AlD6TDX8zf7l87EHX8wGvIxiG8-1740044747-1.0.1.1-kWfKMuYsGYNupswanhvUqrk6YHGHdnsDyB1drzWc.Wqj68eSX8i_h6tTS0k8PbfgWJ4s47EW.6DRdadrWsszkQ; path=/; expires=Thu, 20-Feb-25 10:15:47 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=mxhD.buWZSz_uhxif0I5uiizb_GVqk7YN7hxGLIRfCs-1740044747223-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '914d90d24e91894a-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_902e6c48390dac4aae5bd6acf0aaf18a\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 20 Feb 2025 09:45:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'366'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9994'), (b'x-ratelimit-remaining-tokens', b'194997'), (b'x-ratelimit-reset-requests', b'48.769s'), (b'x-ratelimit-reset-tokens', b'1.5s'), (b'x-request-id', b'req_2f613129da320735b2e5483523abf7df'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=G5bNIn8ymeD4HVDkB91Gf3GETYNF7UKo.wHLRGJkVe4-1740044747-1.0.1.1-9WBnui7o_EaTdX4M1mA4_1txoSlgFOPy8L8no5WsnoDQT_4ImmFO9KWW4CvDRZQ20_KisQmtYNMND_4y.m5vsg; path=/; expires=Thu, 20-Feb-25 10:15:47 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=HOfujYMI7_nncBvLK2CHGj8Hm0DlOvjp06vkUhlc5Hc-1740044747288-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'914d90d24c39ab62-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Thu, 20 Feb 2025 09:45:47 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '366'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9994'), ('x-ratelimit-remaining-tokens', '194997'), ('x-ratelimit-reset-requests', '48.769s'), ('x-ratelimit-reset-tokens', '1.5s'), ('x-request-id', 'req_2f613129da320735b2e5483523abf7df'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=G5bNIn8ymeD4HVDkB91Gf3GETYNF7UKo.wHLRGJkVe4-1740044747-1.0.1.1-9WBnui7o_EaTdX4M1mA4_1txoSlgFOPy8L8no5WsnoDQT_4ImmFO9KWW4CvDRZQ20_KisQmtYNMND_4y.m5vsg; path=/; expires=Thu, 20-Feb-25 10:15:47 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=HOfujYMI7_nncBvLK2CHGj8Hm0DlOvjp06vkUhlc5Hc-1740044747288-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '914d90d24c39ab62-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_2f613129da320735b2e5483523abf7df\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 20 Feb 2025 09:45:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'283'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9996'), (b'x-ratelimit-remaining-tokens', b'197075'), (b'x-ratelimit-reset-requests', b'31.515s'), (b'x-ratelimit-reset-tokens', b'877ms'), (b'x-request-id', b'req_b8a37e4963a7e390e2b41eb7142f8085'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=qZiHfSZGuup3tJns7Hy9sfbDvIXC0F0KXnlfRXqDlR4-1740044748-1.0.1.1-hsrpNUpP38YZgZLs1oQsm8eFDFB9.KbiN886sGlY7eVxgILsKNfgfrLc68Qao5DaSU4488M8Eidjia0jopkHEQ; path=/; expires=Thu, 20-Feb-25 10:15:48 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=FoXDkeGAxB3h_BgqbfyIao4hbj2ivERMm8ly6Ih345Y-1740044748981-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'914d90d249914c89-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Thu, 20 Feb 2025 09:45:48 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '283'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9996'), ('x-ratelimit-remaining-tokens', '197075'), ('x-ratelimit-reset-requests', '31.515s'), ('x-ratelimit-reset-tokens', '877ms'), ('x-request-id', 'req_b8a37e4963a7e390e2b41eb7142f8085'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=qZiHfSZGuup3tJns7Hy9sfbDvIXC0F0KXnlfRXqDlR4-1740044748-1.0.1.1-hsrpNUpP38YZgZLs1oQsm8eFDFB9.KbiN886sGlY7eVxgILsKNfgfrLc68Qao5DaSU4488M8Eidjia0jopkHEQ; path=/; expires=Thu, 20-Feb-25 10:15:48 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=FoXDkeGAxB3h_BgqbfyIao4hbj2ivERMm8ly6Ih345Y-1740044748981-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '914d90d249914c89-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_b8a37e4963a7e390e2b41eb7142f8085\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Classify the Git diff into one of the following six software maintenance activities: feat, fix, perf, test, refactor, or chore. Return the activity that best matches the code changes. If one or more contexts are relevant to the code changes, use them to help classify the Git diff. Note that some contexts may be irrelevant to the code changes.\\n\\nRefer to the definitions below for each activity.\\n\\nfeat: introducing new features into the system.\\nfix: fixing existing bugs or issues in the system.\\nperf: improving the performance of the system.\\ntest: adding, modifying, or deleting test cases.\\nrefactor: changes made to the internal structure of software to make it easier to understand and cheaper to modify without changing its observable behavior, including code styling.\\nchore: regular maintenance tasks, such as updating dependencies or build tasks.\\n\\nAvoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: diff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n\\n> Additional context:\\nTicket ID: DRILL-8400\\nIssue Summary: Fix pruning partitions with pushed transitive predicates\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nSee {{TestHivePartitionPruning.prunePartitionsBasedOnTransitivePredicates()}} test for details.\\n\\n\\n\\nThe issue occurs for queries like these:\\n\\n{code:sql}\\n\\nSELECT * FROM hive.partition_pruning_test t1 \\n\\nJOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \\n\\nWHERE t2.`e` IS NOT NULL AND t1.`d` = 1\\n\\n{code}\\n\\n\\n\\nThe expected behavior is to create additional filters based on the existing filters and join conditions. We have a {{TRANSITIVE_CLOSURE}} planning phase, which is responsible for such query transformations, but Drill pushes down filters from the WHERE condition before that phase, so the optimization is not performed.\\n\\n\\n\\nIdeally, we should move rules from the {{TRANSITIVE_CLOSURE}} phase to the {{LOGICAL}} phase so that the planner will choose the most optimal plan, but it wouldn\\'t help until CALCITE-1048 is fixed (it is required to pull predicates when three has {{RelSubset}} nodes).\\n\\n--- RETRIEVED DOCUMENT SPLIT END ---\\n\\n\\n\\n> Software maintenance activity (feat / fix / perf / test / refactor / chore):\\n', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023447F51710>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023462945B50> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023447FDBB90>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 20 Feb 2025 09:45:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'315'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9993'), (b'x-ratelimit-remaining-tokens', b'198836'), (b'x-ratelimit-reset-requests', b'54.891s'), (b'x-ratelimit-reset-tokens', b'349ms'), (b'x-request-id', b'req_f78c295620246b627218e8e849c78874'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=yKT6hvDMUH6UosQe7XvhTkrdBi1S6Sh63D1s7WMJ6PA-1740044749-1.0.1.1-.c6rBkUdOPodlqMvwv8dHntiayxYw2t4ueehnKl6LM2pouff0sw0ExUJOPC2WXvQ405BEkuqa..q9W0obFlm4A; path=/; expires=Thu, 20-Feb-25 10:15:49 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=1JWYCLHKFOl2.hlXe.AHCC.YCMl873hhrSLZdpUmSOg-1740044749758-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'914d90e23fb36c00-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Thu, 20 Feb 2025 09:45:49 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '315'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9993'), ('x-ratelimit-remaining-tokens', '198836'), ('x-ratelimit-reset-requests', '54.891s'), ('x-ratelimit-reset-tokens', '349ms'), ('x-request-id', 'req_f78c295620246b627218e8e849c78874'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=yKT6hvDMUH6UosQe7XvhTkrdBi1S6Sh63D1s7WMJ6PA-1740044749-1.0.1.1-.c6rBkUdOPodlqMvwv8dHntiayxYw2t4ueehnKl6LM2pouff0sw0ExUJOPC2WXvQ405BEkuqa..q9W0obFlm4A; path=/; expires=Thu, 20-Feb-25 10:15:49 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=1JWYCLHKFOl2.hlXe.AHCC.YCMl873hhrSLZdpUmSOg-1740044749758-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '914d90e23fb36c00-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_f78c295620246b627218e8e849c78874\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Write a concise commit message based on the Git diff and additional context provided. If the context is relevant, include it in the commit body. Use IDs, names, or titles to reference relevant contexts for brevity. Including multiple contexts is allowed.\\n\\nA good commit message explains what changes were made and why they were necessary. Wrap the body at one to three brief sentences.\\n\\nFollow this format for the commit message:\\n\\n{type}: {subject}\\n\\n{body}\\n\\nGit diff 1:\\ndiff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java\\nindex c4e42c94b93..e2fad7560e3 100644\\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java\\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java\\n@@ -53,24 +53,29 @@ public static void registerCommands(CommandFactory factory) {\\n   /** merge multiple files together */\\n   public static class Merge extends FsCommand {\\n     public static final String NAME = \"getmerge\";    \\n-    public static final String USAGE = \"[-nl] <src> <localdst>\";\\n+    public static final String USAGE = \"[-nl] [-skip-empty-file] \"\\n+        + \"<src> <localdst>\";\\n     public static final String DESCRIPTION =\\n-      \"Get all the files in the directories that \" +\\n-      \"match the source file pattern and merge and sort them to only \" +\\n-      \"one file on local fs. <src> is kept.\\\\n\" +\\n-      \"-nl: Add a newline character at the end of each file.\";\\n+        \"Get all the files in the directories that \"\\n+        + \"match the source file pattern and merge and sort them to only \"\\n+        + \"one file on local fs. <src> is kept.\\\\n\"\\n+        + \"-nl: Add a newline character at the end of each file.\\\\n\"\\n+        + \"-skip-empty-file: Do not add new line character for empty file.\";\\n \\n     protected PathData dst = null;\\n     protected String delimiter = null;\\n+    private boolean skipEmptyFileDelimiter;\\n     protected List<PathData> srcs = null;\\n \\n     @Override\\n     protected void processOptions(LinkedList<String> args) throws IOException {\\n       try {\\n-        CommandFormat cf = new CommandFormat(2, Integer.MAX_VALUE, \"nl\");\\n+        CommandFormat cf = new CommandFormat(2, Integer.MAX_VALUE, \"nl\",\\n+            \"skip-empty-file\");\\n         cf.parse(args);\\n \\n         delimiter = cf.getOpt(\"nl\") ? \"\\\\n\" : null;\\n+        skipEmptyFileDelimiter = cf.getOpt(\"skip-empty-file\");\\n \\n         dst = new PathData(new URI(args.removeLast()), getConf());\\n         if (dst.exists && dst.stat.isDirectory()) {\\n@@ -92,21 +97,26 @@ protected void processArguments(LinkedList<PathData> items)\\n       FSDataOutputStream out = dst.fs.create(dst.path);\\n       try {\\n         for (PathData src : srcs) {\\n-          FSDataInputStream in = src.fs.open(src.path);\\n-          try {\\n-            IOUtils.copyBytes(in, out, getConf(), false);\\n-            if (delimiter != null) {\\n-              out.write(delimiter.getBytes(\"UTF-8\"));\\n+          if (src.stat.getLen() != 0) {\\n+            try (FSDataInputStream in = src.fs.open(src.path)) {\\n+              IOUtils.copyBytes(in, out, getConf(), false);\\n+              writeDelimiter(out);\\n             }\\n-          } finally {\\n-            in.close();\\n+          } else if (!skipEmptyFileDelimiter) {\\n+            writeDelimiter(out);\\n           }\\n         }\\n       } finally {\\n         out.close();\\n-      }      \\n+      }\\n     }\\n- \\n+\\n+    private void writeDelimiter(FSDataOutputStream out) throws IOException {\\n+      if (delimiter != null) {\\n+        out.write(delimiter.getBytes(\"UTF-8\"));\\n+      }\\n+    }\\n+\\n     @Override\\n     protected void processNonexistentPath(PathData item) throws IOException {\\n       exitCode = 1; // flag that a path is bad\\n\\n\\nAdditional context 1:\\nTicket ID: HADOOP-12657\\nIssue Summary: Add a option to skip newline on empty files with getMerge -nl\\nIssue Type: New Feature\\nPriority: Minor\\n\\nDescription:\\nHello everyone,\\n\\nI recently was in the need of using the new line option -nl with getMerge because the files I needed to merge simply didn\\'t had one. I was merging all the files from one directory and unfortunately this directory also included empty files, which effectively led to multiple newlines append after some files. I needed to remove them manually afterwards.\\n\\nIn this situation it is maybe good to have another argument that allows skipping empty files.\\nThing one could try to implement this feature:\\n\\nThe call for IOUtils.copyBytes(in, out, getConf(), false); doesn\\'t\\nreturn the number of bytes copied which would be convenient as one could\\nskip append the new line when 0 bytes where copied or one would check the file size before.\\n\\nI posted this Idea on the mailing list http://mail-archives.apache.org/mod_mbox/hadoop-user/201507.mbox/%3C55B25140.3060005%40trivago.com%3E but I didn\\'t really get many responses, so I thought I my try this way.\\n\\n--- RETRIEVED DOCUMENT SPLIT END ---\\n\\n\\n\\nCommit Type 1: feat\\n\\ncommit message 1: feat: add option to skip newlines for empty files in getmerge command\\n\\nAdded a new `-skip-empty-file` option to the `getmerge` command in CopyCommands. This prevents adding newline characters when merging directories that contain empty files, addressing the issue described in HADOOP-12657. Updated the command description, usage, and logic to support the new flag.\\n\\nGit diff 2:\\ndiff --git a/activemq-client/src/main/java/org/apache/activemq/openwire/v10/BaseDataStreamMarshaller.java b/activemq-client/src/main/java/org/apache/activemq/openwire/v10/BaseDataStreamMarshaller.java\\nindex 091743ebd..a570d3d17 100644\\n--- a/activemq-client/src/main/java/org/apache/activemq/openwire/v10/BaseDataStreamMarshaller.java\\n+++ b/activemq-client/src/main/java/org/apache/activemq/openwire/v10/BaseDataStreamMarshaller.java\\n@@ -29,6 +29,7 @@ import org.apache.activemq.util.ByteSequence;\\n public abstract class BaseDataStreamMarshaller implements DataStreamMarshaller {\\n \\n     public static final Constructor STACK_TRACE_ELEMENT_CONSTRUCTOR;\\n+    private static final int MAX_EXCEPTION_MESSAGE_SIZE = 1024;\\n \\n     static {\\n         Constructor constructor = null;\\n@@ -243,7 +244,7 @@ public abstract class BaseDataStreamMarshaller implements DataStreamMarshaller {\\n             int rc = 0;\\n             bs.writeBoolean(true);\\n             rc += tightMarshalString1(o.getClass().getName(), bs);\\n-            rc += tightMarshalString1(o.getMessage(), bs);\\n+            rc += tightMarshalString1(cutMessageIfNeeded(o.getMessage()), bs);\\n             if (wireFormat.isStackTraceEnabled()) {\\n                 rc += 2;\\n                 StackTraceElement[] stackTrace = o.getStackTrace();\\n@@ -264,7 +265,7 @@ public abstract class BaseDataStreamMarshaller implements DataStreamMarshaller {\\n                                           BooleanStream bs) throws IOException {\\n         if (bs.readBoolean()) {\\n             tightMarshalString2(o.getClass().getName(), dataOut, bs);\\n-            tightMarshalString2(o.getMessage(), dataOut, bs);\\n+            tightMarshalString2(cutMessageIfNeeded(o.getMessage()), dataOut, bs);\\n             if (wireFormat.isStackTraceEnabled()) {\\n                 StackTraceElement[] stackTrace = o.getStackTrace();\\n                 dataOut.writeShort(stackTrace.length);\\n@@ -550,7 +551,7 @@ public abstract class BaseDataStreamMarshaller implements DataStreamMarshaller {\\n         dataOut.writeBoolean(o != null);\\n         if (o != null) {\\n             looseMarshalString(o.getClass().getName(), dataOut);\\n-            looseMarshalString(o.getMessage(), dataOut);\\n+            looseMarshalString(cutMessageIfNeeded(o.getMessage()), dataOut);\\n             if (wireFormat.isStackTraceEnabled()) {\\n                 StackTraceElement[] stackTrace = o.getStackTrace();\\n                 dataOut.writeShort(stackTrace.length);\\n@@ -641,4 +642,10 @@ public abstract class BaseDataStreamMarshaller implements DataStreamMarshaller {\\n         }\\n         return rc;\\n     }\\n+    \\n+    protected String cutMessageIfNeeded(final String message) {\\n+        return (message.length() > MAX_EXCEPTION_MESSAGE_SIZE)?\\n+            message.substring(0, MAX_EXCEPTION_MESSAGE_SIZE - 3) + \"...\" : message;\\n+            \\n+    }\\n }\\n\\n\\nAdditional context 2:\\nTicket ID: AMQ-6894\\nIssue Summary: Excessive number of connections by failover transport with priorityBackup\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nMy clients connect to AMQ with this connection string:\\r\\n\\r\\n(tcp://amq1:61616,tcp://amq2:61616)?randomize=false&priorityBackup=true\\r\\n\\r\\n\\xa0It works - for some time. But sooner or later my AMQ server becomes unresponsive because the host it runs on runs out of resources (threads).\\r\\n\\r\\nSuddenly AMQ Server log explodes with the messages like:\\r\\n\\r\\n{code}\\r\\n2018-01-26 09:26:16,909 | WARN\\xa0 | Failed to register MBean org.apache.activemq :type=Broker,brokerName=activemq-vm-primary,connector=clientConnectors,connect\\r\\n\\r\\norName=default,connectionViewType=clientId,connectionName=ID_ca8f70e115d0-3708\\r\\n\\r\\n7-1516883370639-0_22 | org.apache.activemq.broker.jmx.ManagedTransportConnecti\\r\\n\\r\\non | ActiveMQ Transport: tcp:///172.10.7.56:55548@61616\\r\\n\\r\\n2018-01-26 09:26:21,375 | WARN\\xa0 | Ignoring ack received before dispatch; result of failover with an outstanding ack. Acked messages will be replayed if present on this broker. Ignored ack: MessageAck \\\\{commandId = 157, responseRequired = false, ackType = 2, consumerId = ID:ca8f70e115d0-37087-1516883370639-1:22:10:1, firstMessageId = ID:a95345a9c0df-33771-1516883685728-1:17:5:1:23, lastMessageId = ID:a95345a9c0df-33771-1516883685728-1:17:5:1:23, destination = queue://MY_QUEUE_OUT, transactionId = null, messageCount = 1, poisonCause = null} | org.apache.activemq.broker.region.PrefetchSubscription | ActiveMQ Transport: tcp:///172.16.6.56:55464@61616\\r\\n\\r\\n2018-01-26 09:26:39,211 | WARN\\xa0 | Transport Connection to: tcp://172.10.6.56:55860 failed: java.net.SocketException: Connection reset | org.apache.activemq.broker.TransportConnection.Transport | ActiveMQ InactivityMonitor Worker\\r\\n\\r\\n2018-01-26 09:26:47,175 | WARN\\xa0 | Transport Connection to: tcp://172.10.6.56:57012 failed: java.net.SocketException: Broken pipe (Write failed) | org.apache.activemq.broker.TransportConnection.Transport | ActiveMQ InactivityMonitor Worker\\r\\n{code}\\r\\n\\r\\nAfter short period of time AMQ server comes out of resources with \"java.lang.OutOfMemoryError: unable to create new native thread\" error. The AMQ service process in this case has a huge number of threads (some thousands)\\r\\n\\r\\n\\xa0\\r\\n\\r\\nThe client side log contains a lot of reconnection attempts messages like:\\r\\n\\r\\n{code}\\r\\n2018-01-26 00:10:31,387 WARN\\xa0\\xa0\\xa0 [\\\\{{bundle.name,org.apache.activemq.activemq-osgi}{bundle.version,5.14.1}\\\\{bundle.id,181}}]\\xa0\\xa0\\xa0\\xa0 [null]\\xa0 org.apache.activemq.transport.failover.FailoverTransport\\xa0 \\xa0\\xa0\\xa0\\xa0Failed to connect to [tcp://activemq-vm-primary:61616, tcp://activemq-vm-secondary:61616] after: 810 attempt(s) continuing to retry.\\r\\n{code}\\r\\n\\r\\nIt seems that client creates a huge number of connections by failover retry and after some time kills the server.\\r\\n\\r\\nIssue looks very similar to described in https://issues.apache.org/jira/browse/AMQ-6603, however server isn\\'t configured with access control settings.\\r\\n\\r\\nI found the description of similar problem into [http://activemq.2283324.n4.nabble.com/ActiveMQ-5-2-OutOfMemoryError-unable-to-create-new-native-thread-td2366585.html],\\xa0 but without concrete suggestion.\\r\\n\\r\\n\\xa0\\r\\n\\r\\nPart of server log is attached\\n\\n--- RETRIEVED DOCUMENT SPLIT END ---\\n\\n\\n\\nCommit Type 2: fix\\n\\nCommit message 2: fix: limit exception message size to prevent resource exhaustion\\n\\nIntroduced a `MAX_EXCEPTION_MESSAGE_SIZE` constant (1024 characters) and added logic to truncate overly long exception messages in `BaseDataStreamMarshaller`. This prevents excessive memory and resource usage due to unbounded exception message sizes, addressing the issue in AMQ-6894.\\n\\nGit diff 3:\\ndiff --git a/src/java/org/apache/cassandra/hints/HintsDispatchTrigger.java b/src/java/org/apache/cassandra/hints/HintsDispatchTrigger.java\\nindex ca38c0c319..fbaeaebcb8 100644\\n--- a/src/java/org/apache/cassandra/hints/HintsDispatchTrigger.java\\n+++ b/src/java/org/apache/cassandra/hints/HintsDispatchTrigger.java\\n@@ -19,20 +19,13 @@ package org.apache.cassandra.hints;\\n \\n import java.util.concurrent.atomic.AtomicBoolean;\\n \\n-import org.apache.cassandra.gms.ApplicationState;\\n-import org.apache.cassandra.gms.Gossiper;\\n-import org.apache.cassandra.schema.Schema;\\n-\\n-import static org.apache.cassandra.utils.FBUtilities.getBroadcastAddressAndPort;\\n-\\n /**\\n  * A simple dispatch trigger that\\'s being run every 10 seconds.\\n  *\\n  * Goes through all hint stores and schedules for dispatch all the hints for hosts that are:\\n  * 1. Not currently scheduled for dispatch, and\\n  * 2. Either have some hint files, or an active hint writer, and\\n- * 3. Are live, and\\n- * 4. Have matching schema versions\\n+ * 3. Are live\\n  *\\n  * What does triggering a hints store for dispatch mean?\\n  * - If there are existing hint files, it means submitting them for dispatch;\\n@@ -65,7 +58,6 @@ final class HintsDispatchTrigger implements Runnable\\n                .filter(store -> !isScheduled(store))\\n                .filter(HintsStore::isLive)\\n                .filter(store -> store.isWriting() || store.hasFiles())\\n-               .filter(store -> Schema.instance.isSameVersion(Gossiper.instance.getSchemaVersion(store.address())))\\n                .forEach(this::schedule);\\n     }\\n \\n\\n\\nAdditional context 3:\\nTicket ID: CASSANDRA-20188\\nIssue Summary: Allow hint delivery during schema mismatch\\nIssue Type: Bug\\nPriority: Normal\\n\\nDescription:\\nIn CASSANDRA-2083 we made hints require schema agreement to avoid a flood of errors in case the table the hints were destined for did not yet exist.  This, however, has other undesirable effects, such as making keeping upgrades in a mixed mode over a longer period of time less tenable.  We should still try to deliver hints and back off if the destination table doesn\\'t exist.\\n\\n--- RETRIEVED DOCUMENT SPLIT END ---\\n\\n\\n\\nCommit Type 3: fix\\n\\nCommit message 3: fix: allow hint delivery during schema mismatch\\n\\nRemoved schema version checks in `HintsDispatchTrigger` to enable hint delivery even during schema mismatches. This change ensures smoother operations in mixed-mode upgrades by allowing hint dispatch while backing off if the destination table does not exist, addressing issue CASSANDRA-20188.\\n\\nGit diff 4:\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n\\nAdditional context 4:\\nTicket ID: DRILL-8400\\nIssue Summary: Fix pruning partitions with pushed transitive predicates\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nSee {{TestHivePartitionPruning.prunePartitionsBasedOnTransitivePredicates()}} test for details.\\n\\n\\n\\nThe issue occurs for queries like these:\\n\\n{code:sql}\\n\\nSELECT * FROM hive.partition_pruning_test t1 \\n\\nJOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \\n\\nWHERE t2.`e` IS NOT NULL AND t1.`d` = 1\\n\\n{code}\\n\\n\\n\\nThe expected behavior is to create additional filters based on the existing filters and join conditions. We have a {{TRANSITIVE_CLOSURE}} planning phase, which is responsible for such query transformations, but Drill pushes down filters from the WHERE condition before that phase, so the optimization is not performed.\\n\\n\\n\\nIdeally, we should move rules from the {{TRANSITIVE_CLOSURE}} phase to the {{LOGICAL}} phase so that the planner will choose the most optimal plan, but it wouldn\\'t help until CALCITE-1048 is fixed (it is required to pull predicates when three has {{RelSubset}} nodes).\\n\\n--- RETRIEVED DOCUMENT SPLIT END ---\\n\\n\\n\\nCommit type 4: fix\\n\\nCommit message 4:', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023463B4A2D0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023462945E20> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023463B9B540>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 20 Feb 2025 09:45:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'1617'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9992'), (b'x-ratelimit-remaining-tokens', b'195463'), (b'x-ratelimit-reset-requests', b'1m2.742s'), (b'x-ratelimit-reset-tokens', b'1.36s'), (b'x-request-id', b'req_fc43c4577b3a5dd45ff15c8414874e22'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=0.hewYX3c2srvUpB7wYhwqWPtc8EFvC0eXkCs9Swzww-1740044751-1.0.1.1-bTLDofj6.lvkLYocBu_K5ghWnMammhV.D_GNB3pc1DYgqRiXHnvCGDxXtcG1ssTL_BHJ3FgFGjbZhMhFBsg7Yw; path=/; expires=Thu, 20-Feb-25 10:15:51 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=ZKX_Axos94vvu6Tygdn8MK0e9TcDEKszI6_UFl7iCcs-1740044751861-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'914d90e68b4e3f54-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Thu, 20 Feb 2025 09:45:51 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '1617'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9992'), ('x-ratelimit-remaining-tokens', '195463'), ('x-ratelimit-reset-requests', '1m2.742s'), ('x-ratelimit-reset-tokens', '1.36s'), ('x-request-id', 'req_fc43c4577b3a5dd45ff15c8414874e22'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=0.hewYX3c2srvUpB7wYhwqWPtc8EFvC0eXkCs9Swzww-1740044751-1.0.1.1-bTLDofj6.lvkLYocBu_K5ghWnMammhV.D_GNB3pc1DYgqRiXHnvCGDxXtcG1ssTL_BHJ3FgFGjbZhMhFBsg7Yw; path=/; expires=Thu, 20-Feb-25 10:15:51 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=ZKX_Axos94vvu6Tygdn8MK0e9TcDEKszI6_UFl7iCcs-1740044751861-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '914d90e68b4e3f54-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_fc43c4577b3a5dd45ff15c8414874e22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n"
     ]
    }
   ],
   "source": [
    "INCLUDED_GENERATOR_INDEXES = [0, 1, 2]\n",
    "INCLUDED_GENERATOR_INDEXES = [1]\n",
    "\n",
    "filtered_generators = [GENERATORS[i] for i in INCLUDED_GENERATOR_INDEXES]\n",
    "evaluator.evaluate(filtered_generators, COMMITS, CONTEXT_DATA_PATH, CMG_OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
