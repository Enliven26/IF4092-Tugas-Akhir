{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "load_dotenv(dotenv_path=\".env.test\", verbose=True, override=True)\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autocommit.core.enums import EnvironmentKey\n",
    "from autocommit_evaluation.cmg.evaluators import CommitMessageGenerator\n",
    "from autocommit_evaluation.cmg import evaluator\n",
    "from autocommit_evaluation.core import (\n",
    "    open_ai_few_shot_high_level_context_cmg_chain,\n",
    "    open_ai_low_level_context_cmg_chain,\n",
    "    open_ai_zero_shot_high_level_context_cmg_chain,\n",
    "    open_ai_high_level_context_chain,\n",
    "    deepseek_zero_shot_high_level_context_cmg_chain,\n",
    "    deepseek_high_level_context_chain,\n",
    ")\n",
    "from autocommit.core.models import CommitDataModel\n",
    "from autocommit_evaluation.datapreparation import context_generator, example_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMIT_DATA_JSON_FILE_PATH = os.path.join(\"autocommit_evaluation\", \"data\", \"cmg\", \"commits.test.json\")\n",
    "CONTEXT_DATA_PATH = os.path.join(\"autocommit_evaluation\",\"data\", \"context\")\n",
    "\n",
    "DEFAULT_CONTEXT_GENERATION_OUTPUT_PATH = os.path.join(\n",
    "    \"autocommit_evaluation\", \"data\", \"context\"\n",
    ")\n",
    "DEFAULT_HIGH_LEVEL_CONTEXT_OUTPUT_PATH = os.path.join(\n",
    "    \"out\", \"test\", \"highlevelcontext\"\n",
    ")\n",
    "DEFAULT_CMG_OUTPUT_PATH = os.path.join(\"out\", \"test\", \"cmg\")\n",
    "DEFAULT_DIFF_CLASSIFICATION_OUTPUT_PATH = os.path.join(\n",
    "    \"out\", \"test\", \"diffclassification\"\n",
    ")\n",
    "\n",
    "DIFF_CLASSIFIER_CHAINS = [\n",
    "    open_ai_zero_shot_high_level_context_cmg_chain,\n",
    "    open_ai_low_level_context_cmg_chain,\n",
    "    deepseek_zero_shot_high_level_context_cmg_chain\n",
    "]\n",
    "\n",
    "HIGH_LEVEL_CONTEXT_CHAINS = [\n",
    "    open_ai_high_level_context_chain,\n",
    "    deepseek_high_level_context_chain,\n",
    "]\n",
    "\n",
    "GENERATORS = [\n",
    "    CommitMessageGenerator(\n",
    "        \"Open AI Zero-Shot High-Level Context Generator\", open_ai_zero_shot_high_level_context_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\n",
    "        \"Open AI Few-Shot High-Level Context Generator\", open_ai_few_shot_high_level_context_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\"Open AI Low-Level Context Generator\", open_ai_low_level_context_cmg_chain),\n",
    "    CommitMessageGenerator(\n",
    "        \"DeepSeek Zero-Shot High-Level Context Generator\", deepseek_zero_shot_high_level_context_cmg_chain\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_GENERATION_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.CONTEXT_GENERATION_OUTPUT_PATH.value,\n",
    "        DEFAULT_CONTEXT_GENERATION_OUTPUT_PATH,\n",
    "    )\n",
    "\n",
    "HIGH_LEVEL_CONTEXT_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.HIGH_LEVEL_CONTEXT_OUTPUT_PATH.value,\n",
    "        DEFAULT_HIGH_LEVEL_CONTEXT_OUTPUT_PATH,\n",
    "    )\n",
    "\n",
    "CMG_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.CMG_OUTPUT_PATH.value, DEFAULT_CMG_OUTPUT_PATH\n",
    "    )\n",
    "\n",
    "DIFF_CLASSIFICATION_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.DIFF_CLASSIFICATION_OUTPUT_PATH.value,\n",
    "        DEFAULT_DIFF_CLASSIFICATION_OUTPUT_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commits(path: str) -> list[CommitDataModel]:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            json_string = file.read()\n",
    "\n",
    "        return CommitDataModel.from_json(json_string)\n",
    "\n",
    "COMMITS = get_commits(COMMIT_DATA_JSON_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.smith.langchain.com:443\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Given a Git diff and the relevant source code, write a concise summary of the code changes in a way that a non-technical person can understand. The query text must summarize the code changes in two very brief sentences.\\n\\nGit diff:\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n\\nSource code:\\ncontrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java (Before)\\npublic class HiveStoragePlugin extends AbstractStoragePlugin {\\n@Override\\npublic Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n    switch(phase) {\\n        case LOGICAL:\\n            final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n            ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnScan(optimizerContext, defaultPartitionValue));\\n            return ruleBuilder.build();\\n        case PHYSICAL:\\n            {\\n                ruleBuilder = ImmutableSet.builder();\\n                OptionManager options = optimizerContext.getPlannerSettings().getOptions();\\n                // TODO: Remove implicit using of convert_fromTIMESTAMP_IMPALA function\\n                // once \"store.parquet.reader.int96_as_timestamp\" will be true by default\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_SCAN_WITH_NATIVE_READERS) || options.getBoolean(ExecConstants.HIVE_OPTIMIZE_PARQUET_SCAN_WITH_NATIVE_READER)) {\\n                    ruleBuilder.add(ConvertHiveParquetScanToDrillParquetScan.INSTANCE);\\n                }\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER)) {\\n                    try {\\n                        Class<?> hiveToDrillMapRDBJsonRuleClass = Class.forName(\"org.apache.drill.exec.planner.sql.logical.ConvertHiveMapRDBJsonScanToDrillMapRDBJsonScan\");\\n                        ruleBuilder.add((StoragePluginOptimizerRule) hiveToDrillMapRDBJsonRuleClass.getField(\"INSTANCE\").get(null));\\n                    } catch (ReflectiveOperationException e) {\\n                        logger.warn(\"Current Drill build is not designed for working with Hive MapR-DB tables. \" + \"Please disable {} option\", ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER);\\n                    }\\n                }\\n                return ruleBuilder.build();\\n            }\\n        default:\\n            return ImmutableSet.of();\\n    }\\n}\\n}\\n\\ncontrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java (After)\\npublic class HiveStoragePlugin extends AbstractStoragePlugin {\\n@Override\\npublic Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n    switch(phase) {\\n        case PARTITION_PRUNING:\\n            final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n            ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnScan(optimizerContext, defaultPartitionValue));\\n            return ruleBuilder.build();\\n        case PHYSICAL:\\n            {\\n                ruleBuilder = ImmutableSet.builder();\\n                OptionManager options = optimizerContext.getPlannerSettings().getOptions();\\n                // TODO: Remove implicit using of convert_fromTIMESTAMP_IMPALA function\\n                // once \"store.parquet.reader.int96_as_timestamp\" will be true by default\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_SCAN_WITH_NATIVE_READERS) || options.getBoolean(ExecConstants.HIVE_OPTIMIZE_PARQUET_SCAN_WITH_NATIVE_READER)) {\\n                    ruleBuilder.add(ConvertHiveParquetScanToDrillParquetScan.INSTANCE);\\n                }\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER)) {\\n                    try {\\n                        Class<?> hiveToDrillMapRDBJsonRuleClass = Class.forName(\"org.apache.drill.exec.planner.sql.logical.ConvertHiveMapRDBJsonScanToDrillMapRDBJsonScan\");\\n                        ruleBuilder.add((StoragePluginOptimizerRule) hiveToDrillMapRDBJsonRuleClass.getField(\"INSTANCE\").get(null));\\n                    } catch (ReflectiveOperationException e) {\\n                        logger.warn(\"Current Drill build is not designed for working with Hive MapR-DB tables. \" + \"Please disable {} option\", ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER);\\n                    }\\n                }\\n                return ruleBuilder.build();\\n            }\\n        default:\\n            return ImmutableSet.of();\\n    }\\n}\\n}\\n\\ncontrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java (Before)\\nimport org.apache.drill.exec.rpc.user.QueryDataBatch;\\nimport org.junit.AfterClass;\\nimport org.junit.BeforeClass;\\nimport org.junit.Ignore;\\nimport org.junit.Test;\\nimport org.junit.experimental.categories.Category;\\npublic class TestHivePartitionPruning extends HiveTestBase {\\n// DRILL-5032\\n@Test\\npublic void testPartitionColumnsCaching() throws Exception {\\n    final String query = \"EXPLAIN PLAN FOR SELECT * FROM hive.partition_with_few_schemas\";\\n    List<QueryDataBatch> queryDataBatches = testSqlWithResults(query);\\n    String resultString = getResultString(queryDataBatches, \"|\");\\n    // different for both partitions column strings from physical plan\\n    String columnString = \"\\\\\"name\\\\\" : \\\\\"a\\\\\"\";\\n    String secondColumnString = \"\\\\\"name\\\\\" : \\\\\"a1\\\\\"\";\\n    int columnIndex = resultString.indexOf(columnString);\\n    assertTrue(columnIndex >= 0);\\n    columnIndex = resultString.indexOf(columnString, columnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, columnIndex);\\n    int secondColumnIndex = resultString.indexOf(secondColumnString);\\n    assertTrue(secondColumnIndex >= 0);\\n    secondColumnIndex = resultString.indexOf(secondColumnString, secondColumnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, secondColumnIndex);\\n}\\n// DRILL-6173\\n@Test\\n@Ignore(\"DRILL-8400\")\\npublic void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" + \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" + \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n    int actualRowCount = testSql(query);\\n    int expectedRowCount = 450;\\n    assertEquals(\"Expected and actual row count should match\", expectedRowCount, actualRowCount);\\n    final String[] expectedPlan = { \"partition_with_few_schemas.*numPartitions=6\", \"partition_pruning_test.*numPartitions=6\" };\\n    testPlanMatchingPatterns(query, expectedPlan);\\n}\\n}\\n\\ncontrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java (After)\\nimport org.apache.drill.exec.rpc.user.QueryDataBatch;\\nimport org.junit.AfterClass;\\nimport org.junit.BeforeClass;\\nimport org.junit.Test;\\nimport org.junit.experimental.categories.Category;\\npublic class TestHivePartitionPruning extends HiveTestBase {\\n// DRILL-5032\\n@Test\\npublic void testPartitionColumnsCaching() throws Exception {\\n    final String query = \"EXPLAIN PLAN FOR SELECT * FROM hive.partition_with_few_schemas\";\\n    List<QueryDataBatch> queryDataBatches = testSqlWithResults(query);\\n    String resultString = getResultString(queryDataBatches, \"|\");\\n    // different for both partitions column strings from physical plan\\n    String columnString = \"\\\\\"name\\\\\" : \\\\\"a\\\\\"\";\\n    String secondColumnString = \"\\\\\"name\\\\\" : \\\\\"a1\\\\\"\";\\n    int columnIndex = resultString.indexOf(columnString);\\n    assertTrue(columnIndex >= 0);\\n    columnIndex = resultString.indexOf(columnString, columnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, columnIndex);\\n    int secondColumnIndex = resultString.indexOf(secondColumnString);\\n    assertTrue(secondColumnIndex >= 0);\\n    secondColumnIndex = resultString.indexOf(secondColumnString, secondColumnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, secondColumnIndex);\\n}\\n// DRILL-6173\\n@Test\\npublic void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" + \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" + \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n    int actualRowCount = testSql(query);\\n    int expectedRowCount = 450;\\n    assertEquals(\"Expected and actual row count should match\", expectedRowCount, actualRowCount);\\n    final String[] expectedPlan = { \"partition_with_few_schemas.*numPartitions=6\", \"partition_pruning_test.*numPartitions=6\" };\\n    testPlanMatchingPatterns(query, expectedPlan);\\n}\\n}\\n\\n\\nQuery text:', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000164641D6BA0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000164630B6960> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000164641D8050>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"GET /info HTTP/11\" 200 672\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 15 Feb 2025 07:49:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'975'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'197098'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'870ms'), (b'x-request-id', b'req_4ddea915277d9cb8844c71d76d46d752'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=N5UJlx1Y6esgQG6a1lk2R2AiWhv0o5p2wJKL9iq41ec-1739605753-1.0.1.1-9v0Zx_yniuMu8LBiqj9QqQqyctob.gmpwmsiA4Upe53rCsLuAWr7Cxk9joyZcfM1i.45NaFxt_pKr6EckDZEyQ; path=/; expires=Sat, 15-Feb-25 08:19:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=_XZgY0aEKeE41BjAdPXAjnVyJ.NhTSx0.guNvygw4kc-1739605753246-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9123b32a1be048e8-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 15 Feb 2025 07:49:13 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '975'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '197098'), ('x-ratelimit-reset-requests', '8.64s'), ('x-ratelimit-reset-tokens', '870ms'), ('x-request-id', 'req_4ddea915277d9cb8844c71d76d46d752'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=N5UJlx1Y6esgQG6a1lk2R2AiWhv0o5p2wJKL9iq41ec-1739605753-1.0.1.1-9v0Zx_yniuMu8LBiqj9QqQqyctob.gmpwmsiA4Upe53rCsLuAWr7Cxk9joyZcfM1i.45NaFxt_pKr6EckDZEyQ; path=/; expires=Sat, 15-Feb-25 08:19:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=_XZgY0aEKeE41BjAdPXAjnVyJ.NhTSx0.guNvygw4kc-1739605753246-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9123b32a1be048e8-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_4ddea915277d9cb8844c71d76d46d752\n",
      "DEBUG:faiss.loader:Environment variable FAISS_OPT_LEVEL is not set, so let's pick the instruction set according to the current CPU\n",
      "INFO:faiss.loader:Loading faiss with AVX512 support.\n",
      "INFO:faiss.loader:Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x00000164641DF1A0>, 'json_data': {'input': [[791, 2082, 4442, 2713, 279, 1648, 264, 3230, 734, 11618, 828, 555, 21760, 389, 17071, 86292, 4619, 315, 20406, 8863, 13, 23212, 11, 264, 1296, 1162, 574, 11041, 311, 4148, 264, 8767, 12305, 2704, 11, 28462, 433, 311, 387, 1629, 323, 33432, 13]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000164641DA5D0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000164630B6BA0> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000164652A6B10>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 15 Feb 2025 07:49:14 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'77'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-68d69466db-sgmqq'), (b'x-envoy-upstream-service-time', b'56'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999956'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'req_265b65e4dd2fedf4d66a9f74321279cc'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Skk_HsIY0Ok.ITx2xwSKegRP8WdEuZjXT3WaW5_yhEw-1739605754-1.0.1.1-pZ9n7L5k9Zh0UHR3QcSJwC5z_eI6lenUl96NbT0UtuBG8Hq2smEJuBOMLVKzxL3YBF4NrHqWy4CFuRmjimrX1g; path=/; expires=Sat, 15-Feb-25 08:19:14 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=wPYDeuMRrseTvB93DBHThrmq8W_gPHwhENzJZS2hOoA-1739605754710-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9123b3385c6a44be-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Sat, 15 Feb 2025 07:49:14 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '77'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-68d69466db-sgmqq'), ('x-envoy-upstream-service-time', '56'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999956'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '2ms'), ('x-request-id', 'req_265b65e4dd2fedf4d66a9f74321279cc'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=Skk_HsIY0Ok.ITx2xwSKegRP8WdEuZjXT3WaW5_yhEw-1739605754-1.0.1.1-pZ9n7L5k9Zh0UHR3QcSJwC5z_eI6lenUl96NbT0UtuBG8Hq2smEJuBOMLVKzxL3YBF4NrHqWy4CFuRmjimrX1g; path=/; expires=Sat, 15-Feb-25 08:19:14 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=wPYDeuMRrseTvB93DBHThrmq8W_gPHwhENzJZS2hOoA-1739605754710-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9123b3385c6a44be-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_265b65e4dd2fedf4d66a9f74321279cc\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8400\\nIssue Summary: Fix pruning partitions with pushed transitive predicates\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nSee {{TestHivePartitionPruning.prunePartitionsBasedOnTransitivePredicates()}} test for details.\\n\\n\\n\\nThe issue occurs for queries like these:\\n\\n{code:sql}\\n\\nSELECT * FROM hive.partition_pruning_test t1 \\n\\nJOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \\n\\nWHERE t2.`e` IS NOT NULL AND t1.`d` = 1\\n\\n{code}\\n\\n\\n\\nThe expected behavior is to create additional filters based on the existing filters and join conditions. We have a {{TRANSITIVE_CLOSURE}} planning phase, which is responsible for such query transformations, but Drill pushes down filters from the WHERE condition before that phase, so the optimization is not performed.\\n\\n\\n\\nIdeally, we should move rules from the {{TRANSITIVE_CLOSURE}} phase to the {{LOGICAL}} phase so that the planner will choose the most optimal plan, but it wouldn\\'t help until CALCITE-1048 is fixed (it is required to pull predicates when three has {{RelSubset}} nodes).\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8381\\nIssue Summary: Add support for filtered aggregate calls\\nIssue Type: New Feature\\nPriority: Major\\n\\nDescription:\\nCurrently, Drill ignores filters for filtered aggregate calls and returns incorrect results.\\n\\nHere is the example query for which Drill will return incorrect results:\\n\\n{code:sql}\\n\\nSELECT count(n_name) FILTER(WHERE n_regionkey = 1) AS nations_count_in_1_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 2) AS nations_count_in_2_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 3) AS nations_count_in_3_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 4) AS nations_count_in_4_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 0) AS nations_count_in_0_region\\n\\nFROM cp.`tpch/nation.parquet`\\n\\n{code}\\n\\n{noformat}\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| nations_count_in_1_region | nations_count_in_2_region | nations_count_in_3_region | nations_count_in_4_region | nations_count_in_0_region |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| 25                        | 25                        | 25                        | 25                        | 25                        |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n{noformat}\\n\\nBut the correct result is\\n\\n{noformat}\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| nations_count_in_1_region | nations_count_in_2_region | nations_count_in_3_region | nations_count_in_4_region | nations_count_in_0_region |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| 5                         | 5                         | 5                         | 5                         | 5                         |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n{noformat}\\n\\nSide note:\\n\\nThe query above could be rewritten using PIVOT:\\n\\n{code:sql}\\n\\nSELECT `1` nations_count_in_1_region, `2` nations_count_in_2_region, `3` nations_count_in_3_region, `4` nations_count_in_4_region, `0` nations_count_in_0_region\\n\\nFROM (SELECT n_name, n_regionkey FROM cp.`tpch/nation.parquet`) \\n\\nPIVOT(count(n_name) FOR n_regionkey IN (0, 1, 2, 3, 4))\\n\\n{code}\\n\\nAnd will return correct results when this issue is fixed and Calcite is updated to 1.33.0\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8503\\nIssue Summary: Add Configuration Option to Skip Host Validation for Splunk\\nIssue Type: Improvement\\nPriority: Major\\n\\nDescription:\\nThis PR adds an option to skip host validation for SSL connections to Splunk.Â\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8513\\nIssue Summary: Right Hash Join with empty Left table ruturns 0 result\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nDrill returns no results on the right Hash Join if the probe(left) table is empty.\\n\\n\\n\\nThe simplest way to reproduce the issue:\\n\\n\\n\\n1.To force Drill not to use merge join and use the hash join operator instead:\\n\\n{code:java}\\n\\nalter session set planner.enable_mergejoin = false;\\n\\nalter session set planner.enable_nestedloopjoin= false; {code}\\n\\n2. Disable join order optimization to prevent Drill from flipping join tables:\\n\\n{code:java}\\n\\nalter session set planner.enable_join_optimization = false;  {code}\\n\\n3. Execute a query with empty left table outcome:\\n\\n{code:java}\\n\\nSELECT *\\n\\nFROMÂ\\xa0\\n\\nÂ\\xa0 Â\\xa0 (SELECT * FROM (VALUES (1, \\'Max\\', 28),Â\\xa0\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0(2, \\'Jane\\', 32),\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0(3, \\'Saymon\\', 29)\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0) AS users(id, name, age)\\n\\nÂ\\xa0 Â\\xa0 WHERE false\\n\\nÂ\\xa0 Â\\xa0 ) AS users\\n\\nRIGHT JOINÂ\\xa0\\n\\nÂ\\xa0 Â\\xa0 (VALUES (1, \\'Engineer\\'),Â\\xa0\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 (2, \\'Doctor\\'),Â\\xa0\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 (3, \\'Teacher\\')\\n\\nÂ\\xa0 Â\\xa0 ) AS job(id, title)\\n\\nON users.id = job.idÂ\\xa0{code}\\n\\nExpected result is:\\n\\n||id||name||age||id0||title||\\n\\n|null|null|null|1|Engineer|\\n\\n|null|null|null|2|Doctor|\\n\\n|null|null|null|3|Teacher|\\n\\n\\n\\nBut we get 0 rows.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-4935\\nIssue Summary: Allow drillbits to advertise a configurable host address to Zookeeper\\nIssue Type: New Feature\\nPriority: Minor\\n\\nDescription:\\nThere are certain situations, such as running Drill in distributed Docker containers, in which it is desirable to advertise a different hostname to Zookeeper than would be output by INetAddress.getLocalHost().  I propose adding a configuration variable \\'drill.exec.rpc.bit.advertised.host\\' and passing this address to Zookeeper when the configuration variable is populated, otherwise falling back to the present behavior.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001644845D350>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000164630B6A80> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000016448403770>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000164652AA7A0>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000164652AA8B0>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000016448442A50>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000164630B6A80> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000164630B6A80> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000164630B6A80> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000164630B6A80> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001644840B050>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000016465299F40>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001646529AB70>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001646601F4D0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001647E7C8210>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 15 Feb 2025 07:49:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'273'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'198922'), (b'x-ratelimit-reset-requests', b'14.449s'), (b'x-ratelimit-reset-tokens', b'323ms'), (b'x-request-id', b'req_249d6065087f7ed83f7dfcdfab85b00a'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=qVVOzkrvfOIhhklijC9oTzrDJFCbpX.HJD1uqLcYlWo-1739605755-1.0.1.1-PoCuGcGrEdFpyXuk03Gl76J2REoKg.pr8wF1BMkeqUtR7pM5nU0yeJOGIZNIXKxvm36mWo0HErQBEIauJE1ypA; path=/; expires=Sat, 15-Feb-25 08:19:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=paU3pnfkxbzbp0g7L9aT_adSS48WbruvWy.oryGsnIk-1739605755365-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9123b33fa8c0f918-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 15 Feb 2025 07:49:15 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '273'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9998'), ('x-ratelimit-remaining-tokens', '198922'), ('x-ratelimit-reset-requests', '14.449s'), ('x-ratelimit-reset-tokens', '323ms'), ('x-request-id', 'req_249d6065087f7ed83f7dfcdfab85b00a'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=qVVOzkrvfOIhhklijC9oTzrDJFCbpX.HJD1uqLcYlWo-1739605755-1.0.1.1-PoCuGcGrEdFpyXuk03Gl76J2REoKg.pr8wF1BMkeqUtR7pM5nU0yeJOGIZNIXKxvm36mWo0HErQBEIauJE1ypA; path=/; expires=Sat, 15-Feb-25 08:19:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=paU3pnfkxbzbp0g7L9aT_adSS48WbruvWy.oryGsnIk-1739605755365-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9123b33fa8c0f918-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_249d6065087f7ed83f7dfcdfab85b00a\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 15 Feb 2025 07:49:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'213'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9997'), (b'x-ratelimit-remaining-tokens', b'198341'), (b'x-ratelimit-reset-requests', b'22.969s'), (b'x-ratelimit-reset-tokens', b'497ms'), (b'x-request-id', b'req_c0c8723cc45dbf74320d9aa800fb01ba'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=sBI5Ou2_cTn3sHyRBSuEZMWJ6.SCj9Kq1u1.zoBZ.aw-1739605755-1.0.1.1-VBTu4EWwdVNW5jdtEGlNl294cks.LgO8Ky8n24AJpYPGZutWyOvp0kHXBs42w10O64KGrgN_YVDjTfGyiAMtfA; path=/; expires=Sat, 15-Feb-25 08:19:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=Wxf8.yspdpGQdmzN8oMQJdnJ_Rz9AzHbtvaibeZl66I-1739605755435-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9123b33fbe394103-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 15 Feb 2025 07:49:15 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '213'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9997'), ('x-ratelimit-remaining-tokens', '198341'), ('x-ratelimit-reset-requests', '22.969s'), ('x-ratelimit-reset-tokens', '497ms'), ('x-request-id', 'req_c0c8723cc45dbf74320d9aa800fb01ba'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=sBI5Ou2_cTn3sHyRBSuEZMWJ6.SCj9Kq1u1.zoBZ.aw-1739605755-1.0.1.1-VBTu4EWwdVNW5jdtEGlNl294cks.LgO8Ky8n24AJpYPGZutWyOvp0kHXBs42w10O64KGrgN_YVDjTfGyiAMtfA; path=/; expires=Sat, 15-Feb-25 08:19:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=Wxf8.yspdpGQdmzN8oMQJdnJ_Rz9AzHbtvaibeZl66I-1739605755435-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9123b33fbe394103-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_c0c8723cc45dbf74320d9aa800fb01ba\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 15 Feb 2025 07:49:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'213'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9996'), (b'x-ratelimit-remaining-tokens', b'197617'), (b'x-ratelimit-reset-requests', b'31.594s'), (b'x-ratelimit-reset-tokens', b'714ms'), (b'x-request-id', b'req_b1943e27da14b461b2cfb6dbd80246e7'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=mb7XWg17D_5kL4pmBm6Ewd1NM5t0oO8zIKIfQDNU6eE-1739605755-1.0.1.1-Y67gylGiFCME26TXX1rRE6_ph5zvJOD7UMNQwlJPW7S.urmQ0shgMxomqqxowqpSYsFmzk4qVlo4zyc83CV54g; path=/; expires=Sat, 15-Feb-25 08:19:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=UtEA.WaSESOJCeV6dFZFEMzaMqymGlCjAlb0vQf5j78-1739605755447-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9123b3407f523f54-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 15 Feb 2025 07:49:15 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '213'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9996'), ('x-ratelimit-remaining-tokens', '197617'), ('x-ratelimit-reset-requests', '31.594s'), ('x-ratelimit-reset-tokens', '714ms'), ('x-request-id', 'req_b1943e27da14b461b2cfb6dbd80246e7'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=mb7XWg17D_5kL4pmBm6Ewd1NM5t0oO8zIKIfQDNU6eE-1739605755-1.0.1.1-Y67gylGiFCME26TXX1rRE6_ph5zvJOD7UMNQwlJPW7S.urmQ0shgMxomqqxowqpSYsFmzk4qVlo4zyc83CV54g; path=/; expires=Sat, 15-Feb-25 08:19:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=UtEA.WaSESOJCeV6dFZFEMzaMqymGlCjAlb0vQf5j78-1739605755447-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9123b3407f523f54-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_b1943e27da14b461b2cfb6dbd80246e7\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 15 Feb 2025 07:49:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'207'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9994'), (b'x-ratelimit-remaining-tokens', b'196851'), (b'x-ratelimit-reset-requests', b'48.411s'), (b'x-ratelimit-reset-tokens', b'944ms'), (b'x-request-id', b'req_515da5319defeeb552d677c375f210df'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=6wi52VoFfqYHO2iDhN64Dek3sCnt3kYWA1KBjtlj0IU-1739605755-1.0.1.1-D5n_h3vNr2gfGeVOtMjxWXuLRkoUzdW.aRwH7aIqBhsLrsbIMvmr.YOrMS2O0cRwdg50OUIZblpunsBI2bIgSA; path=/; expires=Sat, 15-Feb-25 08:19:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=BGiwNK_lErmwla6IuD8Asjh_agpccnbLeiG9DUczVuM-1739605755894-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9123b33fb9fb9c65-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 15 Feb 2025 07:49:15 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '207'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9994'), ('x-ratelimit-remaining-tokens', '196851'), ('x-ratelimit-reset-requests', '48.411s'), ('x-ratelimit-reset-tokens', '944ms'), ('x-request-id', 'req_515da5319defeeb552d677c375f210df'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=6wi52VoFfqYHO2iDhN64Dek3sCnt3kYWA1KBjtlj0IU-1739605755-1.0.1.1-D5n_h3vNr2gfGeVOtMjxWXuLRkoUzdW.aRwH7aIqBhsLrsbIMvmr.YOrMS2O0cRwdg50OUIZblpunsBI2bIgSA; path=/; expires=Sat, 15-Feb-25 08:19:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=BGiwNK_lErmwla6IuD8Asjh_agpccnbLeiG9DUczVuM-1739605755894-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9123b33fb9fb9c65-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_515da5319defeeb552d677c375f210df\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 15 Feb 2025 07:49:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'218'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9995'), (b'x-ratelimit-remaining-tokens', b'197695'), (b'x-ratelimit-reset-requests', b'39.776s'), (b'x-ratelimit-reset-tokens', b'691ms'), (b'x-request-id', b'req_8896f8ee48f9c992abca4019b2b03369'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=QQTR94LeOSbOziRe27jhIJ.JYRGXF5y.6LM7FScTWks-1739605755-1.0.1.1-RRRvfw_xyeVcWDujeSjUI.usVQq1.e9w90eYUKOV3Q_EhD0FMTNVDGpGmG0Kl7qqheXMUkJjcxS1rAuUcco1Eg; path=/; expires=Sat, 15-Feb-25 08:19:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=JtzK0JbiLza4m6g17x8TMea.lWOk8mkK_54lGsi.dA8-1739605755907-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9123b33fbd43fe04-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 15 Feb 2025 07:49:15 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '218'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9995'), ('x-ratelimit-remaining-tokens', '197695'), ('x-ratelimit-reset-requests', '39.776s'), ('x-ratelimit-reset-tokens', '691ms'), ('x-request-id', 'req_8896f8ee48f9c992abca4019b2b03369'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=QQTR94LeOSbOziRe27jhIJ.JYRGXF5y.6LM7FScTWks-1739605755-1.0.1.1-RRRvfw_xyeVcWDujeSjUI.usVQq1.e9w90eYUKOV3Q_EhD0FMTNVDGpGmG0Kl7qqheXMUkJjcxS1rAuUcco1Eg; path=/; expires=Sat, 15-Feb-25 08:19:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=JtzK0JbiLza4m6g17x8TMea.lWOk8mkK_54lGsi.dA8-1739605755907-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9123b33fbd43fe04-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_8896f8ee48f9c992abca4019b2b03369\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Classify the Git diff into one of the following six software maintenance activities: feat, fix, perf, test, refactor, or chore. Return the activity that best matches the code changes. If one or more contexts are relevant to the code changes, use them to help classify the Git diff. Note that some contexts may be irrelevant to the code changes.\\n\\nRefer to the definitions below for each activity.\\n\\nfeat: introducing new features into the system.\\nfix: fixing existing bugs or issues in the system.\\nperf: improving the performance of the system.\\ntest: adding, modifying, or deleting test cases.\\nrefactor: changes made to the internal structure of software to make it easier to understand and cheaper to modify without changing its observable behavior, including code styling.\\nchore: regular maintenance tasks, such as updating dependencies or build tasks.\\n\\nAvoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: diff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n\\n> Additional context:\\nTicket ID: DRILL-8400\\nIssue Summary: Fix pruning partitions with pushed transitive predicates\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nSee {{TestHivePartitionPruning.prunePartitionsBasedOnTransitivePredicates()}} test for details.\\n\\n\\n\\nThe issue occurs for queries like these:\\n\\n{code:sql}\\n\\nSELECT * FROM hive.partition_pruning_test t1 \\n\\nJOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \\n\\nWHERE t2.`e` IS NOT NULL AND t1.`d` = 1\\n\\n{code}\\n\\n\\n\\nThe expected behavior is to create additional filters based on the existing filters and join conditions. We have a {{TRANSITIVE_CLOSURE}} planning phase, which is responsible for such query transformations, but Drill pushes down filters from the WHERE condition before that phase, so the optimization is not performed.\\n\\n\\n\\nIdeally, we should move rules from the {{TRANSITIVE_CLOSURE}} phase to the {{LOGICAL}} phase so that the planner will choose the most optimal plan, but it wouldn\\'t help until CALCITE-1048 is fixed (it is required to pull predicates when three has {{RelSubset}} nodes).\\n\\n--- RETRIEVED DOCUMENT SPLIT END ---\\n\\n\\n\\n> Software maintenance activity (feat / fix / perf / test / refactor / chore):\\n', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000016448470120>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000164630B6600> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000016463092B10>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 15 Feb 2025 07:49:16 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'256'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9993'), (b'x-ratelimit-remaining-tokens', b'197760'), (b'x-ratelimit-reset-requests', b'56.471s'), (b'x-ratelimit-reset-tokens', b'671ms'), (b'x-request-id', b'req_f57c50e21381abda2a3225323c77f945'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=qUEHx4UawlaDwybRQsoRUrEURmRYVgJ2c2W5EvFY4uM-1739605756-1.0.1.1-lrEk1IH7uQiY2fn_qnPhHHji29MZ7bGpCXHwTh2WbJ5jTABZ48eaom.GlovMWVm5Bi9Iu9rk8bS_zHxbABGkZA; path=/; expires=Sat, 15-Feb-25 08:19:16 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=_fgrStgUDPIxLU6ksdzHWAy43YdGpfsHOaI5ymT_r6U-1739605756529-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9123b346fc079c65-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 15 Feb 2025 07:49:16 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '256'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9993'), ('x-ratelimit-remaining-tokens', '197760'), ('x-ratelimit-reset-requests', '56.471s'), ('x-ratelimit-reset-tokens', '671ms'), ('x-request-id', 'req_f57c50e21381abda2a3225323c77f945'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=qUEHx4UawlaDwybRQsoRUrEURmRYVgJ2c2W5EvFY4uM-1739605756-1.0.1.1-lrEk1IH7uQiY2fn_qnPhHHji29MZ7bGpCXHwTh2WbJ5jTABZ48eaom.GlovMWVm5Bi9Iu9rk8bS_zHxbABGkZA; path=/; expires=Sat, 15-Feb-25 08:19:16 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=_fgrStgUDPIxLU6ksdzHWAy43YdGpfsHOaI5ymT_r6U-1739605756529-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9123b346fc079c65-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_f57c50e21381abda2a3225323c77f945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n"
     ]
    }
   ],
   "source": [
    "INCLUDED_DIFF_CLASSIFIER_CHAIN_INDEXES = [0, 1, 2]\n",
    "INCLUDED_DIFF_CLASSIFIER_CHAIN_INDEXES = [0]\n",
    "\n",
    "for index in INCLUDED_DIFF_CLASSIFIER_CHAIN_INDEXES:\n",
    "    evaluator.classify_diffs(DIFF_CLASSIFIER_CHAINS[index], COMMITS, CONTEXT_DATA_PATH, DIFF_CLASSIFICATION_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_generator.generate_context(COMMITS, CONTEXT_GENERATION_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get High Level Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDED_HIGH_LEVEL_CONTEXT_CHAIN_INDEXES = [0, 1]\n",
    "\n",
    "# for index in INCLUDED_HIGH_LEVEL_CONTEXT_CHAIN_INDEXES:\n",
    "#     evaluator.get_high_level_contexts(\n",
    "#         HIGH_LEVEL_CONTEXT_CHAINS[index],\n",
    "#         COMMITS, \n",
    "#         CONTEXT_DATA_PATH, \n",
    "#         HIGH_LEVEL_CONTEXT_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Commit Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDED_GENERATOR_INDEXES = [0, 1, 2, 3]\n",
    "\n",
    "# filtered_generators = [GENERATORS[i] for i in INCLUDED_GENERATOR_INDEXES]\n",
    "# evaluator.evaluate(filtered_generators, COMMITS, CONTEXT_DATA_PATH, CMG_OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
