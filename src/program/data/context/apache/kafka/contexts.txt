Ticket ID: KAFKA-18401
Issue Summary: Transaction version 2 does not support commit transaction without records
Issue Type: Bug
Priority: Blocker

Description:
This issue was observed when implementing https://issues.apache.org/jira/browse/KAFKA-18206.

In short, under transaction version 2, it doesn't support commit transaction without sending any records while transaction version 0 & 1 do support this kind of scenario.

Commit transactions without sending any records is fine when using transaction versions 0 or 1 because the producer won't send EndTxnRequest to the broker [0]. However, with transaction version 2, the producer still sends an EndTxnRequest to the broker while in transaction coordinator, the txn state is still in EMPTY, resulting in an error from the broker.

This issue can be reproduced with the test in below. I'm unsure if this behavior is expected. If it's not, one potential fix could be to follow the approach used in TV_0 and TV_1, where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change.
{code:java}
    @ClusterTests({
        @ClusterTest(brokers = 3, features = {
            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 0)}),
        @ClusterTest(brokers = 3, features = {
            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 1)}),
        @ClusterTest(brokers = 3, features = {
            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 2)})
    })
    public void testProducerEndTransaction2(ClusterInstance cluster) throws InterruptedException {
        Map<String, Object> properties = new HashMap<>();
        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "foobar");
        properties.put(ProducerConfig.CLIENT_ID_CONFIG, "test");
        properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        try (Producer<byte[], byte[]> producer1 = cluster.producer(properties)) {

            producer1.initTransactions();
            producer1.beginTransaction();
            producer1.commitTransaction(); // In TV_2, we'll get InvalidTxnStateException
        }
    }
{code}
Another test case, which is essentially the same as the previous one, starts with a transaction that includes records, and then proceeds to start the next transaction. When using transaction version 2, we encounter an error, but this time it's a different error from the one seen in the previous case.
{code:java}
    @ClusterTests({
        @ClusterTest(brokers = 3, features = {
            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 0)}),
        @ClusterTest(brokers = 3, features = {
            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 1)}),
        @ClusterTest(brokers = 3, features = {
            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 2)})
    })
    public void testProducerEndTransaction(ClusterInstance cluster) {
        Map<String, Object> properties = new HashMap<>();
        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "foobar");
        properties.put(ProducerConfig.CLIENT_ID_CONFIG, "test");
        properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        try (Producer<byte[], byte[]> producer1 = cluster.producer(properties)) {

            producer1.initTransactions();
            producer1.beginTransaction();
            producer1.send(new ProducerRecord<>("test", "key".getBytes(), "value".getBytes()));
            producer1.commitTransaction();

            producer1.beginTransaction();
            producer1.commitTransaction(); // In TV_2, we'll get ProducerFencedException
        }
    }
{code}
 

[0]: [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L857-L865]

--- RETRIEVED DOCUMENT SPLIT END ---

Ticket ID: KAFKA-806
Issue Summary: Index may not always observe log.index.interval.bytes
Issue Type: Improvement
Priority: Major

Description:
Currently, each log.append() will add at most 1 index entry, even when the appended data is larger than log.index.interval.bytes. One potential issue is that if a follower restarts after being down for a long time, it may fetch data much bigger than log.index.interval.bytes at a time. This means that fewer index entries are created, which can increase the fetch time from the consumers.

--- RETRIEVED DOCUMENT SPLIT END ---

Ticket ID: KAFKA-17561
Issue Summary: Operator Metrics for Kafka Streams
Issue Type: Improvement
Priority: Major

Description:
This task for adding 3 new metrics to Kafka Streams 
 # client-state the current state of the Kafka Streams client
 # thread-state a client-level metric for the state of each `StreamThread`
 # recording-level a metric that reflects the current recording level of KafkaStreams metrics

--- RETRIEVED DOCUMENT SPLIT END ---

Ticket ID: KAFKA-18211
Issue Summary: ClassGraph scanning does not correctly find isolated connect plugins
Issue Type: Bug
Priority: Blocker

Description:
Connect used to use reflections scanner for scanning and identifying connect plugins in its plugin.path. This would load said plugins in isolation via the use of a child first PluginClassloader, which is designed to load class from its set of URIs before delegating to parent, if not found. This effectively enforces that if a plugin and its dependencies are part of a plugin path it would not conflict with other plugins in the plugin path or plugins in classpath. 

 

GlassGraph was introduced as a replacement for the older reflections scanner in [KAFKA-15203 Use Classgraph since org.reflections is no longer under maintainence by PARADOXST · Pull Request #16604 · apache/kafka|https://github.com/apache/kafka/pull/16604]. It is used in place of reflections scanner for finding plugins during plugin scanning. The issue here is that it is missing any plugins present in isolated plugin paths if its already present in classpath.  We can repro this by adding the json converter under an isolated plugin path and starting connect with debug logs. We can see the logs from ReflectionsScanner and find that the ClassGraph loader is always fetching the plugin from the classpath even though the PluginClassLoader is provided. This is causing [kafka/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/ReflectionScanner.java at 520681c38dbefe497181c4fd5dfc793d54233408 · apache/kafka|https://github.com/apache/kafka/blob/520681c38dbefe497181c4fd5dfc793d54233408/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/ReflectionScanner.java#L134] check to fail with the logs like. 
{code:java}
[2024-12-11 07:29:28,968] DEBUG class org.apache.kafka.connect.json.JsonConverter from other classloader jdk.internal.loader.ClassLoaders$AppClassLoader@c387f44 is visible from C:\Users\user\Desktop\confluent\testing\plugins3\connect-file-1.2.1-T-0.9.0-P-3.1, excluding to prevent isolated loading (org.apache.kafka.connect.runtime.isolation.ReflectionScanner:135)
[2024-12-11 07:29:28,969] DEBUG class org.apache.kafka.connect.json.JsonConverter from other classloader jdk.internal.loader.ClassLoaders$AppClassLoader@c387f44 is visible from C:\Users\user\Desktop\confluent\testing\plugins3\connect-file-1.2.1-T-0.9.0-P-3.1, excluding to prevent isolated loading (org.apache.kafka.connect.runtime.isolation.ReflectionScanner:135) {code}

--- RETRIEVED DOCUMENT SPLIT END ---

Ticket ID: KAFKA-9192
Issue Summary: NullPointerException if field in schema not present in value
Issue Type: Bug
Priority: Major

Description:
Given a message:
{code:java}
{
   "schema":{
      "type":"struct",
      "fields":[
         {
            "type":"string",
            "optional":true,
            "field":"abc"
         }
      ],
      "optional":false,
      "name":"foobar"
   },
   "payload":{
   }
}
{code}


I would expect, given the field is optional, for the JsonConverter to still process this value. 

What happens is I get a null pointer exception, the stacktrace points to this line: https://github.com/apache/kafka/blob/2.1/connect/json/src/main/java/org/apache/kafka/connect/json/JsonConverter.java#L701 called by https://github.com/apache/kafka/blob/2.1/connect/json/src/main/java/org/apache/kafka/connect/json/JsonConverter.java#L181

Issue seems to be that we need to check and see if the jsonValue is null before checking if the jsonValue has a null value.