{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from core.enums import EnvironmentKey\n",
    "from cmg.evaluators import CommitMessageGenerator\n",
    "from cmg import evaluator\n",
    "from core import (\n",
    "    open_ai_few_shot_high_level_cmg_chain,\n",
    "    open_ai_low_level_cmg_chain,\n",
    "    open_ai_zero_shot_high_level_cmg_chain,\n",
    "    open_ai_high_level_context_chain,\n",
    "    deepseek_few_shot_high_level_cmg_chain,\n",
    "    deepseek_low_level_cmg_chain,\n",
    "    deepseek_zero_shot_high_level_cmg_chain,\n",
    "    deepseek_high_level_context_chain,\n",
    ")\n",
    "from core.models import CommitDataModel\n",
    "from datapreparation import context_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMIT_DATA_JSON_FILE_PATH = os.path.join(\"data\", \"cmg\", \"commits.test.json\")\n",
    "CONTEXT_DATA_PATH = os.path.join(\"data\", \"context\")\n",
    "\n",
    "DEFAULT_CONTEXT_GENERATION_OUTPUT_PATH = os.path.join(\n",
    "    \"data\", \"context\"\n",
    ")\n",
    "DEFAULT_HIGH_LEVEL_CONTEXT_OUTPUT_PATH = os.path.join(\n",
    "    \"out\", \"test\", \"highlevelcontext\"\n",
    ")\n",
    "DEFAULT_CMG_OUTPUT_PATH = os.path.join(\"out\", \"test\", \"cmg\")\n",
    "DEFAULT_DIFF_CLASSIFICATION_OUTPUT_PATH = os.path.join(\n",
    "    \"out\", \"test\", \"diffclassification\"\n",
    ")\n",
    "\n",
    "DIFF_CLASSIFIER_CHAINS = [\n",
    "    open_ai_zero_shot_high_level_cmg_chain,\n",
    "    open_ai_low_level_cmg_chain,\n",
    "    deepseek_zero_shot_high_level_cmg_chain,\n",
    "    deepseek_low_level_cmg_chain,\n",
    "]\n",
    "\n",
    "HIGH_LEVEL_CONTEXT_CHAINS = [\n",
    "    open_ai_high_level_context_chain,\n",
    "    deepseek_high_level_context_chain,\n",
    "]\n",
    "\n",
    "GENERATORS = [\n",
    "    CommitMessageGenerator(\n",
    "        \"Open AI Zero-Shot High-Level Context Generator\", open_ai_zero_shot_high_level_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\n",
    "        \"Open AI Few-Shot High-Level Context Generator\", open_ai_few_shot_high_level_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\"Open AI Low-Level Context Generator\", open_ai_low_level_cmg_chain),\n",
    "    CommitMessageGenerator(\n",
    "        \"DeepSeek Zero-Shot High-Level Context Generator\", deepseek_zero_shot_high_level_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\n",
    "        \"DeepSeek Few-Shot High-Level Context Generator\", deepseek_few_shot_high_level_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\"DeepSeek Low-Level Context Generator\", deepseek_low_level_cmg_chain),\n",
    "]\n",
    "\n",
    "load_dotenv(dotenv_path=\".env.test\", verbose=True, override=True)\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_GENERATION_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.CONTEXT_GENERATION_OUTPUT_PATH.value,\n",
    "        DEFAULT_CONTEXT_GENERATION_OUTPUT_PATH,\n",
    "    )\n",
    "\n",
    "HIGH_LEVEL_CONTEXT_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.HIGH_LEVEL_CONTEXT_OUTPUT_PATH.value,\n",
    "        DEFAULT_HIGH_LEVEL_CONTEXT_OUTPUT_PATH,\n",
    "    )\n",
    "\n",
    "CMG_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.CMG_OUTPUT_PATH.value, DEFAULT_CMG_OUTPUT_PATH\n",
    "    )\n",
    "\n",
    "DIFF_CLASSIFICATION_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.DIFF_CLASSIFICATION_OUTPUT_PATH.value,\n",
    "        DEFAULT_DIFF_CLASSIFICATION_OUTPUT_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commits(path: str) -> list[CommitDataModel]:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            json_string = file.read()\n",
    "\n",
    "        return CommitDataModel.from_json(json_string)\n",
    "\n",
    "COMMITS = get_commits(COMMIT_DATA_JSON_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDED_DIFF_CLASSIFIER_CHAIN_INDEXES = [0, 1, 2, 3]\n",
    "\n",
    "for index in INCLUDED_DIFF_CLASSIFIER_CHAIN_INDEXES:\n",
    "    evaluator.classify_diffs(DIFF_CLASSIFIER_CHAINS[index], COMMITS, CONTEXT_DATA_PATH, DIFF_CLASSIFICATION_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_generator.generate_context(COMMITS, CONTEXT_GENERATION_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get High Level Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.smith.langchain.com:443\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Given a Git diff and the relevant source code, write a concise summary of the code changes in a way that a non-technical person can understand. The query text must summarize the code changes in two very brief sentences.\\n\\nGit diff:\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n\\nSource code:\\ncontrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java (Before)\\npublic class HiveStoragePlugin extends AbstractStoragePlugin {\\n@Override\\npublic Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n    switch(phase) {\\n        case LOGICAL:\\n            final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n            ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnScan(optimizerContext, defaultPartitionValue));\\n            return ruleBuilder.build();\\n        case PHYSICAL:\\n            {\\n                ruleBuilder = ImmutableSet.builder();\\n                OptionManager options = optimizerContext.getPlannerSettings().getOptions();\\n                // TODO: Remove implicit using of convert_fromTIMESTAMP_IMPALA function\\n                // once \"store.parquet.reader.int96_as_timestamp\" will be true by default\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_SCAN_WITH_NATIVE_READERS) || options.getBoolean(ExecConstants.HIVE_OPTIMIZE_PARQUET_SCAN_WITH_NATIVE_READER)) {\\n                    ruleBuilder.add(ConvertHiveParquetScanToDrillParquetScan.INSTANCE);\\n                }\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER)) {\\n                    try {\\n                        Class<?> hiveToDrillMapRDBJsonRuleClass = Class.forName(\"org.apache.drill.exec.planner.sql.logical.ConvertHiveMapRDBJsonScanToDrillMapRDBJsonScan\");\\n                        ruleBuilder.add((StoragePluginOptimizerRule) hiveToDrillMapRDBJsonRuleClass.getField(\"INSTANCE\").get(null));\\n                    } catch (ReflectiveOperationException e) {\\n                        logger.warn(\"Current Drill build is not designed for working with Hive MapR-DB tables. \" + \"Please disable {} option\", ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER);\\n                    }\\n                }\\n                return ruleBuilder.build();\\n            }\\n        default:\\n            return ImmutableSet.of();\\n    }\\n}\\n}\\n\\ncontrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java (After)\\npublic class HiveStoragePlugin extends AbstractStoragePlugin {\\n@Override\\npublic Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n    switch(phase) {\\n        case PARTITION_PRUNING:\\n            final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n            ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnScan(optimizerContext, defaultPartitionValue));\\n            return ruleBuilder.build();\\n        case PHYSICAL:\\n            {\\n                ruleBuilder = ImmutableSet.builder();\\n                OptionManager options = optimizerContext.getPlannerSettings().getOptions();\\n                // TODO: Remove implicit using of convert_fromTIMESTAMP_IMPALA function\\n                // once \"store.parquet.reader.int96_as_timestamp\" will be true by default\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_SCAN_WITH_NATIVE_READERS) || options.getBoolean(ExecConstants.HIVE_OPTIMIZE_PARQUET_SCAN_WITH_NATIVE_READER)) {\\n                    ruleBuilder.add(ConvertHiveParquetScanToDrillParquetScan.INSTANCE);\\n                }\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER)) {\\n                    try {\\n                        Class<?> hiveToDrillMapRDBJsonRuleClass = Class.forName(\"org.apache.drill.exec.planner.sql.logical.ConvertHiveMapRDBJsonScanToDrillMapRDBJsonScan\");\\n                        ruleBuilder.add((StoragePluginOptimizerRule) hiveToDrillMapRDBJsonRuleClass.getField(\"INSTANCE\").get(null));\\n                    } catch (ReflectiveOperationException e) {\\n                        logger.warn(\"Current Drill build is not designed for working with Hive MapR-DB tables. \" + \"Please disable {} option\", ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER);\\n                    }\\n                }\\n                return ruleBuilder.build();\\n            }\\n        default:\\n            return ImmutableSet.of();\\n    }\\n}\\n}\\n\\ncontrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java (Before)\\nimport org.apache.drill.exec.rpc.user.QueryDataBatch;\\nimport org.junit.AfterClass;\\nimport org.junit.BeforeClass;\\nimport org.junit.Ignore;\\nimport org.junit.Test;\\nimport org.junit.experimental.categories.Category;\\npublic class TestHivePartitionPruning extends HiveTestBase {\\n// DRILL-5032\\n@Test\\npublic void testPartitionColumnsCaching() throws Exception {\\n    final String query = \"EXPLAIN PLAN FOR SELECT * FROM hive.partition_with_few_schemas\";\\n    List<QueryDataBatch> queryDataBatches = testSqlWithResults(query);\\n    String resultString = getResultString(queryDataBatches, \"|\");\\n    // different for both partitions column strings from physical plan\\n    String columnString = \"\\\\\"name\\\\\" : \\\\\"a\\\\\"\";\\n    String secondColumnString = \"\\\\\"name\\\\\" : \\\\\"a1\\\\\"\";\\n    int columnIndex = resultString.indexOf(columnString);\\n    assertTrue(columnIndex >= 0);\\n    columnIndex = resultString.indexOf(columnString, columnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, columnIndex);\\n    int secondColumnIndex = resultString.indexOf(secondColumnString);\\n    assertTrue(secondColumnIndex >= 0);\\n    secondColumnIndex = resultString.indexOf(secondColumnString, secondColumnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, secondColumnIndex);\\n}\\n// DRILL-6173\\n@Test\\n@Ignore(\"DRILL-8400\")\\npublic void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" + \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" + \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n    int actualRowCount = testSql(query);\\n    int expectedRowCount = 450;\\n    assertEquals(\"Expected and actual row count should match\", expectedRowCount, actualRowCount);\\n    final String[] expectedPlan = { \"partition_with_few_schemas.*numPartitions=6\", \"partition_pruning_test.*numPartitions=6\" };\\n    testPlanMatchingPatterns(query, expectedPlan);\\n}\\n}\\n\\ncontrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java (After)\\nimport org.apache.drill.exec.rpc.user.QueryDataBatch;\\nimport org.junit.AfterClass;\\nimport org.junit.BeforeClass;\\nimport org.junit.Test;\\nimport org.junit.experimental.categories.Category;\\npublic class TestHivePartitionPruning extends HiveTestBase {\\n// DRILL-5032\\n@Test\\npublic void testPartitionColumnsCaching() throws Exception {\\n    final String query = \"EXPLAIN PLAN FOR SELECT * FROM hive.partition_with_few_schemas\";\\n    List<QueryDataBatch> queryDataBatches = testSqlWithResults(query);\\n    String resultString = getResultString(queryDataBatches, \"|\");\\n    // different for both partitions column strings from physical plan\\n    String columnString = \"\\\\\"name\\\\\" : \\\\\"a\\\\\"\";\\n    String secondColumnString = \"\\\\\"name\\\\\" : \\\\\"a1\\\\\"\";\\n    int columnIndex = resultString.indexOf(columnString);\\n    assertTrue(columnIndex >= 0);\\n    columnIndex = resultString.indexOf(columnString, columnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, columnIndex);\\n    int secondColumnIndex = resultString.indexOf(secondColumnString);\\n    assertTrue(secondColumnIndex >= 0);\\n    secondColumnIndex = resultString.indexOf(secondColumnString, secondColumnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, secondColumnIndex);\\n}\\n// DRILL-6173\\n@Test\\npublic void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" + \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" + \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n    int actualRowCount = testSql(query);\\n    int expectedRowCount = 450;\\n    assertEquals(\"Expected and actual row count should match\", expectedRowCount, actualRowCount);\\n    final String[] expectedPlan = { \"partition_with_few_schemas.*numPartitions=6\", \"partition_pruning_test.*numPartitions=6\" };\\n    testPlanMatchingPatterns(query, expectedPlan);\\n}\\n}\\n\\n\\nQuery text:', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C99CDB6660>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C99AAD75C0> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C99CDAC190>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"GET /info HTTP/11\" 200 672\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 33\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 08 Feb 2025 10:53:11 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'1796'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'197098'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'870ms'), (b'x-request-id', b'req_eebbca5b7f3a1a9d3715321d5e68582d'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=cteFlgltfkjQKggiMCvWAOiASubl1l7RxIBPssnDDHg-1739011991-1.0.1.1-.BAhcoqqxYi9RQoRAkj_0JekDKan8sAC015xaRhu21QkRgm4ZvsowXK7l0UnXxOu70Plzq2zT_.bNSNWtm9DGQ; path=/; expires=Sat, 08-Feb-25 11:23:11 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=KYk7jheAwleDeEvNiMUd..3HIhX_He0bqtwZ3NzNQU0-1739011991996-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'90eb1308ca60fd06-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 08 Feb 2025 10:53:11 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '1796'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '197098'), ('x-ratelimit-reset-requests', '8.64s'), ('x-ratelimit-reset-tokens', '870ms'), ('x-request-id', 'req_eebbca5b7f3a1a9d3715321d5e68582d'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=cteFlgltfkjQKggiMCvWAOiASubl1l7RxIBPssnDDHg-1739011991-1.0.1.1-.BAhcoqqxYi9RQoRAkj_0JekDKan8sAC015xaRhu21QkRgm4ZvsowXK7l0UnXxOu70Plzq2zT_.bNSNWtm9DGQ; path=/; expires=Sat, 08-Feb-25 11:23:11 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=KYk7jheAwleDeEvNiMUd..3HIhX_He0bqtwZ3NzNQU0-1739011991996-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '90eb1308ca60fd06-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_eebbca5b7f3a1a9d3715321d5e68582d\n",
      "DEBUG:faiss.loader:Environment variable FAISS_OPT_LEVEL is not set, so let's pick the instruction set according to the current CPU\n",
      "INFO:faiss.loader:Loading faiss with AVX512 support.\n",
      "INFO:faiss.loader:Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x000002C99DD019E0>, 'json_data': {'input': [[791, 2082, 4442, 2713, 264, 3857, 315, 279, 2068, 430, 29972, 1268, 828, 374, 18797, 994, 82198, 264, 4729, 11, 18899, 1202, 15374, 555, 21760, 389, 3230, 828, 47788, 13, 23212, 11, 264, 1296, 1162, 574, 11041, 311, 20891, 264, 8767, 12305, 1296, 11, 23391, 430, 279, 2068, 12722, 13777, 828, 17071, 287, 3196, 389, 3738, 4787, 13]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C99CDAF4D0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C99AAD79B0> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C99D7DE2C0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 33\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 08 Feb 2025 10:53:12 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'200'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-f75cbbb97-hwxhz'), (b'x-envoy-upstream-service-time', b'60'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999942'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'req_6de4e2617c4f7281d69b326a0733d973'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Pz3ziE_f864YxmKhKpgser84MOR4MDyPp_AYIf30LmU-1739011992-1.0.1.1-CGtBiPM5yRKz2U_VTG8GKMaeMqnwriM7I6QP5JQpiMYV2gK6WULCmdQm7r1SoQN7zQ8RB8vDT7Wgbn1USS30ZQ; path=/; expires=Sat, 08-Feb-25 11:23:12 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=Rly0h2r1.Ccg4XPAKCSGVlk3oQr1RnMa9vkoFzw2tjM-1739011992901-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'90eb1317aef7fd32-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Sat, 08 Feb 2025 10:53:12 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '200'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-f75cbbb97-hwxhz'), ('x-envoy-upstream-service-time', '60'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999942'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '3ms'), ('x-request-id', 'req_6de4e2617c4f7281d69b326a0733d973'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=Pz3ziE_f864YxmKhKpgser84MOR4MDyPp_AYIf30LmU-1739011992-1.0.1.1-CGtBiPM5yRKz2U_VTG8GKMaeMqnwriM7I6QP5JQpiMYV2gK6WULCmdQm7r1SoQN7zQ8RB8vDT7Wgbn1USS30ZQ; path=/; expires=Sat, 08-Feb-25 11:23:12 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=Rly0h2r1.Ccg4XPAKCSGVlk3oQr1RnMa9vkoFzw2tjM-1739011992901-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '90eb1317aef7fd32-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_6de4e2617c4f7281d69b326a0733d973\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8400\\nIssue Summary: Fix pruning partitions with pushed transitive predicates\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nSee {{TestHivePartitionPruning.prunePartitionsBasedOnTransitivePredicates()}} test for details.\\n\\n\\n\\nThe issue occurs for queries like these:\\n\\n{code:sql}\\n\\nSELECT * FROM hive.partition_pruning_test t1 \\n\\nJOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \\n\\nWHERE t2.`e` IS NOT NULL AND t1.`d` = 1\\n\\n{code}\\n\\n\\n\\nThe expected behavior is to create additional filters based on the existing filters and join conditions. We have a {{TRANSITIVE_CLOSURE}} planning phase, which is responsible for such query transformations, but Drill pushes down filters from the WHERE condition before that phase, so the optimization is not performed.\\n\\n\\n\\nIdeally, we should move rules from the {{TRANSITIVE_CLOSURE}} phase to the {{LOGICAL}} phase so that the planner will choose the most optimal plan, but it wouldn\\'t help until CALCITE-1048 is fixed (it is required to pull predicates when three has {{RelSubset}} nodes).\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8381\\nIssue Summary: Add support for filtered aggregate calls\\nIssue Type: New Feature\\nPriority: Major\\n\\nDescription:\\nCurrently, Drill ignores filters for filtered aggregate calls and returns incorrect results.\\n\\nHere is the example query for which Drill will return incorrect results:\\n\\n{code:sql}\\n\\nSELECT count(n_name) FILTER(WHERE n_regionkey = 1) AS nations_count_in_1_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 2) AS nations_count_in_2_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 3) AS nations_count_in_3_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 4) AS nations_count_in_4_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 0) AS nations_count_in_0_region\\n\\nFROM cp.`tpch/nation.parquet`\\n\\n{code}\\n\\n{noformat}\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| nations_count_in_1_region | nations_count_in_2_region | nations_count_in_3_region | nations_count_in_4_region | nations_count_in_0_region |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| 25                        | 25                        | 25                        | 25                        | 25                        |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n{noformat}\\n\\nBut the correct result is\\n\\n{noformat}\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| nations_count_in_1_region | nations_count_in_2_region | nations_count_in_3_region | nations_count_in_4_region | nations_count_in_0_region |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| 5                         | 5                         | 5                         | 5                         | 5                         |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n{noformat}\\n\\nSide note:\\n\\nThe query above could be rewritten using PIVOT:\\n\\n{code:sql}\\n\\nSELECT `1` nations_count_in_1_region, `2` nations_count_in_2_region, `3` nations_count_in_3_region, `4` nations_count_in_4_region, `0` nations_count_in_0_region\\n\\nFROM (SELECT n_name, n_regionkey FROM cp.`tpch/nation.parquet`) \\n\\nPIVOT(count(n_name) FOR n_regionkey IN (0, 1, 2, 3, 4))\\n\\n{code}\\n\\nAnd will return correct results when this issue is fixed and Calcite is updated to 1.33.0\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8513\\nIssue Summary: Right Hash Join with empty Left table ruturns 0 result\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nDrill returns no results on the right Hash Join if the probe(left) table is empty.\\n\\n\\n\\nThe simplest way to reproduce the issue:\\n\\n\\n\\n1.To force Drill not to use merge join and use the hash join operator instead:\\n\\n{code:java}\\n\\nalter session set planner.enable_mergejoin = false;\\n\\nalter session set planner.enable_nestedloopjoin= false; {code}\\n\\n2. Disable join order optimization to prevent Drill from flipping join tables:\\n\\n{code:java}\\n\\nalter session set planner.enable_join_optimization = false;  {code}\\n\\n3. Execute a query with empty left table outcome:\\n\\n{code:java}\\n\\nSELECT *\\n\\nFROMÂ\\xa0\\n\\nÂ\\xa0 Â\\xa0 (SELECT * FROM (VALUES (1, \\'Max\\', 28),Â\\xa0\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0(2, \\'Jane\\', 32),\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0(3, \\'Saymon\\', 29)\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0) AS users(id, name, age)\\n\\nÂ\\xa0 Â\\xa0 WHERE false\\n\\nÂ\\xa0 Â\\xa0 ) AS users\\n\\nRIGHT JOINÂ\\xa0\\n\\nÂ\\xa0 Â\\xa0 (VALUES (1, \\'Engineer\\'),Â\\xa0\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 (2, \\'Doctor\\'),Â\\xa0\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 (3, \\'Teacher\\')\\n\\nÂ\\xa0 Â\\xa0 ) AS job(id, title)\\n\\nON users.id = job.idÂ\\xa0{code}\\n\\nExpected result is:\\n\\n||id||name||age||id0||title||\\n\\n|null|null|null|1|Engineer|\\n\\n|null|null|null|2|Doctor|\\n\\n|null|null|null|3|Teacher|\\n\\n\\n\\nBut we get 0 rows.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-4935\\nIssue Summary: Allow drillbits to advertise a configurable host address to Zookeeper\\nIssue Type: New Feature\\nPriority: Minor\\n\\nDescription:\\nThere are certain situations, such as running Drill in distributed Docker containers, in which it is desirable to advertise a different hostname to Zookeeper than would be output by INetAddress.getLocalHost().  I propose adding a configuration variable \\'drill.exec.rpc.bit.advertised.host\\' and passing this address to Zookeeper when the configuration variable is populated, otherwise falling back to the present behavior.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context directly or indirectly correlates with the changes in the Git diff. Otherwise, return NO. Avoid adding any additional comments or annotations to the classification.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8503\\nIssue Summary: Add Configuration Option to Skip Host Validation for Splunk\\nIssue Type: Improvement\\nPriority: Major\\n\\nDescription:\\nThis PR adds an option to skip host validation for SSL connections to Splunk.Â\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C99D7DCD60>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C98031B9B0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C99AAD7890> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C99AAD7890> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C99CCBFCE0>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C99CCBE470>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C99AAD7890> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C980352C50>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C99AAD7890> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C99AAD7890> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C980321D50>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C99D8009B0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C99D801E50>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C9F69F1D30>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C9F69F2190>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 33\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 08 Feb 2025 10:53:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'199'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'199018'), (b'x-ratelimit-reset-requests', b'14.108s'), (b'x-ratelimit-reset-tokens', b'294ms'), (b'x-request-id', b'req_90803dbe59989edbc7cd13083c65b1ee'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=TBsBqKW_kgL8b9dNw04Ui3h3PcAih0B9pqN1ZCfDwtM-1739011993-1.0.1.1-9ez1.MedAtc_3knzTi991gXkI8_lVWWs6_ajAlFRLdhr53jcynJnPP60yrvqAe4u08cyEopQvj.9DbMdNadpSw; path=/; expires=Sat, 08-Feb-25 11:23:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=LLghbNOXSiePpEaji6LCRDlNonRQyKSUoswgc0jX3hw-1739011993582-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'90eb131cdafe4095-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 08 Feb 2025 10:53:13 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '199'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9998'), ('x-ratelimit-remaining-tokens', '199018'), ('x-ratelimit-reset-requests', '14.108s'), ('x-ratelimit-reset-tokens', '294ms'), ('x-request-id', 'req_90803dbe59989edbc7cd13083c65b1ee'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=TBsBqKW_kgL8b9dNw04Ui3h3PcAih0B9pqN1ZCfDwtM-1739011993-1.0.1.1-9ez1.MedAtc_3knzTi991gXkI8_lVWWs6_ajAlFRLdhr53jcynJnPP60yrvqAe4u08cyEopQvj.9DbMdNadpSw; path=/; expires=Sat, 08-Feb-25 11:23:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=LLghbNOXSiePpEaji6LCRDlNonRQyKSUoswgc0jX3hw-1739011993582-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '90eb131cdafe4095-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_90803dbe59989edbc7cd13083c65b1ee\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 08 Feb 2025 10:53:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'219'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9996'), (b'x-ratelimit-remaining-tokens', b'196673'), (b'x-ratelimit-reset-requests', b'31.35s'), (b'x-ratelimit-reset-tokens', b'997ms'), (b'x-request-id', b'req_730bb02d32cdde7cbbb41655f635225b'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=SlSmG.EOKizrH.awbhrFYIDkloojHTbjppuk2HiQgJ4-1739011993-1.0.1.1-DHFFjydk8djOTk8CTFjAXUhoZjsJJHYjDbR4v0xn7vZM1QcfloVC7EOfo3FL7KUgSVJpYnQHY8A12TbQta9MaA; path=/; expires=Sat, 08-Feb-25 11:23:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=VoJQgCZduS_PwYdNn2IvPADj2DsePyQjrdsxK94L9Jg-1739011993623-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'90eb131d0bafce81-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 08 Feb 2025 10:53:13 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '219'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9996'), ('x-ratelimit-remaining-tokens', '196673'), ('x-ratelimit-reset-requests', '31.35s'), ('x-ratelimit-reset-tokens', '997ms'), ('x-request-id', 'req_730bb02d32cdde7cbbb41655f635225b'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=SlSmG.EOKizrH.awbhrFYIDkloojHTbjppuk2HiQgJ4-1739011993-1.0.1.1-DHFFjydk8djOTk8CTFjAXUhoZjsJJHYjDbR4v0xn7vZM1QcfloVC7EOfo3FL7KUgSVJpYnQHY8A12TbQta9MaA; path=/; expires=Sat, 08-Feb-25 11:23:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=VoJQgCZduS_PwYdNn2IvPADj2DsePyQjrdsxK94L9Jg-1739011993623-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '90eb131d0bafce81-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_730bb02d32cdde7cbbb41655f635225b\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 08 Feb 2025 10:53:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'222'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9995'), (b'x-ratelimit-remaining-tokens', b'195910'), (b'x-ratelimit-reset-requests', b'39.987s'), (b'x-ratelimit-reset-tokens', b'1.226s'), (b'x-request-id', b'req_d05628896dfe4b698f81fc92b36ec680'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=f4wIFz_7b87zwXj3ooZ0ZgbDiO2cCtEsQh37OSasUTI-1739011993-1.0.1.1-.nV3mWR1ZpoFq2bUu3nrgyh9sEVYUbkv8KDvi5Dhc6ksN2XuB.t9PG7bwgbj3CSqBvVyzhYyu0ynRkRJy5_zvw; path=/; expires=Sat, 08-Feb-25 11:23:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=6vLi5Ej0ywW6dntRz.MOUiq0Ub7F_prz0XlLtzBSalU-1739011993632-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'90eb131d0b638813-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 08 Feb 2025 10:53:13 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '222'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9995'), ('x-ratelimit-remaining-tokens', '195910'), ('x-ratelimit-reset-requests', '39.987s'), ('x-ratelimit-reset-tokens', '1.226s'), ('x-request-id', 'req_d05628896dfe4b698f81fc92b36ec680'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=f4wIFz_7b87zwXj3ooZ0ZgbDiO2cCtEsQh37OSasUTI-1739011993-1.0.1.1-.nV3mWR1ZpoFq2bUu3nrgyh9sEVYUbkv8KDvi5Dhc6ksN2XuB.t9PG7bwgbj3CSqBvVyzhYyu0ynRkRJy5_zvw; path=/; expires=Sat, 08-Feb-25 11:23:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=6vLi5Ej0ywW6dntRz.MOUiq0Ub7F_prz0XlLtzBSalU-1739011993632-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '90eb131d0b638813-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_d05628896dfe4b698f81fc92b36ec680\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 08 Feb 2025 10:53:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'256'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9997'), (b'x-ratelimit-remaining-tokens', b'197706'), (b'x-ratelimit-reset-requests', b'22.724s'), (b'x-ratelimit-reset-tokens', b'687ms'), (b'x-request-id', b'req_b1506263c1acfdfdb78c814c2a18b940'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=0309NiiANUXmsThfu2brcWDtal.WWyLPMnOUcnnrRBs-1739011993-1.0.1.1-hMwDdkBvsaKhdtrUV2_fHsSA9lVCMMrZpDpDNyfGlHuYcXfHt_oWyiGqsYv5WBvX2G6UfmSfRfaLE_LM5beKJg; path=/; expires=Sat, 08-Feb-25 11:23:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=7ReyvsVGsJa2aXAveRpZT290_U2ey8hfJ.7gPAyFuHM-1739011993658-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'90eb131cea884094-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 08 Feb 2025 10:53:13 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '256'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9997'), ('x-ratelimit-remaining-tokens', '197706'), ('x-ratelimit-reset-requests', '22.724s'), ('x-ratelimit-reset-tokens', '687ms'), ('x-request-id', 'req_b1506263c1acfdfdb78c814c2a18b940'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=0309NiiANUXmsThfu2brcWDtal.WWyLPMnOUcnnrRBs-1739011993-1.0.1.1-hMwDdkBvsaKhdtrUV2_fHsSA9lVCMMrZpDpDNyfGlHuYcXfHt_oWyiGqsYv5WBvX2G6UfmSfRfaLE_LM5beKJg; path=/; expires=Sat, 08-Feb-25 11:23:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=7ReyvsVGsJa2aXAveRpZT290_U2ey8hfJ.7gPAyFuHM-1739011993658-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '90eb131cea884094-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_b1506263c1acfdfdb78c814c2a18b940\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 33\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 08 Feb 2025 10:53:14 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'241'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9994'), (b'x-ratelimit-remaining-tokens', b'196722'), (b'x-ratelimit-reset-requests', b'48.141s'), (b'x-ratelimit-reset-tokens', b'983ms'), (b'x-request-id', b'req_6dc31129026267513962107a73855d66'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=E38nS.vUoUI4HFPyUqEtTF27Al3cVk58TjgOZeTBBzo-1739011994-1.0.1.1-Uc_lpe1jk3N.4xEontVk4ErCq3IuKLrJnVw3Jvrf8ZoMlWa5RMPUSNn_42yIg1u2M4u2e8JLWHMbqmjvESa2bw; path=/; expires=Sat, 08-Feb-25 11:23:14 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=FlwKyF_amS5XYz9UL35.vpu5sjFZvcA_Jk2skcPh2kk-1739011994147-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'90eb131cfeb36bd5-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 08 Feb 2025 10:53:14 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '241'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9994'), ('x-ratelimit-remaining-tokens', '196722'), ('x-ratelimit-reset-requests', '48.141s'), ('x-ratelimit-reset-tokens', '983ms'), ('x-request-id', 'req_6dc31129026267513962107a73855d66'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=E38nS.vUoUI4HFPyUqEtTF27Al3cVk58TjgOZeTBBzo-1739011994-1.0.1.1-Uc_lpe1jk3N.4xEontVk4ErCq3IuKLrJnVw3Jvrf8ZoMlWa5RMPUSNn_42yIg1u2M4u2e8JLWHMbqmjvESa2bw; path=/; expires=Sat, 08-Feb-25 11:23:14 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=FlwKyF_amS5XYz9UL35.vpu5sjFZvcA_Jk2skcPh2kk-1739011994147-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '90eb131cfeb36bd5-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_6dc31129026267513962107a73855d66\n"
     ]
    }
   ],
   "source": [
    "INCLUDED_HIGH_LEVEL_CONTEXT_CHAIN_INDEXES = [0, 1]\n",
    "\n",
    "for index in INCLUDED_HIGH_LEVEL_CONTEXT_CHAIN_INDEXES:\n",
    "    evaluator.get_high_level_contexts(\n",
    "        HIGH_LEVEL_CONTEXT_CHAINS[index],\n",
    "        COMMITS, \n",
    "        CONTEXT_DATA_PATH, \n",
    "        HIGH_LEVEL_CONTEXT_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Commit Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDED_GENERATOR_INDEXES = [0, 1]\n",
    "\n",
    "filtered_generators = [GENERATORS[i] for i in INCLUDED_GENERATOR_INDEXES]\n",
    "evaluator.evaluate(filtered_generators, COMMITS, CONTEXT_DATA_PATH, CMG_OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
