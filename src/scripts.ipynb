{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", verbose=True, override=True)\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from typing import Any\n",
    "import jsonpickle\n",
    "import statistics\n",
    "\n",
    "from autocommit_evaluation.core.enums import EnvironmentKey\n",
    "from autocommit_evaluation.cmg.evaluators import CommitMessageGenerator\n",
    "from autocommit_evaluation.cmg import evaluator\n",
    "from autocommit_evaluation.core import (\n",
    "    main_few_shot_high_level_context_cmg_chain,\n",
    "    main_zero_shot_low_level_context_cmg_chain,\n",
    "    main_few_shot_low_level_context_cmg_chain,\n",
    "    main_zero_shot_high_level_context_cmg_chain,\n",
    "    main_high_level_context_chain\n",
    ")\n",
    "from autocommit.core.models import CommitDataModel\n",
    "from autocommit_evaluation.datapreparation import context_generator, example_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMIT_DATA_JSON_FILE_PATH = os.path.join(\"autocommit_evaluation\", \"data\", \"cmg\", \"commits.json\")\n",
    "EVALUATION_COMMIT_DATA_JSON_FILE_PATH = os.path.join(\"autocommit_evaluation\", \"data\", \"cmg\", \"commits.evaluation.json\")\n",
    "TEST_COMMIT_DATA_JSON_FILE_PATH = os.path.join(\"autocommit_evaluation\", \"data\", \"cmg\", \"commits.test.json\")\n",
    "EXAMPLE_DATA_JSON_FILE_PATH = os.path.join(\"autocommit_evaluation\", \"data\", \"cmg\", \"commits.example.json\")\n",
    "RESULT_DATA_JSON_FILE_PATH = os.path.join(\"autocommit_evaluation\", \"data\", \"result\", \"evaluation.json\")\n",
    "SCORE_DATA_JSON_FILE_PATH = os.path.join(\"autocommit_evaluation\", \"data\", \"result\", \"score.json\")\n",
    "\n",
    "CONTEXT_DATA_PATH = os.path.join(\"autocommit_evaluation\",\"data\", \"context\")\n",
    "\n",
    "DEFAULT_CONTEXT_GENERATION_OUTPUT_PATH = os.path.join(\n",
    "    \"autocommit_evaluation\", \"data\", \"context\"\n",
    ")\n",
    "DEFAULT_HIGH_LEVEL_CONTEXT_OUTPUT_PATH = os.path.join(\n",
    "    \"out\", \"result\", \"highlevelcontext\"\n",
    ")\n",
    "DEFAULT_CMG_OUTPUT_PATH = os.path.join(\"out\", \"result\", \"cmg\")\n",
    "DEFAULT_DIFF_CLASSIFICATION_OUTPUT_PATH = os.path.join(\n",
    "    \"out\", \"result\", \"diffclassification\"\n",
    ")\n",
    "DEFAULT_EXAMPLE_GENERATION_OUTPUT_PATH = os.path.join(\"out\", \"result\", \"example\")\n",
    "DEFAULT_CLEANING_RESULT_OUTPUT_PATH = os.path.join(\"autocommit_evaluation\", \"data\", \"result\", \"evaluation.cleaned.json\")\n",
    "DEFAULT_SCORE_SUMMARY_OUTPUT_PATH = os.path.join(\"autocommit_evaluation\", \"data\", \"result\", \"score.summary.json\")\n",
    "\n",
    "DIFF_CLASSIFIER_CHAINS = [\n",
    "    main_zero_shot_low_level_context_cmg_chain,\n",
    "    main_zero_shot_high_level_context_cmg_chain,\n",
    "]\n",
    "\n",
    "HIGH_LEVEL_CONTEXT_CHAINS = [\n",
    "    main_high_level_context_chain,\n",
    "]\n",
    "\n",
    "GENERATORS = [\n",
    "    CommitMessageGenerator(\n",
    "        \"Main Few-Shot Low-Level Context Generator\", main_few_shot_low_level_context_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\n",
    "        \"Main Zero-Shot High-Level Context Generator\", main_zero_shot_high_level_context_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\n",
    "        \"Main Few-Shot High-Level Context Generator\", main_few_shot_high_level_context_cmg_chain\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_GENERATION_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.CONTEXT_GENERATION_OUTPUT_PATH.value,\n",
    "        DEFAULT_CONTEXT_GENERATION_OUTPUT_PATH,\n",
    "    )\n",
    "\n",
    "HIGH_LEVEL_CONTEXT_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.HIGH_LEVEL_CONTEXT_OUTPUT_PATH.value,\n",
    "        DEFAULT_HIGH_LEVEL_CONTEXT_OUTPUT_PATH,\n",
    "    )\n",
    "\n",
    "CMG_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.CMG_OUTPUT_PATH.value, DEFAULT_CMG_OUTPUT_PATH\n",
    "    )\n",
    "\n",
    "DIFF_CLASSIFICATION_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.DIFF_CLASSIFICATION_OUTPUT_PATH.value,\n",
    "        DEFAULT_DIFF_CLASSIFICATION_OUTPUT_PATH,\n",
    "    )\n",
    "\n",
    "EXAMPLE_GENERATION_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.EXAMPLE_GENERATION_OUTPUT_PATH.value,\n",
    "        DEFAULT_EXAMPLE_GENERATION_OUTPUT_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commits(path: str) -> list[CommitDataModel]:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            json_string = file.read()\n",
    "\n",
    "        return CommitDataModel.from_json(json_string)\n",
    "\n",
    "COMMITS = get_commits(COMMIT_DATA_JSON_FILE_PATH)\n",
    "EVALUATION_COMMITS = get_commits(EVALUATION_COMMIT_DATA_JSON_FILE_PATH)\n",
    "TEST_COMMITS = get_commits(TEST_COMMIT_DATA_JSON_FILE_PATH)\n",
    "EXAMPLE_COMMITS = get_commits(EXAMPLE_DATA_JSON_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_commits = COMMITS + EVALUATION_COMMITS + TEST_COMMITS + EXAMPLE_COMMITS\n",
    "# repo_name_filters = [\"camel\"]\n",
    "\n",
    "# context_generator.generate_context(all_commits, CONTEXT_GENERATION_OUTPUT_PATH, repo_name_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): issues.apache.org:443\n",
      "DEBUG:urllib3.connectionpool:https://issues.apache.org:443 \"GET /jira/rest/api/2/serverInfo HTTP/11\" 200 229\n",
      "DEBUG:urllib3.connectionpool:https://issues.apache.org:443 \"GET /jira/rest/api/2/issue/HADOOP-12657 HTTP/11\" 200 None\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): issues.apache.org:443\n",
      "DEBUG:urllib3.connectionpool:https://issues.apache.org:443 \"GET /jira/rest/api/2/serverInfo HTTP/11\" 200 229\n",
      "DEBUG:urllib3.connectionpool:https://issues.apache.org:443 \"GET /jira/rest/api/2/issue/AMQ-6894 HTTP/11\" 200 None\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): issues.apache.org:443\n",
      "DEBUG:urllib3.connectionpool:https://issues.apache.org:443 \"GET /jira/rest/api/2/serverInfo HTTP/11\" 200 None\n",
      "DEBUG:urllib3.connectionpool:https://issues.apache.org:443 \"GET /jira/rest/api/2/issue/CASSANDRA-20188 HTTP/11\" 200 None\n"
     ]
    }
   ],
   "source": [
    "example_generator.generate_examples(EXAMPLE_COMMITS, EXAMPLE_GENERATION_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): issues.apache.org:443\n",
      "DEBUG:urllib3.connectionpool:https://issues.apache.org:443 \"GET /jira/rest/api/2/serverInfo HTTP/11\" 200 None\n",
      "DEBUG:urllib3.connectionpool:https://issues.apache.org:443 \"GET /jira/rest/api/2/issue/HADOOP-12657 HTTP/11\" 200 None\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): issues.apache.org:443\n",
      "DEBUG:urllib3.connectionpool:https://issues.apache.org:443 \"GET /jira/rest/api/2/serverInfo HTTP/11\" 200 229\n",
      "DEBUG:urllib3.connectionpool:https://issues.apache.org:443 \"GET /jira/rest/api/2/issue/AMQ-6894 HTTP/11\" 200 None\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): issues.apache.org:443\n",
      "DEBUG:urllib3.connectionpool:https://issues.apache.org:443 \"GET /jira/rest/api/2/serverInfo HTTP/11\" 200 229\n",
      "DEBUG:urllib3.connectionpool:https://issues.apache.org:443 \"GET /jira/rest/api/2/issue/CASSANDRA-20188 HTTP/11\" 200 None\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "\n",
    "cProfile.run(\n",
    "    \"example_generator.generate_examples(EXAMPLE_COMMITS, EXAMPLE_GENERATION_OUTPUT_PATH)\",\n",
    "    \"profile_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Commit Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator.evaluate(GENERATORS, COMMITS, CONTEXT_DATA_PATH, CMG_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMG Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_commit_subject_length(commit_message: str):\n",
    "#     return len(commit_message.split(\"\\n\")[0])\n",
    "\n",
    "# data = None\n",
    "\n",
    "# with open(RESULT_DATA_JSON_FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "#     json_string = file.read()\n",
    "#     data = json.loads(json_string)\n",
    "\n",
    "# random_state = random.getstate()\n",
    "\n",
    "# for commit in data:\n",
    "\n",
    "#     commit[\"generation_results\"] = [\n",
    "#         result for result in commit[\"generation_results\"] if result[\"generator_id\"] != \"Main Zero-Shot Low-Level Context Generator\"\n",
    "#     ]\n",
    "    \n",
    "#     for result in commit[\"generation_results\"]:    \n",
    "#         commit_message = result.get(\"cleaned_commit_message\") or result[\"commit_message\"]\n",
    "#         result[\"commit_subject_length\"] = calculate_commit_subject_length(commit_message)\n",
    "\n",
    "#     seed_value = int(commit[\"evaluation_id\"][2:]) + 42\n",
    "#     random.seed(seed_value)\n",
    "#     random.shuffle(commit[\"generation_results\"])\n",
    "    \n",
    "# random.setstate(random_state)\n",
    "\n",
    "# with open(DEFAULT_CLEANING_RESULT_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as file:\n",
    "#     json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Result Processing\n",
    "\n",
    "# class CommitMessageScore:\n",
    "#     def __init__(self):\n",
    "#         self.rationality_score: int = 0\n",
    "#         self.comprehensiveness_score: int = 0\n",
    "#         self.conciseness_score: int = 0\n",
    "#         self.correctness_score: int = 0\n",
    "\n",
    "# class GeneratorScore:\n",
    "#     def __init__(self):\n",
    "#         self.generator_id: str = \"\"\n",
    "#         self.scores: list[CommitMessageScore] = []\n",
    "\n",
    "# class TestCaseScore:\n",
    "#     def __init__(self):\n",
    "#         self.evaluation_id: str = \"\"\n",
    "#         self.scores: list[GeneratorScore] = []\n",
    "\n",
    "# class ScoreSummary:\n",
    "#     def __init__(self):\n",
    "#         self.generator_id: str = \"\"\n",
    "#         self.rationality_score: float = 0\n",
    "#         self.comprehensiveness_score: float = 0\n",
    "#         self.conciseness_score: float = 0\n",
    "#         self.correctness_score: float = 0\n",
    "\n",
    "# def json_to_object(name: str, data: Any) -> Any:\n",
    "#     if isinstance(data, dict):\n",
    "#         return type(name, (object,), {k: json_to_object(k, v) for k, v in data.items()})()\n",
    "#     elif isinstance(data, list):\n",
    "#         return [json_to_object(name, item) for item in data]\n",
    "#     else:\n",
    "#         return data\n",
    "\n",
    "# def is_rationality_score_valid(\n",
    "#         commit_message_score: CommitMessageScore, commit_message: str) -> bool:\n",
    "#     jira_ticket_pattern = r'\\b[A-Z]+-\\d+\\b'\n",
    "\n",
    "#     if commit_message_score.rationality_score == 3:\n",
    "#         if re.search(jira_ticket_pattern, commit_message):\n",
    "#             return False\n",
    "\n",
    "#     elif commit_message_score.rationality_score == 4:\n",
    "#         if not re.search(jira_ticket_pattern, commit_message):\n",
    "#             return False\n",
    "\n",
    "#     return True\n",
    "\n",
    "# def is_conciseness_score_valid(\n",
    "#         commit_message_score: CommitMessageScore, commit_subject_length: int) -> bool:\n",
    "#     if commit_message_score.conciseness_score != 1:\n",
    "#         if (commit_subject_length > 100):\n",
    "#             return False\n",
    "    \n",
    "#     return True\n",
    "\n",
    "# def is_correctness_score_valid(\n",
    "#         commit_message_score: CommitMessageScore, \n",
    "#         commit_message: str,\n",
    "#         jira_url: str) -> bool:\n",
    "    \n",
    "#     ground_truth_ticket_id = jira_url.split(\"/\")[-1]\n",
    "#     ticket_ids = re.findall(r'\\b[A-Z]+-\\d+\\b', commit_message)\n",
    "\n",
    "#     if commit_message_score.correctness_score == 4:\n",
    "#         if (len(ticket_ids) > 1 \n",
    "#             or (len(ticket_ids) == 1 and ticket_ids[0] != ground_truth_ticket_id)):\n",
    "#             return False\n",
    "\n",
    "#     return True\n",
    "\n",
    "# def clean_scores(test_case_scores: list[TestCaseScore]) -> list[TestCaseScore]:\n",
    "#     print(\"Cleaning scores...\")\n",
    "\n",
    "#     data: list[Any] = None\n",
    "#     cleaned_test_case_scores: list[TestCaseScore] = []\n",
    "#     total_individual_responses = 0\n",
    "#     total_invalid_individual_responses = 0\n",
    "\n",
    "#     with open(DEFAULT_CLEANING_RESULT_OUTPUT_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "#         json_string = file.read()\n",
    "#         data = json.loads(json_string)\n",
    "\n",
    "#     for test_case_score in test_case_scores:\n",
    "#         commit = next((commit for commit in data if commit[\"evaluation_id\"] == test_case_score.evaluation_id), None)\n",
    "\n",
    "#         if commit is None:\n",
    "#             continue\n",
    "        \n",
    "#         invalid_indexes = set()\n",
    "\n",
    "#         total_individual_responses += len(test_case_score.scores[0].scores)\n",
    "\n",
    "#         for generator_score in test_case_score.scores:\n",
    "#             for idx, commit_message_score in enumerate(generator_score.scores):\n",
    "#                 commit_message = next((result for result in commit[\"generation_results\"] if result[\"generator_id\"] == generator_score.generator_id), None)\n",
    "\n",
    "#                 if commit_message is None:\n",
    "#                     continue\n",
    "\n",
    "#                 is_rationality_valid = is_rationality_score_valid(\n",
    "#                     commit_message_score, \n",
    "#                     commit_message.get(\"cleaned_commit_message\") or commit_message[\"commit_message\"])\n",
    "#                 is_comprehensiveness_valid = True\n",
    "#                 is_conciseness_valid = is_conciseness_score_valid(\n",
    "#                     commit_message_score, \n",
    "#                     commit_message[\"commit_subject_length\"])\n",
    "#                 is_correctness_valid = is_correctness_score_valid(\n",
    "#                     commit_message_score, \n",
    "#                     commit_message.get(\"cleaned_commit_message\") or commit_message[\"commit_message\"],\n",
    "#                     commit[\"jira_url\"])\n",
    "                \n",
    "#                 is_valid = is_rationality_valid and is_comprehensiveness_valid and is_conciseness_valid and is_correctness_valid\n",
    "                \n",
    "#                 if not is_valid:\n",
    "#                     invalid_indexes.add(idx)\n",
    "\n",
    "#         valid_test_case_score = TestCaseScore()\n",
    "#         valid_test_case_score.evaluation_id = test_case_score.evaluation_id\n",
    "#         valid_test_case_score.scores = []\n",
    "\n",
    "#         for generator_score in test_case_score.scores:\n",
    "#             valid_generator_score = GeneratorScore()\n",
    "#             valid_generator_score.generator_id = generator_score.generator_id\n",
    "#             valid_generator_score.scores = []\n",
    "\n",
    "#             for idx, commit_message_score in enumerate(generator_score.scores):\n",
    "#                 if idx in invalid_indexes:\n",
    "#                     continue\n",
    "\n",
    "#                 valid_generator_score.scores.append(commit_message_score)\n",
    "\n",
    "#             valid_test_case_score.scores.append(valid_generator_score)\n",
    "\n",
    "#         cleaned_test_case_scores.append(valid_test_case_score)\n",
    "#         total_invalid_individual_responses += len(invalid_indexes)\n",
    "\n",
    "#     print(f\"Total invalid individual responses: {total_invalid_individual_responses}\")\n",
    "#     print(f\"Percentage of invalid individual responses: {total_invalid_individual_responses / total_individual_responses * 100:.2f}%\")\n",
    "#     print(f\"Total individual responses: {total_individual_responses}\")\n",
    "#     print(f\"Remaining individual responses: {total_individual_responses - total_invalid_individual_responses}\")\n",
    "#     print(\"Finished cleaning scores.\\n\")\n",
    "#     return cleaned_test_case_scores\n",
    "\n",
    "# def get_outlier_indexes(samples: list[int]) -> set[int]:\n",
    "#     median = statistics.median(samples)\n",
    "#     mad = statistics.median([abs(x - median) for x in samples])\n",
    "#     made = 1.483 * mad\n",
    "\n",
    "#     lower_bound = median - 3 * made\n",
    "#     upper_bound = median + 3 * made\n",
    "\n",
    "#     return {\n",
    "#         i for i, x in enumerate(samples)\n",
    "#         if x < lower_bound or x > upper_bound\n",
    "#     }\n",
    "\n",
    "# def remove_outliers(test_case_scores: list[TestCaseScore]) -> list[TestCaseScore]:\n",
    "#     print(\"Removing outliers...\")\n",
    "\n",
    "#     cleaned_test_case_scores: list[TestCaseScore] = []\n",
    "#     total_individual_responses = 0\n",
    "#     total_outlier_individual_responses = 0\n",
    "\n",
    "#     for test_case_score in test_case_scores:\n",
    "#         outlier_indexes = set()\n",
    "\n",
    "#         total_individual_responses += len(test_case_score.scores[0].scores)\n",
    "\n",
    "#         for generator_score in test_case_score.scores:\n",
    "#             if (len(generator_score.scores) >= 4):\n",
    "#                 samples_collection = [[] for _ in range(4)]\n",
    "\n",
    "#                 for commit_message_score in generator_score.scores:\n",
    "#                     samples_collection[0].append(commit_message_score.rationality_score)\n",
    "#                     samples_collection[1].append(commit_message_score.comprehensiveness_score)\n",
    "#                     samples_collection[2].append(commit_message_score.conciseness_score)\n",
    "#                     samples_collection[3].append(commit_message_score.correctness_score)\n",
    "\n",
    "#                 print(f\"Generator ID: {generator_score.generator_id}\")\n",
    "#                 print(f\"Evaluation ID: {test_case_score.evaluation_id}\")\n",
    "#                 print(f\"Samples: {samples_collection}\")\n",
    "#                 for samples in samples_collection:\n",
    "#                     new_outlier_indexes = get_outlier_indexes(samples)\n",
    "#                     print(f\"Outlier indexes: {new_outlier_indexes}\")\n",
    "#                     outlier_indexes = outlier_indexes.union(new_outlier_indexes)\n",
    "\n",
    "#         valid_test_case_score = TestCaseScore()\n",
    "#         valid_test_case_score.evaluation_id = test_case_score.evaluation_id\n",
    "#         valid_test_case_score.scores = []\n",
    "\n",
    "#         for generator_score in test_case_score.scores:\n",
    "#             valid_generator_score = GeneratorScore()\n",
    "#             valid_generator_score.generator_id = generator_score.generator_id\n",
    "#             valid_generator_score.scores = []\n",
    "\n",
    "#             for idx, commit_message_score in enumerate(generator_score.scores):\n",
    "#                 if idx in outlier_indexes:\n",
    "#                     continue\n",
    "\n",
    "#                 valid_generator_score.scores.append(commit_message_score)\n",
    "\n",
    "#             valid_test_case_score.scores.append(valid_generator_score)\n",
    "\n",
    "#         cleaned_test_case_scores.append(valid_test_case_score)\n",
    "#         total_outlier_individual_responses += len(outlier_indexes)\n",
    "\n",
    "#     print(f\"Total outlier individual responses: {total_outlier_individual_responses}\")\n",
    "#     print(f\"Percentage of outlier individual responses: {total_outlier_individual_responses / total_individual_responses * 100:.2f}%\")\n",
    "#     print(f\"Total individual responses: {total_individual_responses}\")\n",
    "#     print(f\"Remaining individual responses: {total_individual_responses - total_outlier_individual_responses}\")\n",
    "\n",
    "#     print(\"Finished cleaning scores.\\n\")\n",
    "#     return cleaned_test_case_scores\n",
    "\n",
    "\n",
    "# score_data = None\n",
    "\n",
    "# with open(SCORE_DATA_JSON_FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "#     score_data = json.load(file)\n",
    "\n",
    "# test_case_scores: list[TestCaseScore] = json_to_object(\"TestCaseScore\", score_data)\n",
    "# test_case_scores = clean_scores(test_case_scores)\n",
    "# test_case_scores = remove_outliers(test_case_scores)\n",
    "\n",
    "# score_summaries: list[ScoreSummary] = []\n",
    "\n",
    "# for test_case_score in test_case_scores:\n",
    "#     for generatorScore in test_case_score.scores:\n",
    "#         score_summary = next((score for score in score_summaries if score.generator_id == generatorScore.generator_id), None)\n",
    "\n",
    "#         if score_summary is None:\n",
    "#             score_summary = ScoreSummary()\n",
    "#             score_summary.generator_id = generatorScore.generator_id\n",
    "#             score_summaries.append(score_summary)\n",
    "\n",
    "#         for commitMessageScore in generatorScore.scores:\n",
    "#             score_summary.rationality_score += commitMessageScore.rationality_score\n",
    "#             score_summary.comprehensiveness_score += commitMessageScore.comprehensiveness_score\n",
    "#             score_summary.conciseness_score += commitMessageScore.conciseness_score\n",
    "#             score_summary.correctness_score += commitMessageScore.correctness_score\n",
    "\n",
    "# for score_summary in score_summaries:\n",
    "#     score_count = sum([\n",
    "#         sum([\n",
    "#             len(generator_score.scores) \n",
    "#             for generator_score \n",
    "#             in test_case_score.scores\n",
    "#             if generator_score.generator_id == score_summary.generator_id\n",
    "#         ])\n",
    "#         for test_case_score\n",
    "#         in test_case_scores\n",
    "#     ])\n",
    "    \n",
    "#     score_summary.rationality_score /= score_count\n",
    "#     score_summary.comprehensiveness_score /= score_count\n",
    "#     score_summary.conciseness_score /= score_count\n",
    "#     score_summary.correctness_score /= score_count\n",
    "\n",
    "# json_string = jsonpickle.encode(score_summaries, unpicklable=False, indent=4)\n",
    "\n",
    "# with open(DEFAULT_SCORE_SUMMARY_OUTPUT_PATH, \"w\") as file:\n",
    "#     file.write(json_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
