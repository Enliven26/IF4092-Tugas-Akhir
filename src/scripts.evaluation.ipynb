{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "load_dotenv(dotenv_path=\".env.evaluation\", verbose=True, override=True)\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autocommit_evaluation.core.enums import EnvironmentKey\n",
    "from autocommit_evaluation.cmg.evaluators import CommitMessageGenerator\n",
    "from autocommit_evaluation.cmg import evaluator\n",
    "from autocommit_evaluation.core import (\n",
    "    main_few_shot_high_level_context_cmg_chain,\n",
    "    main_zero_shot_low_level_context_cmg_chain,\n",
    "    main_few_shot_low_level_context_cmg_chain,\n",
    "    main_zero_shot_high_level_context_cmg_chain,\n",
    "    main_high_level_context_chain,\n",
    "    baseline_high_level_context_chain,\n",
    "    baseline_zero_shot_low_level_context_cmg_chain,\n",
    "    baseline_zero_shot_high_level_context_cmg_chain,\n",
    "    baseline_few_shot_high_level_context_cmg_chain\n",
    ")\n",
    "from autocommit.core.models import CommitDataModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMIT_DATA_JSON_FILE_PATH = os.path.join(\"autocommit_evaluation\", \"data\", \"cmg\", \"commits.evaluation.json\")\n",
    "CONTEXT_DATA_PATH = os.path.join(\"autocommit_evaluation\",\"data\", \"context\")\n",
    "\n",
    "DEFAULT_HIGH_LEVEL_CONTEXT_OUTPUT_PATH = os.path.join(\n",
    "    \"out\", \"evaluation\", \"highlevelcontext\"\n",
    ")\n",
    "DEFAULT_CMG_OUTPUT_PATH = os.path.join(\"out\", \"evaluation\", \"cmg\")\n",
    "DEFAULT_DIFF_CLASSIFICATION_OUTPUT_PATH = os.path.join(\n",
    "    \"out\", \"evaluation\", \"diffclassification\"\n",
    ")\n",
    "\n",
    "DIFF_CLASSIFIER_CHAINS = [\n",
    "    main_zero_shot_low_level_context_cmg_chain,\n",
    "    main_zero_shot_high_level_context_cmg_chain,\n",
    "]\n",
    "\n",
    "HIGH_LEVEL_CONTEXT_CHAINS = [\n",
    "    main_high_level_context_chain,\n",
    "    baseline_high_level_context_chain\n",
    "]\n",
    "\n",
    "GENERATORS = [\n",
    "    CommitMessageGenerator(\n",
    "        \"Main Zero-Shot Low-Level Context Generator\", main_zero_shot_low_level_context_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\n",
    "        \"Main Few-Shot Low-Level Context Generator\", main_few_shot_low_level_context_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\n",
    "        \"Main Zero-Shot High-Level Context Generator\", main_zero_shot_high_level_context_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\n",
    "        \"Main Few-Shot High-Level Context Generator\", main_few_shot_high_level_context_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\n",
    "        \"Baseline Few-Shot Low-Level Context Generator\", baseline_zero_shot_low_level_context_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\n",
    "        \"Baseline Zero-Shot High-Level Context Generator\", baseline_zero_shot_high_level_context_cmg_chain\n",
    "    ),\n",
    "    CommitMessageGenerator(\n",
    "        \"Baseline Few-Shot High-Level Context Generator\", baseline_few_shot_high_level_context_cmg_chain\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH_LEVEL_CONTEXT_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.HIGH_LEVEL_CONTEXT_OUTPUT_PATH.value,\n",
    "        DEFAULT_HIGH_LEVEL_CONTEXT_OUTPUT_PATH,\n",
    "    )\n",
    "\n",
    "CMG_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.CMG_OUTPUT_PATH.value, DEFAULT_CMG_OUTPUT_PATH\n",
    "    )\n",
    "\n",
    "DIFF_CLASSIFICATION_OUTPUT_PATH = os.getenv(\n",
    "        EnvironmentKey.DIFF_CLASSIFICATION_OUTPUT_PATH.value,\n",
    "        DEFAULT_DIFF_CLASSIFICATION_OUTPUT_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commits(path: str) -> list[CommitDataModel]:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            json_string = file.read()\n",
    "\n",
    "        return CommitDataModel.from_json(json_string)\n",
    "\n",
    "COMMITS = get_commits(COMMIT_DATA_JSON_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDED_DIFF_CLASSIFIER_CHAIN_INDICES = [0, 1]\n",
    "\n",
    "for index in INCLUDED_DIFF_CLASSIFIER_CHAIN_INDICES:\n",
    "    evaluator.classify_diffs(DIFF_CLASSIFIER_CHAINS[index], COMMITS, CONTEXT_DATA_PATH, DIFF_CLASSIFICATION_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get High Level Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.smith.langchain.com:443\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Given a Git diff and the relevant source code, write a concise summary of the code changes in a way that a non-technical person can understand. Summarize in maximum three concise sentences. \\n\\nAvoid adding any additional comments or annotations to the summary.\\n\\nGit diff:\\ndiff --git a/core/src/main/java/kafka/log/remote/RemoteLogManager.java b/core/src/main/java/kafka/log/remote/RemoteLogManager.java\\nindex c1c87d579e..3eacbea475 100644\\n--- a/core/src/main/java/kafka/log/remote/RemoteLogManager.java\\n+++ b/core/src/main/java/kafka/log/remote/RemoteLogManager.java\\n@@ -983,7 +983,9 @@ public class RemoteLogManager implements Closeable {\\n                     }\\n                 }\\n                 if (shouldDeleteSegment) {\\n-                    logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    if (!logStartOffset.isPresent() || logStartOffset.getAsLong() < metadata.endOffset() + 1) {\\n+                        logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    }\\n                     logger.info(\"About to delete remote log segment {} due to retention size {} breach. Log size after deletion will be {}.\",\\n                             metadata.remoteLogSegmentId(), retentionSizeData.get().retentionSize, remainingBreachedSize + retentionSizeData.get().retentionSize);\\n                 }\\n@@ -1000,7 +1002,9 @@ public class RemoteLogManager implements Closeable {\\n                     remainingBreachedSize = Math.max(0, remainingBreachedSize - metadata.segmentSizeInBytes());\\n                     // It is fine to have logStartOffset as `metadata.endOffset() + 1` as the segment offset intervals\\n                     // are ascending with in an epoch.\\n-                    logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    if (!logStartOffset.isPresent() || logStartOffset.getAsLong() < metadata.endOffset() + 1) {\\n+                        logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    }\\n                     logger.info(\"About to delete remote log segment {} due to retention time {}ms breach based on the largest record timestamp in the segment\",\\n                             metadata.remoteLogSegmentId(), retentionTimeData.get().retentionMs);\\n                 }\\ndiff --git a/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java b/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java\\nindex 4c4976f060..3c9b8a48e9 100644\\n--- a/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java\\n+++ b/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java\\n@@ -2055,6 +2055,75 @@ public class RemoteLogManagerTest {\\n         assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n     }\\n \\n+    @ParameterizedTest(name = \"testDeletionOnOverlappingRetentionBreachedSegments retentionSize={0} retentionMs={1}\")\\n+    @CsvSource(value = {\"0, -1\", \"-1, 0\"})\\n+    public void testDeletionOnOverlappingRetentionBreachedSegments(long retentionSize,\\n+                                                                   long retentionMs)\\n+            throws RemoteStorageException, ExecutionException, InterruptedException {\\n+        Map<String, Long> logProps = new HashMap<>();\\n+        logProps.put(\"retention.bytes\", retentionSize);\\n+        logProps.put(\"retention.ms\", retentionMs);\\n+        LogConfig mockLogConfig = new LogConfig(logProps);\\n+        when(mockLog.config()).thenReturn(mockLogConfig);\\n+\\n+        List<EpochEntry> epochEntries = Collections.singletonList(epochEntry0);\\n+        checkpoint.write(epochEntries);\\n+        LeaderEpochFileCache cache = new LeaderEpochFileCache(tp, checkpoint, scheduler);\\n+        when(mockLog.leaderEpochCache()).thenReturn(Option.apply(cache));\\n+\\n+        when(mockLog.topicPartition()).thenReturn(leaderTopicIdPartition.topicPartition());\\n+        when(mockLog.logEndOffset()).thenReturn(200L);\\n+\\n+        RemoteLogSegmentMetadata metadata1 = listRemoteLogSegmentMetadata(leaderTopicIdPartition, 1, 100, 1024,\\n+                epochEntries, RemoteLogSegmentState.COPY_SEGMENT_FINISHED)\\n+                .get(0);\\n+        // overlapping segment\\n+        RemoteLogSegmentMetadata metadata2 = new RemoteLogSegmentMetadata(new RemoteLogSegmentId(leaderTopicIdPartition, Uuid.randomUuid()),\\n+                metadata1.startOffset(), metadata1.endOffset() + 5, metadata1.maxTimestampMs(),\\n+                metadata1.brokerId() + 1, metadata1.eventTimestampMs(), metadata1.segmentSizeInBytes() + 128,\\n+                metadata1.customMetadata(), metadata1.state(), metadata1.segmentLeaderEpochs());\\n+\\n+        // When there are overlapping/duplicate segments, the RemoteLogMetadataManager#listRemoteLogSegments\\n+        // returns the segments in order of (valid ++ unreferenced) segments:\\n+        // (eg) B0 uploaded segment S0 with offsets 0-100 and B1 uploaded segment S1 with offsets 0-200.\\n+        //      We will mark the segment S0 as duplicate and add it to unreferencedSegmentIds.\\n+        //      The order of segments returned by listRemoteLogSegments will be S1, S0.\\n+        // While computing the next-log-start-offset, taking the max of deleted segment\\'s end-offset + 1.\\n+        List<RemoteLogSegmentMetadata> metadataList = new ArrayList<>();\\n+        metadataList.add(metadata2);\\n+        metadataList.add(metadata1);\\n+\\n+        when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition))\\n+                .thenReturn(metadataList.iterator());\\n+        when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition, 0))\\n+                .thenAnswer(ans -> metadataList.iterator());\\n+        when(remoteLogMetadataManager.updateRemoteLogSegmentMetadata(any(RemoteLogSegmentMetadataUpdate.class)))\\n+                .thenReturn(CompletableFuture.runAsync(() -> { }));\\n+\\n+        // Verify the metrics for remote deletes and for failures is zero before attempt to delete segments\\n+        assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n+        assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n+        // Verify aggregate metrics\\n+        assertEquals(0, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n+        assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n+\\n+        RemoteLogManager.RLMTask task = remoteLogManager.new RLMTask(leaderTopicIdPartition, 128);\\n+        task.convertToLeader(0);\\n+        task.cleanupExpiredRemoteLogSegments();\\n+\\n+        assertEquals(metadata2.endOffset() + 1, currentLogStartOffset.get());\\n+        verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(0));\\n+        verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(1));\\n+\\n+        // Verify the metric for remote delete is updated correctly\\n+        assertEquals(2, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n+        // Verify we did not report any failure for remote deletes\\n+        assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n+        // Verify aggregate metrics\\n+        assertEquals(2, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n+        assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n+    }\\n+\\n     @ParameterizedTest(name = \"testRemoteDeleteLagsOnRetentionBreachedSegments retentionSize={0} retentionMs={1}\")\\n     @CsvSource(value = {\"0, -1\", \"-1, 0\"})\\n     public void testRemoteDeleteLagsOnRetentionBreachedSegments(long retentionSize,\\n\\n\\nSource code:\\ncore/src/main/java/kafka/log/remote/RemoteLogManager.java (Before)\\npublic class RemoteLogManager implements Closeable {\\nclass RLMTask extends CancellableRunnable {\\n\\n    private final TopicIdPartition topicIdPartition;\\n\\n    private final int customMetadataSizeLimit;\\n\\n    private final Logger logger;\\n\\n    private volatile int leaderEpoch = -1;\\n\\n    public RLMTask(TopicIdPartition topicIdPartition, int customMetadataSizeLimit) {\\n        this.topicIdPartition = topicIdPartition;\\n        this.customMetadataSizeLimit = customMetadataSizeLimit;\\n        LogContext logContext = new LogContext(\"[RemoteLogManager=\" + brokerId + \" partition=\" + topicIdPartition + \"] \");\\n        logger = logContext.logger(RLMTask.class);\\n    }\\n\\n    boolean isLeader() {\\n        return leaderEpoch >= 0;\\n    }\\n\\n    // The copied and log-start offset is empty initially for a new leader RLMTask, and needs to be fetched inside\\n    // the task\\'s run() method.\\n    private volatile Optional<OffsetAndEpoch> copiedOffsetOption = Optional.empty();\\n\\n    private volatile boolean isLogStartOffsetUpdatedOnBecomingLeader = false;\\n\\n    private volatile Optional<String> logDirectory = Optional.empty();\\n\\n    public void convertToLeader(int leaderEpochVal) {\\n        if (leaderEpochVal < 0) {\\n            throw new KafkaException(\"leaderEpoch value for topic partition \" + topicIdPartition + \" can not be negative\");\\n        }\\n        if (this.leaderEpoch != leaderEpochVal) {\\n            leaderEpoch = leaderEpochVal;\\n        }\\n        // Reset copied and log-start offset, so that it is set in next run of RLMTask\\n        copiedOffsetOption = Optional.empty();\\n        isLogStartOffsetUpdatedOnBecomingLeader = false;\\n    }\\n\\n    public void convertToFollower() {\\n        leaderEpoch = -1;\\n    }\\n\\n    private void maybeUpdateLogStartOffsetOnBecomingLeader(UnifiedLog log) throws RemoteStorageException {\\n        if (!isLogStartOffsetUpdatedOnBecomingLeader) {\\n            long logStartOffset = findLogStartOffset(topicIdPartition, log);\\n            updateRemoteLogStartOffset.accept(topicIdPartition.topicPartition(), logStartOffset);\\n            isLogStartOffsetUpdatedOnBecomingLeader = true;\\n            logger.info(\"Found the logStartOffset: {} for partition: {} after becoming leader, leaderEpoch: {}\", logStartOffset, topicIdPartition, leaderEpoch);\\n        }\\n    }\\n\\n    private void maybeUpdateCopiedOffset(UnifiedLog log) throws RemoteStorageException {\\n        if (!copiedOffsetOption.isPresent()) {\\n            // This is found by traversing from the latest leader epoch from leader epoch history and find the highest offset\\n            // of a segment with that epoch copied into remote storage. If it can not find an entry then it checks for the\\n            // previous leader epoch till it finds an entry, If there are no entries till the earliest leader epoch in leader\\n            // epoch cache then it starts copying the segments from the earliest epoch entry\\'s offset.\\n            copiedOffsetOption = Optional.of(findHighestRemoteOffset(topicIdPartition, log));\\n            logger.info(\"Found the highest copiedRemoteOffset: {} for partition: {} after becoming leader, \" + \"leaderEpoch: {}\", copiedOffsetOption, topicIdPartition, leaderEpoch);\\n            copiedOffsetOption.ifPresent(offsetAndEpoch -> log.updateHighestOffsetInRemoteStorage(offsetAndEpoch.offset()));\\n        }\\n    }\\n\\n    /**\\n     *  Segments which match the following criteria are eligible for copying to remote storage:\\n     *  1) Segment is not the active segment and\\n     *  2) Segment end-offset is less than the last-stable-offset as remote storage should contain only\\n     *     committed/acked messages\\n     * @param log The log from which the segments are to be copied\\n     * @param fromOffset The offset from which the segments are to be copied\\n     * @param lastStableOffset The last stable offset of the log\\n     * @return candidate log segments to be copied to remote storage\\n     */\\n    List<EnrichedLogSegment> candidateLogSegments(UnifiedLog log, Long fromOffset, Long lastStableOffset) {\\n        List<EnrichedLogSegment> candidateLogSegments = new ArrayList<>();\\n        List<LogSegment> segments = JavaConverters.seqAsJavaList(log.logSegments(fromOffset, Long.MAX_VALUE).toSeq());\\n        if (!segments.isEmpty()) {\\n            for (int idx = 1; idx < segments.size(); idx++) {\\n                LogSegment previousSeg = segments.get(idx - 1);\\n                LogSegment currentSeg = segments.get(idx);\\n                if (currentSeg.baseOffset() <= lastStableOffset) {\\n                    candidateLogSegments.add(new EnrichedLogSegment(previousSeg, currentSeg.baseOffset()));\\n                }\\n            }\\n            // Discard the last active segment\\n        }\\n        return candidateLogSegments;\\n    }\\n\\n    public void copyLogSegmentsToRemote(UnifiedLog log) throws InterruptedException {\\n        if (isCancelled())\\n            return;\\n        try {\\n            maybeUpdateLogStartOffsetOnBecomingLeader(log);\\n            maybeUpdateCopiedOffset(log);\\n            long copiedOffset = copiedOffsetOption.get().offset();\\n            // LSO indicates the offset below are ready to be consumed (high-watermark or committed)\\n            long lso = log.lastStableOffset();\\n            if (lso < 0) {\\n                logger.warn(\"lastStableOffset for partition {} is {}, which should not be negative.\", topicIdPartition, lso);\\n            } else if (lso > 0 && copiedOffset < lso) {\\n                // log-start-offset can be ahead of the copied-offset, when:\\n                // 1) log-start-offset gets incremented via delete-records API (or)\\n                // 2) enabling the remote log for the first time\\n                long fromOffset = Math.max(copiedOffset + 1, log.logStartOffset());\\n                List<EnrichedLogSegment> candidateLogSegments = candidateLogSegments(log, fromOffset, lso);\\n                logger.debug(\"Candidate log segments, logStartOffset: {}, copiedOffset: {}, fromOffset: {}, lso: {} \" + \"and candidateLogSegments: {}\", log.logStartOffset(), copiedOffset, fromOffset, lso, candidateLogSegments);\\n                if (candidateLogSegments.isEmpty()) {\\n                    logger.debug(\"No segments found to be copied for partition {} with copiedOffset: {} and active segment\\'s base-offset: {}\", topicIdPartition, copiedOffset, log.activeSegment().baseOffset());\\n                } else {\\n                    for (EnrichedLogSegment candidateLogSegment : candidateLogSegments) {\\n                        if (isCancelled() || !isLeader()) {\\n                            logger.info(\"Skipping copying log segments as the current task state is changed, cancelled: {} leader:{}\", isCancelled(), isLeader());\\n                            return;\\n                        }\\n                        copyQuotaManagerLock.lock();\\n                        try {\\n                            while (rlmCopyQuotaManager.isQuotaExceeded()) {\\n                                logger.debug(\"Quota exceeded for copying log segments, waiting for the quota to be available.\");\\n                                // If the thread gets interrupted while waiting, the InterruptedException is thrown\\n                                // back to the caller. It\\'s important to note that the task being executed is already\\n                                // cancelled before the executing thread is interrupted. The caller is responsible\\n                                // for handling the exception gracefully by checking if the task is already cancelled.\\n                                boolean ignored = copyQuotaManagerLockCondition.await(quotaTimeout().toMillis(), TimeUnit.MILLISECONDS);\\n                            }\\n                            rlmCopyQuotaManager.record(candidateLogSegment.logSegment.log().sizeInBytes());\\n                            // Signal waiting threads to check the quota again\\n                            copyQuotaManagerLockCondition.signalAll();\\n                        } finally {\\n                            copyQuotaManagerLock.unlock();\\n                        }\\n                        copyLogSegment(log, candidateLogSegment.logSegment, candidateLogSegment.nextSegmentOffset);\\n                    }\\n                }\\n            } else {\\n                logger.debug(\"Skipping copying segments, current read-offset:{}, and LSO:{}\", copiedOffset, lso);\\n            }\\n        } catch (CustomMetadataSizeLimitExceededException e) {\\n            // Only stop this task. Logging is done where the exception is thrown.\\n            brokerTopicStats.topicStats(log.topicPartition().topic()).failedRemoteCopyRequestRate().mark();\\n            brokerTopicStats.allTopicsStats().failedRemoteCopyRequestRate().mark();\\n            this.cancel();\\n        } catch (InterruptedException | RetriableException ex) {\\n            throw ex;\\n        } catch (Exception ex) {\\n            if (!isCancelled()) {\\n                brokerTopicStats.topicStats(log.topicPartition().topic()).failedRemoteCopyRequestRate().mark();\\n                brokerTopicStats.allTopicsStats().failedRemoteCopyRequestRate().mark();\\n                logger.error(\"Error occurred while copying log segments of partition: {}\", topicIdPartition, ex);\\n            }\\n        }\\n    }\\n\\n    private void copyLogSegment(UnifiedLog log, LogSegment segment, long nextSegmentBaseOffset) throws InterruptedException, ExecutionException, RemoteStorageException, IOException, CustomMetadataSizeLimitExceededException {\\n        File logFile = segment.log().file();\\n        String logFileName = logFile.getName();\\n        logger.info(\"Copying {} to remote storage.\", logFileName);\\n        RemoteLogSegmentId id = RemoteLogSegmentId.generateNew(topicIdPartition);\\n        long endOffset = nextSegmentBaseOffset - 1;\\n        File producerStateSnapshotFile = log.producerStateManager().fetchSnapshot(nextSegmentBaseOffset).orElse(null);\\n        List<EpochEntry> epochEntries = getLeaderEpochEntries(log, segment.baseOffset(), nextSegmentBaseOffset);\\n        Map<Integer, Long> segmentLeaderEpochs = new HashMap<>(epochEntries.size());\\n        epochEntries.forEach(entry -> segmentLeaderEpochs.put(entry.epoch, entry.startOffset));\\n        RemoteLogSegmentMetadata copySegmentStartedRlsm = new RemoteLogSegmentMetadata(id, segment.baseOffset(), endOffset, segment.largestTimestamp(), brokerId, time.milliseconds(), segment.log().sizeInBytes(), segmentLeaderEpochs);\\n        remoteLogMetadataManager.addRemoteLogSegmentMetadata(copySegmentStartedRlsm).get();\\n        ByteBuffer leaderEpochsIndex = epochEntriesAsByteBuffer(getLeaderEpochEntries(log, -1, nextSegmentBaseOffset));\\n        LogSegmentData segmentData = new LogSegmentData(logFile.toPath(), toPathIfExists(segment.offsetIndex().file()), toPathIfExists(segment.timeIndex().file()), Optional.ofNullable(toPathIfExists(segment.txnIndex().file())), producerStateSnapshotFile.toPath(), leaderEpochsIndex);\\n        brokerTopicStats.topicStats(log.topicPartition().topic()).remoteCopyRequestRate().mark();\\n        brokerTopicStats.allTopicsStats().remoteCopyRequestRate().mark();\\n        Optional<CustomMetadata> customMetadata = remoteLogStorageManager.copyLogSegmentData(copySegmentStartedRlsm, segmentData);\\n        RemoteLogSegmentMetadataUpdate copySegmentFinishedRlsm = new RemoteLogSegmentMetadataUpdate(id, time.milliseconds(), customMetadata, RemoteLogSegmentState.COPY_SEGMENT_FINISHED, brokerId);\\n        if (customMetadata.isPresent()) {\\n            long customMetadataSize = customMetadata.get().value().length;\\n            if (customMetadataSize > this.customMetadataSizeLimit) {\\n                CustomMetadataSizeLimitExceededException e = new CustomMetadataSizeLimitExceededException();\\n                logger.error(\"Custom metadata size {} exceeds configured limit {}.\" + \" Copying will be stopped and copied segment will be attempted to clean.\" + \" Original metadata: {}\", customMetadataSize, this.customMetadataSizeLimit, copySegmentStartedRlsm, e);\\n                try {\\n                    // For deletion, we provide back the custom metadata by creating a new metadata object from the update.\\n                    // However, the update itself will not be stored in this case.\\n                    remoteLogStorageManager.deleteLogSegmentData(copySegmentStartedRlsm.createWithUpdates(copySegmentFinishedRlsm));\\n                    logger.info(\"Successfully cleaned segment after custom metadata size exceeded\");\\n                } catch (RemoteStorageException e1) {\\n                    logger.error(\"Error while cleaning segment after custom metadata size exceeded, consider cleaning manually\", e1);\\n                }\\n                throw e;\\n            }\\n        }\\n        remoteLogMetadataManager.updateRemoteLogSegmentMetadata(copySegmentFinishedRlsm).get();\\n        brokerTopicStats.topicStats(log.topicPartition().topic()).remoteCopyBytesRate().mark(copySegmentStartedRlsm.segmentSizeInBytes());\\n        brokerTopicStats.allTopicsStats().remoteCopyBytesRate().mark(copySegmentStartedRlsm.segmentSizeInBytes());\\n        // `epochEntries` cannot be empty, there is a pre-condition validation in RemoteLogSegmentMetadata\\n        // constructor\\n        int lastEpochInSegment = epochEntries.get(epochEntries.size() - 1).epoch;\\n        copiedOffsetOption = Optional.of(new OffsetAndEpoch(endOffset, lastEpochInSegment));\\n        // Update the highest offset in remote storage for this partition\\'s log so that the local log segments\\n        // are not deleted before they are copied to remote storage.\\n        log.updateHighestOffsetInRemoteStorage(endOffset);\\n        logger.info(\"Copied {} to remote storage with segment-id: {}\", logFileName, copySegmentFinishedRlsm.remoteLogSegmentId());\\n        long bytesLag = log.onlyLocalLogSegmentsSize() - log.activeSegment().size();\\n        String topic = topicIdPartition.topic();\\n        int partition = topicIdPartition.partition();\\n        long segmentsLag = log.onlyLocalLogSegmentsCount();\\n        brokerTopicStats.recordRemoteCopyLagBytes(topic, partition, bytesLag);\\n        brokerTopicStats.recordRemoteCopyLagSegments(topic, partition, segmentsLag);\\n    }\\n\\n    private Path toPathIfExists(File file) {\\n        return file.exists() ? file.toPath() : null;\\n    }\\n\\n    public void run() {\\n        if (isCancelled())\\n            return;\\n        try {\\n            Optional<UnifiedLog> unifiedLogOptional = fetchLog.apply(topicIdPartition.topicPartition());\\n            if (!unifiedLogOptional.isPresent()) {\\n                return;\\n            }\\n            UnifiedLog log = unifiedLogOptional.get();\\n            // In the first run after completing altering logDir within broker, we should make sure the state is reset. (KAFKA-16711)\\n            if (!log.parentDir().equals(logDirectory.orElse(null))) {\\n                copiedOffsetOption = Optional.empty();\\n                isLogStartOffsetUpdatedOnBecomingLeader = false;\\n                logDirectory = Optional.of(log.parentDir());\\n            }\\n            if (isLeader()) {\\n                // Copy log segments to remote storage\\n                copyLogSegmentsToRemote(log);\\n                // Cleanup/delete expired remote log segments\\n                cleanupExpiredRemoteLogSegments();\\n            } else {\\n                OffsetAndEpoch offsetAndEpoch = findHighestRemoteOffset(topicIdPartition, log);\\n                // Update the highest offset in remote storage for this partition\\'s log so that the local log segments\\n                // are not deleted before they are copied to remote storage.\\n                log.updateHighestOffsetInRemoteStorage(offsetAndEpoch.offset());\\n            }\\n        } catch (InterruptedException ex) {\\n            if (!isCancelled()) {\\n                logger.warn(\"Current thread for topic-partition-id {} is interrupted\", topicIdPartition, ex);\\n            }\\n        } catch (RetriableException ex) {\\n            logger.debug(\"Encountered a retryable error while executing current task for topic-partition {}\", topicIdPartition, ex);\\n        } catch (Exception ex) {\\n            if (!isCancelled()) {\\n                logger.warn(\"Current task for topic-partition {} received error but it will be scheduled\", topicIdPartition, ex);\\n            }\\n        }\\n    }\\n\\n    public void handleLogStartOffsetUpdate(TopicPartition topicPartition, long remoteLogStartOffset) {\\n        if (isLeader()) {\\n            logger.debug(\"Updating {} with remoteLogStartOffset: {}\", topicPartition, remoteLogStartOffset);\\n            updateRemoteLogStartOffset.accept(topicPartition, remoteLogStartOffset);\\n        }\\n    }\\n\\n    class RemoteLogRetentionHandler {\\n\\n        private final Optional<RetentionSizeData> retentionSizeData;\\n\\n        private final Optional<RetentionTimeData> retentionTimeData;\\n\\n        private long remainingBreachedSize;\\n\\n        private OptionalLong logStartOffset = OptionalLong.empty();\\n\\n        public RemoteLogRetentionHandler(Optional<RetentionSizeData> retentionSizeData, Optional<RetentionTimeData> retentionTimeData) {\\n            this.retentionSizeData = retentionSizeData;\\n            this.retentionTimeData = retentionTimeData;\\n            remainingBreachedSize = retentionSizeData.map(sizeData -> sizeData.remainingBreachedSize).orElse(0L);\\n        }\\n\\n        private boolean isSegmentBreachedByRetentionSize(RemoteLogSegmentMetadata metadata) {\\n            boolean shouldDeleteSegment = false;\\n            if (!retentionSizeData.isPresent()) {\\n                return shouldDeleteSegment;\\n            }\\n            // Assumption that segments contain size >= 0\\n            if (remainingBreachedSize > 0) {\\n                long remainingBytes = remainingBreachedSize - metadata.segmentSizeInBytes();\\n                if (remainingBytes >= 0) {\\n                    remainingBreachedSize = remainingBytes;\\n                    shouldDeleteSegment = true;\\n                }\\n            }\\n            if (shouldDeleteSegment) {\\n                logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n                logger.info(\"About to delete remote log segment {} due to retention size {} breach. Log size after deletion will be {}.\", metadata.remoteLogSegmentId(), retentionSizeData.get().retentionSize, remainingBreachedSize + retentionSizeData.get().retentionSize);\\n            }\\n            return shouldDeleteSegment;\\n        }\\n\\n        public boolean isSegmentBreachedByRetentionTime(RemoteLogSegmentMetadata metadata) {\\n            boolean shouldDeleteSegment = false;\\n            if (!retentionTimeData.isPresent()) {\\n                return shouldDeleteSegment;\\n            }\\n            shouldDeleteSegment = metadata.maxTimestampMs() <= retentionTimeData.get().cleanupUntilMs;\\n            if (shouldDeleteSegment) {\\n                remainingBreachedSize = Math.max(0, remainingBreachedSize - metadata.segmentSizeInBytes());\\n                // It is fine to have logStartOffset as `metadata.endOffset() + 1` as the segment offset intervals\\n                // are ascending with in an epoch.\\n                logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n                logger.info(\"About to delete remote log segment {} due to retention time {}ms breach based on the largest record timestamp in the segment\", metadata.remoteLogSegmentId(), retentionTimeData.get().retentionMs);\\n            }\\n            return shouldDeleteSegment;\\n        }\\n\\n        private boolean isSegmentBreachByLogStartOffset(RemoteLogSegmentMetadata metadata, long logStartOffset, NavigableMap<Integer, Long> leaderEpochEntries) {\\n            boolean shouldDeleteSegment = false;\\n            if (!leaderEpochEntries.isEmpty()) {\\n                // Note that `logStartOffset` and `leaderEpochEntries.firstEntry().getValue()` should be same\\n                Integer firstEpoch = leaderEpochEntries.firstKey();\\n                shouldDeleteSegment = metadata.segmentLeaderEpochs().keySet().stream().allMatch(epoch -> epoch <= firstEpoch) && metadata.endOffset() < logStartOffset;\\n            }\\n            if (shouldDeleteSegment) {\\n                logger.info(\"About to delete remote log segment {} due to log-start-offset {} breach. \" + \"Current earliest-epoch-entry: {}, segment-end-offset: {} and segment-epochs: {}\", metadata.remoteLogSegmentId(), logStartOffset, leaderEpochEntries.firstEntry(), metadata.endOffset(), metadata.segmentLeaderEpochs());\\n            }\\n            return shouldDeleteSegment;\\n        }\\n\\n        // It removes the segments beyond the current leader\\'s earliest epoch. Those segments are considered as\\n        // unreferenced because they are not part of the current leader epoch lineage.\\n        private boolean deleteLogSegmentsDueToLeaderEpochCacheTruncation(EpochEntry earliestEpochEntry, RemoteLogSegmentMetadata metadata) throws RemoteStorageException, ExecutionException, InterruptedException {\\n            boolean isSegmentDeleted = deleteRemoteLogSegment(metadata, ignored -> metadata.segmentLeaderEpochs().keySet().stream().allMatch(epoch -> epoch < earliestEpochEntry.epoch));\\n            if (isSegmentDeleted) {\\n                logger.info(\"Deleted remote log segment {} due to leader-epoch-cache truncation. \" + \"Current earliest-epoch-entry: {}, segment-end-offset: {} and segment-epochs: {}\", metadata.remoteLogSegmentId(), earliestEpochEntry, metadata.endOffset(), metadata.segmentLeaderEpochs().keySet());\\n            }\\n            // No need to update the log-start-offset as these epochs/offsets are earlier to that value.\\n            return isSegmentDeleted;\\n        }\\n\\n        private boolean deleteRemoteLogSegment(RemoteLogSegmentMetadata segmentMetadata, Predicate<RemoteLogSegmentMetadata> predicate) throws RemoteStorageException, ExecutionException, InterruptedException {\\n            if (predicate.test(segmentMetadata)) {\\n                logger.debug(\"Deleting remote log segment {}\", segmentMetadata.remoteLogSegmentId());\\n                String topic = segmentMetadata.topicIdPartition().topic();\\n                // Publish delete segment started event.\\n                remoteLogMetadataManager.updateRemoteLogSegmentMetadata(new RemoteLogSegmentMetadataUpdate(segmentMetadata.remoteLogSegmentId(), time.milliseconds(), segmentMetadata.customMetadata(), RemoteLogSegmentState.DELETE_SEGMENT_STARTED, brokerId)).get();\\n                brokerTopicStats.topicStats(topic).remoteDeleteRequestRate().mark();\\n                brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().mark();\\n                // Delete the segment in remote storage.\\n                try {\\n                    remoteLogStorageManager.deleteLogSegmentData(segmentMetadata);\\n                } catch (RemoteStorageException e) {\\n                    brokerTopicStats.topicStats(topic).failedRemoteDeleteRequestRate().mark();\\n                    brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().mark();\\n                    throw e;\\n                }\\n                // Publish delete segment finished event.\\n                remoteLogMetadataManager.updateRemoteLogSegmentMetadata(new RemoteLogSegmentMetadataUpdate(segmentMetadata.remoteLogSegmentId(), time.milliseconds(), segmentMetadata.customMetadata(), RemoteLogSegmentState.DELETE_SEGMENT_FINISHED, brokerId)).get();\\n                logger.debug(\"Deleted remote log segment {}\", segmentMetadata.remoteLogSegmentId());\\n                return true;\\n            }\\n            return false;\\n        }\\n    }\\n\\n    private void updateMetadataCountAndLogSizeWith(int metadataCount, long remoteLogSizeBytes) {\\n        int partition = topicIdPartition.partition();\\n        String topic = topicIdPartition.topic();\\n        brokerTopicStats.recordRemoteLogMetadataCount(topic, partition, metadataCount);\\n        brokerTopicStats.recordRemoteLogSizeBytes(topic, partition, remoteLogSizeBytes);\\n    }\\n\\n    private void updateRemoteDeleteLagWith(int segmentsLeftToDelete, long sizeOfDeletableSegmentsBytes) {\\n        String topic = topicIdPartition.topic();\\n        int partition = topicIdPartition.partition();\\n        brokerTopicStats.recordRemoteDeleteLagSegments(topic, partition, segmentsLeftToDelete);\\n        brokerTopicStats.recordRemoteDeleteLagBytes(topic, partition, sizeOfDeletableSegmentsBytes);\\n    }\\n\\n    void cleanupExpiredRemoteLogSegments() throws RemoteStorageException, ExecutionException, InterruptedException {\\n        if (isCancelled() || !isLeader()) {\\n            logger.info(\"Returning from remote log segments cleanup as the task state is changed\");\\n            return;\\n        }\\n        final Optional<UnifiedLog> logOptional = fetchLog.apply(topicIdPartition.topicPartition());\\n        if (!logOptional.isPresent()) {\\n            logger.debug(\"No UnifiedLog instance available for partition: {}\", topicIdPartition);\\n            return;\\n        }\\n        final UnifiedLog log = logOptional.get();\\n        final Option<LeaderEpochFileCache> leaderEpochCacheOption = log.leaderEpochCache();\\n        if (leaderEpochCacheOption.isEmpty()) {\\n            logger.debug(\"No leader epoch cache available for partition: {}\", topicIdPartition);\\n            return;\\n        }\\n        // Cleanup remote log segments and update the log start offset if applicable.\\n        final Iterator<RemoteLogSegmentMetadata> segmentMetadataIter = remoteLogMetadataManager.listRemoteLogSegments(topicIdPartition);\\n        if (!segmentMetadataIter.hasNext()) {\\n            updateMetadataCountAndLogSizeWith(0, 0);\\n            logger.debug(\"No remote log segments available on remote storage for partition: {}\", topicIdPartition);\\n            return;\\n        }\\n        final Set<Integer> epochsSet = new HashSet<>();\\n        int metadataCount = 0;\\n        long remoteLogSizeBytes = 0;\\n        // Good to have an API from RLMM to get all the remote leader epochs of all the segments of a partition\\n        // instead of going through all the segments and building it here.\\n        while (segmentMetadataIter.hasNext()) {\\n            RemoteLogSegmentMetadata segmentMetadata = segmentMetadataIter.next();\\n            epochsSet.addAll(segmentMetadata.segmentLeaderEpochs().keySet());\\n            metadataCount++;\\n            remoteLogSizeBytes += segmentMetadata.segmentSizeInBytes();\\n        }\\n        updateMetadataCountAndLogSizeWith(metadataCount, remoteLogSizeBytes);\\n        // All the leader epochs in sorted order that exists in remote storage\\n        final List<Integer> remoteLeaderEpochs = new ArrayList<>(epochsSet);\\n        Collections.sort(remoteLeaderEpochs);\\n        LeaderEpochFileCache leaderEpochCache = leaderEpochCacheOption.get();\\n        // Build the leader epoch map by filtering the epochs that do not have any records.\\n        NavigableMap<Integer, Long> epochWithOffsets = buildFilteredLeaderEpochMap(leaderEpochCache.epochWithOffsets());\\n        long logStartOffset = log.logStartOffset();\\n        long logEndOffset = log.logEndOffset();\\n        Optional<RetentionSizeData> retentionSizeData = buildRetentionSizeData(log.config().retentionSize, log.onlyLocalLogSegmentsSize(), logEndOffset, epochWithOffsets);\\n        Optional<RetentionTimeData> retentionTimeData = buildRetentionTimeData(log.config().retentionMs);\\n        RemoteLogRetentionHandler remoteLogRetentionHandler = new RemoteLogRetentionHandler(retentionSizeData, retentionTimeData);\\n        Iterator<Integer> epochIterator = epochWithOffsets.navigableKeySet().iterator();\\n        boolean canProcess = true;\\n        List<RemoteLogSegmentMetadata> segmentsToDelete = new ArrayList<>();\\n        long sizeOfDeletableSegmentsBytes = 0L;\\n        while (canProcess && epochIterator.hasNext()) {\\n            Integer epoch = epochIterator.next();\\n            Iterator<RemoteLogSegmentMetadata> segmentsIterator = remoteLogMetadataManager.listRemoteLogSegments(topicIdPartition, epoch);\\n            while (canProcess && segmentsIterator.hasNext()) {\\n                if (isCancelled() || !isLeader()) {\\n                    logger.info(\"Returning from remote log segments cleanup for the remaining segments as the task state is changed.\");\\n                    return;\\n                }\\n                RemoteLogSegmentMetadata metadata = segmentsIterator.next();\\n                if (RemoteLogSegmentState.DELETE_SEGMENT_FINISHED.equals(metadata.state())) {\\n                    continue;\\n                }\\n                if (segmentsToDelete.contains(metadata)) {\\n                    continue;\\n                }\\n                // When the log-start-offset is moved by the user, the leader-epoch-checkpoint file gets truncated\\n                // as per the log-start-offset. Until the rlm-cleaner-thread runs in the next iteration, those\\n                // remote log segments won\\'t be removed. The `isRemoteSegmentWithinLeaderEpoch` validates whether\\n                // the epochs present in the segment lies in the checkpoint file. It will always return false\\n                // since the checkpoint file was already truncated.\\n                boolean shouldDeleteSegment = remoteLogRetentionHandler.isSegmentBreachByLogStartOffset(metadata, logStartOffset, epochWithOffsets);\\n                boolean isValidSegment = false;\\n                if (!shouldDeleteSegment) {\\n                    // check whether the segment contains the required epoch range with in the current leader epoch lineage.\\n                    isValidSegment = isRemoteSegmentWithinLeaderEpochs(metadata, logEndOffset, epochWithOffsets);\\n                    if (isValidSegment) {\\n                        shouldDeleteSegment = remoteLogRetentionHandler.isSegmentBreachedByRetentionTime(metadata) || remoteLogRetentionHandler.isSegmentBreachedByRetentionSize(metadata);\\n                    }\\n                }\\n                if (shouldDeleteSegment) {\\n                    segmentsToDelete.add(metadata);\\n                    sizeOfDeletableSegmentsBytes += metadata.segmentSizeInBytes();\\n                }\\n                canProcess = shouldDeleteSegment || !isValidSegment;\\n            }\\n        }\\n        // Update log start offset with the computed value after retention cleanup is done\\n        remoteLogRetentionHandler.logStartOffset.ifPresent(offset -> handleLogStartOffsetUpdate(topicIdPartition.topicPartition(), offset));\\n        // At this point in time we have updated the log start offsets, but not initiated a deletion.\\n        // Either a follower has picked up the changes to the log start offset, or they have not.\\n        // If the follower HAS picked up the changes, and they become the leader this replica won\\'t successfully complete\\n        // the deletion.\\n        // However, the new leader will correctly pick up all breaching segments as log start offset breaching ones\\n        // and delete them accordingly.\\n        // If the follower HAS NOT picked up the changes, and they become the leader then they will go through this process\\n        // again and delete them with the original deletion reason i.e. size, time or log start offset breach.\\n        int segmentsLeftToDelete = segmentsToDelete.size();\\n        updateRemoteDeleteLagWith(segmentsLeftToDelete, sizeOfDeletableSegmentsBytes);\\n        List<String> undeletedSegments = new ArrayList<>();\\n        for (RemoteLogSegmentMetadata segmentMetadata : segmentsToDelete) {\\n            if (!remoteLogRetentionHandler.deleteRemoteLogSegment(segmentMetadata, x -> !isCancelled() && isLeader())) {\\n                undeletedSegments.add(segmentMetadata.remoteLogSegmentId().toString());\\n            } else {\\n                sizeOfDeletableSegmentsBytes -= segmentMetadata.segmentSizeInBytes();\\n                segmentsLeftToDelete--;\\n                updateRemoteDeleteLagWith(segmentsLeftToDelete, sizeOfDeletableSegmentsBytes);\\n            }\\n        }\\n        if (!undeletedSegments.isEmpty()) {\\n            logger.info(\"The following remote segments could not be deleted: {}\", String.join(\",\", undeletedSegments));\\n        }\\n        // Remove the remote log segments whose segment-leader-epochs are less than the earliest-epoch known\\n        // to the leader. This will remove the unreferenced segments in the remote storage. This is needed for\\n        // unclean leader election scenarios as the remote storage can have epochs earlier to the current leader\\'s\\n        // earliest leader epoch.\\n        Optional<EpochEntry> earliestEpochEntryOptional = leaderEpochCache.earliestEntry();\\n        if (earliestEpochEntryOptional.isPresent()) {\\n            EpochEntry earliestEpochEntry = earliestEpochEntryOptional.get();\\n            Iterator<Integer> epochsToClean = remoteLeaderEpochs.stream().filter(remoteEpoch -> remoteEpoch < earliestEpochEntry.epoch).iterator();\\n            List<RemoteLogSegmentMetadata> listOfSegmentsToBeCleaned = new ArrayList<>();\\n            while (epochsToClean.hasNext()) {\\n                int epoch = epochsToClean.next();\\n                Iterator<RemoteLogSegmentMetadata> segmentsToBeCleaned = remoteLogMetadataManager.listRemoteLogSegments(topicIdPartition, epoch);\\n                while (segmentsToBeCleaned.hasNext()) {\\n                    if (!isCancelled() && isLeader()) {\\n                        RemoteLogSegmentMetadata nextSegmentMetadata = segmentsToBeCleaned.next();\\n                        sizeOfDeletableSegmentsBytes += nextSegmentMetadata.segmentSizeInBytes();\\n                        listOfSegmentsToBeCleaned.add(nextSegmentMetadata);\\n                    }\\n                }\\n            }\\n            segmentsLeftToDelete += listOfSegmentsToBeCleaned.size();\\n            updateRemoteDeleteLagWith(segmentsLeftToDelete, sizeOfDeletableSegmentsBytes);\\n            for (RemoteLogSegmentMetadata segmentMetadata : listOfSegmentsToBeCleaned) {\\n                if (!isCancelled() && isLeader()) {\\n                    // No need to update the log-start-offset even though the segment is deleted as these epochs/offsets are earlier to that value.\\n                    if (remoteLogRetentionHandler.deleteLogSegmentsDueToLeaderEpochCacheTruncation(earliestEpochEntry, segmentMetadata)) {\\n                        sizeOfDeletableSegmentsBytes -= segmentMetadata.segmentSizeInBytes();\\n                        segmentsLeftToDelete--;\\n                        updateRemoteDeleteLagWith(segmentsLeftToDelete, sizeOfDeletableSegmentsBytes);\\n                    }\\n                }\\n            }\\n        }\\n    }\\n\\n    private Optional<RetentionTimeData> buildRetentionTimeData(long retentionMs) {\\n        return retentionMs > -1 ? Optional.of(new RetentionTimeData(retentionMs, time.milliseconds() - retentionMs)) : Optional.empty();\\n    }\\n\\n    private Optional<RetentionSizeData> buildRetentionSizeData(long retentionSize, long onlyLocalLogSegmentsSize, long logEndOffset, NavigableMap<Integer, Long> epochEntries) throws RemoteStorageException {\\n        if (retentionSize > -1) {\\n            long startTimeMs = time.milliseconds();\\n            long remoteLogSizeBytes = 0L;\\n            Set<RemoteLogSegmentId> visitedSegmentIds = new HashSet<>();\\n            for (Integer epoch : epochEntries.navigableKeySet()) {\\n                // remoteLogSize(topicIdPartition, epochEntry.epoch) may not be completely accurate as the remote\\n                // log size may be computed for all the segments but not for segments with in the current\\n                // partition\\'s leader epoch lineage. Better to revisit this API.\\n                // remoteLogSizeBytes += remoteLogMetadataManager.remoteLogSize(topicIdPartition, epochEntry.epoch);\\n                Iterator<RemoteLogSegmentMetadata> segmentsIterator = remoteLogMetadataManager.listRemoteLogSegments(topicIdPartition, epoch);\\n                while (segmentsIterator.hasNext()) {\\n                    RemoteLogSegmentMetadata segmentMetadata = segmentsIterator.next();\\n                    RemoteLogSegmentId segmentId = segmentMetadata.remoteLogSegmentId();\\n                    if (!visitedSegmentIds.contains(segmentId) && isRemoteSegmentWithinLeaderEpochs(segmentMetadata, logEndOffset, epochEntries)) {\\n                        remoteLogSizeBytes += segmentMetadata.segmentSizeInBytes();\\n                        visitedSegmentIds.add(segmentId);\\n                    }\\n                }\\n            }\\n            brokerTopicStats.recordRemoteLogSizeComputationTime(topicIdPartition.topic(), topicIdPartition.partition(), time.milliseconds() - startTimeMs);\\n            // This is the total size of segments in local log that have their base-offset > local-log-start-offset\\n            // and size of the segments in remote storage which have their end-offset < local-log-start-offset.\\n            long totalSize = onlyLocalLogSegmentsSize + remoteLogSizeBytes;\\n            if (totalSize > retentionSize) {\\n                long remainingBreachedSize = totalSize - retentionSize;\\n                RetentionSizeData retentionSizeData = new RetentionSizeData(retentionSize, remainingBreachedSize);\\n                return Optional.of(retentionSizeData);\\n            }\\n        }\\n        return Optional.empty();\\n    }\\n\\n    public String toString() {\\n        return this.getClass() + \"[\" + topicIdPartition + \"]\";\\n    }\\n}\\n}\\n\\ncore/src/main/java/kafka/log/remote/RemoteLogManager.java (After)\\npublic class RemoteLogManager implements Closeable {\\nclass RLMTask extends CancellableRunnable {\\n\\n    private final TopicIdPartition topicIdPartition;\\n\\n    private final int customMetadataSizeLimit;\\n\\n    private final Logger logger;\\n\\n    private volatile int leaderEpoch = -1;\\n\\n    public RLMTask(TopicIdPartition topicIdPartition, int customMetadataSizeLimit) {\\n        this.topicIdPartition = topicIdPartition;\\n        this.customMetadataSizeLimit = customMetadataSizeLimit;\\n        LogContext logContext = new LogContext(\"[RemoteLogManager=\" + brokerId + \" partition=\" + topicIdPartition + \"] \");\\n        logger = logContext.logger(RLMTask.class);\\n    }\\n\\n    boolean isLeader() {\\n        return leaderEpoch >= 0;\\n    }\\n\\n    // The copied and log-start offset is empty initially for a new leader RLMTask, and needs to be fetched inside\\n    // the task\\'s run() method.\\n    private volatile Optional<OffsetAndEpoch> copiedOffsetOption = Optional.empty();\\n\\n    private volatile boolean isLogStartOffsetUpdatedOnBecomingLeader = false;\\n\\n    private volatile Optional<String> logDirectory = Optional.empty();\\n\\n    public void convertToLeader(int leaderEpochVal) {\\n        if (leaderEpochVal < 0) {\\n            throw new KafkaException(\"leaderEpoch value for topic partition \" + topicIdPartition + \" can not be negative\");\\n        }\\n        if (this.leaderEpoch != leaderEpochVal) {\\n            leaderEpoch = leaderEpochVal;\\n        }\\n        // Reset copied and log-start offset, so that it is set in next run of RLMTask\\n        copiedOffsetOption = Optional.empty();\\n        isLogStartOffsetUpdatedOnBecomingLeader = false;\\n    }\\n\\n    public void convertToFollower() {\\n        leaderEpoch = -1;\\n    }\\n\\n    private void maybeUpdateLogStartOffsetOnBecomingLeader(UnifiedLog log) throws RemoteStorageException {\\n        if (!isLogStartOffsetUpdatedOnBecomingLeader) {\\n            long logStartOffset = findLogStartOffset(topicIdPartition, log);\\n            updateRemoteLogStartOffset.accept(topicIdPartition.topicPartition(), logStartOffset);\\n            isLogStartOffsetUpdatedOnBecomingLeader = true;\\n            logger.info(\"Found the logStartOffset: {} for partition: {} after becoming leader, leaderEpoch: {}\", logStartOffset, topicIdPartition, leaderEpoch);\\n        }\\n    }\\n\\n    private void maybeUpdateCopiedOffset(UnifiedLog log) throws RemoteStorageException {\\n        if (!copiedOffsetOption.isPresent()) {\\n            // This is found by traversing from the latest leader epoch from leader epoch history and find the highest offset\\n            // of a segment with that epoch copied into remote storage. If it can not find an entry then it checks for the\\n            // previous leader epoch till it finds an entry, If there are no entries till the earliest leader epoch in leader\\n            // epoch cache then it starts copying the segments from the earliest epoch entry\\'s offset.\\n            copiedOffsetOption = Optional.of(findHighestRemoteOffset(topicIdPartition, log));\\n            logger.info(\"Found the highest copiedRemoteOffset: {} for partition: {} after becoming leader, \" + \"leaderEpoch: {}\", copiedOffsetOption, topicIdPartition, leaderEpoch);\\n            copiedOffsetOption.ifPresent(offsetAndEpoch -> log.updateHighestOffsetInRemoteStorage(offsetAndEpoch.offset()));\\n        }\\n    }\\n\\n    /**\\n     *  Segments which match the following criteria are eligible for copying to remote storage:\\n     *  1) Segment is not the active segment and\\n     *  2) Segment end-offset is less than the last-stable-offset as remote storage should contain only\\n     *     committed/acked messages\\n     * @param log The log from which the segments are to be copied\\n     * @param fromOffset The offset from which the segments are to be copied\\n     * @param lastStableOffset The last stable offset of the log\\n     * @return candidate log segments to be copied to remote storage\\n     */\\n    List<EnrichedLogSegment> candidateLogSegments(UnifiedLog log, Long fromOffset, Long lastStableOffset) {\\n        List<EnrichedLogSegment> candidateLogSegments = new ArrayList<>();\\n        List<LogSegment> segments = JavaConverters.seqAsJavaList(log.logSegments(fromOffset, Long.MAX_VALUE).toSeq());\\n        if (!segments.isEmpty()) {\\n            for (int idx = 1; idx < segments.size(); idx++) {\\n                LogSegment previousSeg = segments.get(idx - 1);\\n                LogSegment currentSeg = segments.get(idx);\\n                if (currentSeg.baseOffset() <= lastStableOffset) {\\n                    candidateLogSegments.add(new EnrichedLogSegment(previousSeg, currentSeg.baseOffset()));\\n                }\\n            }\\n            // Discard the last active segment\\n        }\\n        return candidateLogSegments;\\n    }\\n\\n    public void copyLogSegmentsToRemote(UnifiedLog log) throws InterruptedException {\\n        if (isCancelled())\\n            return;\\n        try {\\n            maybeUpdateLogStartOffsetOnBecomingLeader(log);\\n            maybeUpdateCopiedOffset(log);\\n            long copiedOffset = copiedOffsetOption.get().offset();\\n            // LSO indicates the offset below are ready to be consumed (high-watermark or committed)\\n            long lso = log.lastStableOffset();\\n            if (lso < 0) {\\n                logger.warn(\"lastStableOffset for partition {} is {}, which should not be negative.\", topicIdPartition, lso);\\n            } else if (lso > 0 && copiedOffset < lso) {\\n                // log-start-offset can be ahead of the copied-offset, when:\\n                // 1) log-start-offset gets incremented via delete-records API (or)\\n                // 2) enabling the remote log for the first time\\n                long fromOffset = Math.max(copiedOffset + 1, log.logStartOffset());\\n                List<EnrichedLogSegment> candidateLogSegments = candidateLogSegments(log, fromOffset, lso);\\n                logger.debug(\"Candidate log segments, logStartOffset: {}, copiedOffset: {}, fromOffset: {}, lso: {} \" + \"and candidateLogSegments: {}\", log.logStartOffset(), copiedOffset, fromOffset, lso, candidateLogSegments);\\n                if (candidateLogSegments.isEmpty()) {\\n                    logger.debug(\"No segments found to be copied for partition {} with copiedOffset: {} and active segment\\'s base-offset: {}\", topicIdPartition, copiedOffset, log.activeSegment().baseOffset());\\n                } else {\\n                    for (EnrichedLogSegment candidateLogSegment : candidateLogSegments) {\\n                        if (isCancelled() || !isLeader()) {\\n                            logger.info(\"Skipping copying log segments as the current task state is changed, cancelled: {} leader:{}\", isCancelled(), isLeader());\\n                            return;\\n                        }\\n                        copyQuotaManagerLock.lock();\\n                        try {\\n                            while (rlmCopyQuotaManager.isQuotaExceeded()) {\\n                                logger.debug(\"Quota exceeded for copying log segments, waiting for the quota to be available.\");\\n                                // If the thread gets interrupted while waiting, the InterruptedException is thrown\\n                                // back to the caller. It\\'s important to note that the task being executed is already\\n                                // cancelled before the executing thread is interrupted. The caller is responsible\\n                                // for handling the exception gracefully by checking if the task is already cancelled.\\n                                boolean ignored = copyQuotaManagerLockCondition.await(quotaTimeout().toMillis(), TimeUnit.MILLISECONDS);\\n                            }\\n                            rlmCopyQuotaManager.record(candidateLogSegment.logSegment.log().sizeInBytes());\\n                            // Signal waiting threads to check the quota again\\n                            copyQuotaManagerLockCondition.signalAll();\\n                        } finally {\\n                            copyQuotaManagerLock.unlock();\\n                        }\\n                        copyLogSegment(log, candidateLogSegment.logSegment, candidateLogSegment.nextSegmentOffset);\\n                    }\\n                }\\n            } else {\\n                logger.debug(\"Skipping copying segments, current read-offset:{}, and LSO:{}\", copiedOffset, lso);\\n            }\\n        } catch (CustomMetadataSizeLimitExceededException e) {\\n            // Only stop this task. Logging is done where the exception is thrown.\\n            brokerTopicStats.topicStats(log.topicPartition().topic()).failedRemoteCopyRequestRate().mark();\\n            brokerTopicStats.allTopicsStats().failedRemoteCopyRequestRate().mark();\\n            this.cancel();\\n        } catch (InterruptedException | RetriableException ex) {\\n            throw ex;\\n        } catch (Exception ex) {\\n            if (!isCancelled()) {\\n                brokerTopicStats.topicStats(log.topicPartition().topic()).failedRemoteCopyRequestRate().mark();\\n                brokerTopicStats.allTopicsStats().failedRemoteCopyRequestRate().mark();\\n                logger.error(\"Error occurred while copying log segments of partition: {}\", topicIdPartition, ex);\\n            }\\n        }\\n    }\\n\\n    private void copyLogSegment(UnifiedLog log, LogSegment segment, long nextSegmentBaseOffset) throws InterruptedException, ExecutionException, RemoteStorageException, IOException, CustomMetadataSizeLimitExceededException {\\n        File logFile = segment.log().file();\\n        String logFileName = logFile.getName();\\n        logger.info(\"Copying {} to remote storage.\", logFileName);\\n        RemoteLogSegmentId id = RemoteLogSegmentId.generateNew(topicIdPartition);\\n        long endOffset = nextSegmentBaseOffset - 1;\\n        File producerStateSnapshotFile = log.producerStateManager().fetchSnapshot(nextSegmentBaseOffset).orElse(null);\\n        List<EpochEntry> epochEntries = getLeaderEpochEntries(log, segment.baseOffset(), nextSegmentBaseOffset);\\n        Map<Integer, Long> segmentLeaderEpochs = new HashMap<>(epochEntries.size());\\n        epochEntries.forEach(entry -> segmentLeaderEpochs.put(entry.epoch, entry.startOffset));\\n        RemoteLogSegmentMetadata copySegmentStartedRlsm = new RemoteLogSegmentMetadata(id, segment.baseOffset(), endOffset, segment.largestTimestamp(), brokerId, time.milliseconds(), segment.log().sizeInBytes(), segmentLeaderEpochs);\\n        remoteLogMetadataManager.addRemoteLogSegmentMetadata(copySegmentStartedRlsm).get();\\n        ByteBuffer leaderEpochsIndex = epochEntriesAsByteBuffer(getLeaderEpochEntries(log, -1, nextSegmentBaseOffset));\\n        LogSegmentData segmentData = new LogSegmentData(logFile.toPath(), toPathIfExists(segment.offsetIndex().file()), toPathIfExists(segment.timeIndex().file()), Optional.ofNullable(toPathIfExists(segment.txnIndex().file())), producerStateSnapshotFile.toPath(), leaderEpochsIndex);\\n        brokerTopicStats.topicStats(log.topicPartition().topic()).remoteCopyRequestRate().mark();\\n        brokerTopicStats.allTopicsStats().remoteCopyRequestRate().mark();\\n        Optional<CustomMetadata> customMetadata = remoteLogStorageManager.copyLogSegmentData(copySegmentStartedRlsm, segmentData);\\n        RemoteLogSegmentMetadataUpdate copySegmentFinishedRlsm = new RemoteLogSegmentMetadataUpdate(id, time.milliseconds(), customMetadata, RemoteLogSegmentState.COPY_SEGMENT_FINISHED, brokerId);\\n        if (customMetadata.isPresent()) {\\n            long customMetadataSize = customMetadata.get().value().length;\\n            if (customMetadataSize > this.customMetadataSizeLimit) {\\n                CustomMetadataSizeLimitExceededException e = new CustomMetadataSizeLimitExceededException();\\n                logger.error(\"Custom metadata size {} exceeds configured limit {}.\" + \" Copying will be stopped and copied segment will be attempted to clean.\" + \" Original metadata: {}\", customMetadataSize, this.customMetadataSizeLimit, copySegmentStartedRlsm, e);\\n                try {\\n                    // For deletion, we provide back the custom metadata by creating a new metadata object from the update.\\n                    // However, the update itself will not be stored in this case.\\n                    remoteLogStorageManager.deleteLogSegmentData(copySegmentStartedRlsm.createWithUpdates(copySegmentFinishedRlsm));\\n                    logger.info(\"Successfully cleaned segment after custom metadata size exceeded\");\\n                } catch (RemoteStorageException e1) {\\n                    logger.error(\"Error while cleaning segment after custom metadata size exceeded, consider cleaning manually\", e1);\\n                }\\n                throw e;\\n            }\\n        }\\n        remoteLogMetadataManager.updateRemoteLogSegmentMetadata(copySegmentFinishedRlsm).get();\\n        brokerTopicStats.topicStats(log.topicPartition().topic()).remoteCopyBytesRate().mark(copySegmentStartedRlsm.segmentSizeInBytes());\\n        brokerTopicStats.allTopicsStats().remoteCopyBytesRate().mark(copySegmentStartedRlsm.segmentSizeInBytes());\\n        // `epochEntries` cannot be empty, there is a pre-condition validation in RemoteLogSegmentMetadata\\n        // constructor\\n        int lastEpochInSegment = epochEntries.get(epochEntries.size() - 1).epoch;\\n        copiedOffsetOption = Optional.of(new OffsetAndEpoch(endOffset, lastEpochInSegment));\\n        // Update the highest offset in remote storage for this partition\\'s log so that the local log segments\\n        // are not deleted before they are copied to remote storage.\\n        log.updateHighestOffsetInRemoteStorage(endOffset);\\n        logger.info(\"Copied {} to remote storage with segment-id: {}\", logFileName, copySegmentFinishedRlsm.remoteLogSegmentId());\\n        long bytesLag = log.onlyLocalLogSegmentsSize() - log.activeSegment().size();\\n        String topic = topicIdPartition.topic();\\n        int partition = topicIdPartition.partition();\\n        long segmentsLag = log.onlyLocalLogSegmentsCount();\\n        brokerTopicStats.recordRemoteCopyLagBytes(topic, partition, bytesLag);\\n        brokerTopicStats.recordRemoteCopyLagSegments(topic, partition, segmentsLag);\\n    }\\n\\n    private Path toPathIfExists(File file) {\\n        return file.exists() ? file.toPath() : null;\\n    }\\n\\n    public void run() {\\n        if (isCancelled())\\n            return;\\n        try {\\n            Optional<UnifiedLog> unifiedLogOptional = fetchLog.apply(topicIdPartition.topicPartition());\\n            if (!unifiedLogOptional.isPresent()) {\\n                return;\\n            }\\n            UnifiedLog log = unifiedLogOptional.get();\\n            // In the first run after completing altering logDir within broker, we should make sure the state is reset. (KAFKA-16711)\\n            if (!log.parentDir().equals(logDirectory.orElse(null))) {\\n                copiedOffsetOption = Optional.empty();\\n                isLogStartOffsetUpdatedOnBecomingLeader = false;\\n                logDirectory = Optional.of(log.parentDir());\\n            }\\n            if (isLeader()) {\\n                // Copy log segments to remote storage\\n                copyLogSegmentsToRemote(log);\\n                // Cleanup/delete expired remote log segments\\n                cleanupExpiredRemoteLogSegments();\\n            } else {\\n                OffsetAndEpoch offsetAndEpoch = findHighestRemoteOffset(topicIdPartition, log);\\n                // Update the highest offset in remote storage for this partition\\'s log so that the local log segments\\n                // are not deleted before they are copied to remote storage.\\n                log.updateHighestOffsetInRemoteStorage(offsetAndEpoch.offset());\\n            }\\n        } catch (InterruptedException ex) {\\n            if (!isCancelled()) {\\n                logger.warn(\"Current thread for topic-partition-id {} is interrupted\", topicIdPartition, ex);\\n            }\\n        } catch (RetriableException ex) {\\n            logger.debug(\"Encountered a retryable error while executing current task for topic-partition {}\", topicIdPartition, ex);\\n        } catch (Exception ex) {\\n            if (!isCancelled()) {\\n                logger.warn(\"Current task for topic-partition {} received error but it will be scheduled\", topicIdPartition, ex);\\n            }\\n        }\\n    }\\n\\n    public void handleLogStartOffsetUpdate(TopicPartition topicPartition, long remoteLogStartOffset) {\\n        if (isLeader()) {\\n            logger.debug(\"Updating {} with remoteLogStartOffset: {}\", topicPartition, remoteLogStartOffset);\\n            updateRemoteLogStartOffset.accept(topicPartition, remoteLogStartOffset);\\n        }\\n    }\\n\\n    class RemoteLogRetentionHandler {\\n\\n        private final Optional<RetentionSizeData> retentionSizeData;\\n\\n        private final Optional<RetentionTimeData> retentionTimeData;\\n\\n        private long remainingBreachedSize;\\n\\n        private OptionalLong logStartOffset = OptionalLong.empty();\\n\\n        public RemoteLogRetentionHandler(Optional<RetentionSizeData> retentionSizeData, Optional<RetentionTimeData> retentionTimeData) {\\n            this.retentionSizeData = retentionSizeData;\\n            this.retentionTimeData = retentionTimeData;\\n            remainingBreachedSize = retentionSizeData.map(sizeData -> sizeData.remainingBreachedSize).orElse(0L);\\n        }\\n\\n        private boolean isSegmentBreachedByRetentionSize(RemoteLogSegmentMetadata metadata) {\\n            boolean shouldDeleteSegment = false;\\n            if (!retentionSizeData.isPresent()) {\\n                return shouldDeleteSegment;\\n            }\\n            // Assumption that segments contain size >= 0\\n            if (remainingBreachedSize > 0) {\\n                long remainingBytes = remainingBreachedSize - metadata.segmentSizeInBytes();\\n                if (remainingBytes >= 0) {\\n                    remainingBreachedSize = remainingBytes;\\n                    shouldDeleteSegment = true;\\n                }\\n            }\\n            if (shouldDeleteSegment) {\\n                if (!logStartOffset.isPresent() || logStartOffset.getAsLong() < metadata.endOffset() + 1) {\\n                    logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n                }\\n                logger.info(\"About to delete remote log segment {} due to retention size {} breach. Log size after deletion will be {}.\", metadata.remoteLogSegmentId(), retentionSizeData.get().retentionSize, remainingBreachedSize + retentionSizeData.get().retentionSize);\\n            }\\n            return shouldDeleteSegment;\\n        }\\n\\n        public boolean isSegmentBreachedByRetentionTime(RemoteLogSegmentMetadata metadata) {\\n            boolean shouldDeleteSegment = false;\\n            if (!retentionTimeData.isPresent()) {\\n                return shouldDeleteSegment;\\n            }\\n            shouldDeleteSegment = metadata.maxTimestampMs() <= retentionTimeData.get().cleanupUntilMs;\\n            if (shouldDeleteSegment) {\\n                remainingBreachedSize = Math.max(0, remainingBreachedSize - metadata.segmentSizeInBytes());\\n                // It is fine to have logStartOffset as `metadata.endOffset() + 1` as the segment offset intervals\\n                // are ascending with in an epoch.\\n                if (!logStartOffset.isPresent() || logStartOffset.getAsLong() < metadata.endOffset() + 1) {\\n                    logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n                }\\n                logger.info(\"About to delete remote log segment {} due to retention time {}ms breach based on the largest record timestamp in the segment\", metadata.remoteLogSegmentId(), retentionTimeData.get().retentionMs);\\n            }\\n            return shouldDeleteSegment;\\n        }\\n\\n        private boolean isSegmentBreachByLogStartOffset(RemoteLogSegmentMetadata metadata, long logStartOffset, NavigableMap<Integer, Long> leaderEpochEntries) {\\n            boolean shouldDeleteSegment = false;\\n            if (!leaderEpochEntries.isEmpty()) {\\n                // Note that `logStartOffset` and `leaderEpochEntries.firstEntry().getValue()` should be same\\n                Integer firstEpoch = leaderEpochEntries.firstKey();\\n                shouldDeleteSegment = metadata.segmentLeaderEpochs().keySet().stream().allMatch(epoch -> epoch <= firstEpoch) && metadata.endOffset() < logStartOffset;\\n            }\\n            if (shouldDeleteSegment) {\\n                logger.info(\"About to delete remote log segment {} due to log-start-offset {} breach. \" + \"Current earliest-epoch-entry: {}, segment-end-offset: {} and segment-epochs: {}\", metadata.remoteLogSegmentId(), logStartOffset, leaderEpochEntries.firstEntry(), metadata.endOffset(), metadata.segmentLeaderEpochs());\\n            }\\n            return shouldDeleteSegment;\\n        }\\n\\n        // It removes the segments beyond the current leader\\'s earliest epoch. Those segments are considered as\\n        // unreferenced because they are not part of the current leader epoch lineage.\\n        private boolean deleteLogSegmentsDueToLeaderEpochCacheTruncation(EpochEntry earliestEpochEntry, RemoteLogSegmentMetadata metadata) throws RemoteStorageException, ExecutionException, InterruptedException {\\n            boolean isSegmentDeleted = deleteRemoteLogSegment(metadata, ignored -> metadata.segmentLeaderEpochs().keySet().stream().allMatch(epoch -> epoch < earliestEpochEntry.epoch));\\n            if (isSegmentDeleted) {\\n                logger.info(\"Deleted remote log segment {} due to leader-epoch-cache truncation. \" + \"Current earliest-epoch-entry: {}, segment-end-offset: {} and segment-epochs: {}\", metadata.remoteLogSegmentId(), earliestEpochEntry, metadata.endOffset(), metadata.segmentLeaderEpochs().keySet());\\n            }\\n            // No need to update the log-start-offset as these epochs/offsets are earlier to that value.\\n            return isSegmentDeleted;\\n        }\\n\\n        private boolean deleteRemoteLogSegment(RemoteLogSegmentMetadata segmentMetadata, Predicate<RemoteLogSegmentMetadata> predicate) throws RemoteStorageException, ExecutionException, InterruptedException {\\n            if (predicate.test(segmentMetadata)) {\\n                logger.debug(\"Deleting remote log segment {}\", segmentMetadata.remoteLogSegmentId());\\n                String topic = segmentMetadata.topicIdPartition().topic();\\n                // Publish delete segment started event.\\n                remoteLogMetadataManager.updateRemoteLogSegmentMetadata(new RemoteLogSegmentMetadataUpdate(segmentMetadata.remoteLogSegmentId(), time.milliseconds(), segmentMetadata.customMetadata(), RemoteLogSegmentState.DELETE_SEGMENT_STARTED, brokerId)).get();\\n                brokerTopicStats.topicStats(topic).remoteDeleteRequestRate().mark();\\n                brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().mark();\\n                // Delete the segment in remote storage.\\n                try {\\n                    remoteLogStorageManager.deleteLogSegmentData(segmentMetadata);\\n                } catch (RemoteStorageException e) {\\n                    brokerTopicStats.topicStats(topic).failedRemoteDeleteRequestRate().mark();\\n                    brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().mark();\\n                    throw e;\\n                }\\n                // Publish delete segment finished event.\\n                remoteLogMetadataManager.updateRemoteLogSegmentMetadata(new RemoteLogSegmentMetadataUpdate(segmentMetadata.remoteLogSegmentId(), time.milliseconds(), segmentMetadata.customMetadata(), RemoteLogSegmentState.DELETE_SEGMENT_FINISHED, brokerId)).get();\\n                logger.debug(\"Deleted remote log segment {}\", segmentMetadata.remoteLogSegmentId());\\n                return true;\\n            }\\n            return false;\\n        }\\n    }\\n\\n    private void updateMetadataCountAndLogSizeWith(int metadataCount, long remoteLogSizeBytes) {\\n        int partition = topicIdPartition.partition();\\n        String topic = topicIdPartition.topic();\\n        brokerTopicStats.recordRemoteLogMetadataCount(topic, partition, metadataCount);\\n        brokerTopicStats.recordRemoteLogSizeBytes(topic, partition, remoteLogSizeBytes);\\n    }\\n\\n    private void updateRemoteDeleteLagWith(int segmentsLeftToDelete, long sizeOfDeletableSegmentsBytes) {\\n        String topic = topicIdPartition.topic();\\n        int partition = topicIdPartition.partition();\\n        brokerTopicStats.recordRemoteDeleteLagSegments(topic, partition, segmentsLeftToDelete);\\n        brokerTopicStats.recordRemoteDeleteLagBytes(topic, partition, sizeOfDeletableSegmentsBytes);\\n    }\\n\\n    void cleanupExpiredRemoteLogSegments() throws RemoteStorageException, ExecutionException, InterruptedException {\\n        if (isCancelled() || !isLeader()) {\\n            logger.info(\"Returning from remote log segments cleanup as the task state is changed\");\\n            return;\\n        }\\n        final Optional<UnifiedLog> logOptional = fetchLog.apply(topicIdPartition.topicPartition());\\n        if (!logOptional.isPresent()) {\\n            logger.debug(\"No UnifiedLog instance available for partition: {}\", topicIdPartition);\\n            return;\\n        }\\n        final UnifiedLog log = logOptional.get();\\n        final Option<LeaderEpochFileCache> leaderEpochCacheOption = log.leaderEpochCache();\\n        if (leaderEpochCacheOption.isEmpty()) {\\n            logger.debug(\"No leader epoch cache available for partition: {}\", topicIdPartition);\\n            return;\\n        }\\n        // Cleanup remote log segments and update the log start offset if applicable.\\n        final Iterator<RemoteLogSegmentMetadata> segmentMetadataIter = remoteLogMetadataManager.listRemoteLogSegments(topicIdPartition);\\n        if (!segmentMetadataIter.hasNext()) {\\n            updateMetadataCountAndLogSizeWith(0, 0);\\n            logger.debug(\"No remote log segments available on remote storage for partition: {}\", topicIdPartition);\\n            return;\\n        }\\n        final Set<Integer> epochsSet = new HashSet<>();\\n        int metadataCount = 0;\\n        long remoteLogSizeBytes = 0;\\n        // Good to have an API from RLMM to get all the remote leader epochs of all the segments of a partition\\n        // instead of going through all the segments and building it here.\\n        while (segmentMetadataIter.hasNext()) {\\n            RemoteLogSegmentMetadata segmentMetadata = segmentMetadataIter.next();\\n            epochsSet.addAll(segmentMetadata.segmentLeaderEpochs().keySet());\\n            metadataCount++;\\n            remoteLogSizeBytes += segmentMetadata.segmentSizeInBytes();\\n        }\\n        updateMetadataCountAndLogSizeWith(metadataCount, remoteLogSizeBytes);\\n        // All the leader epochs in sorted order that exists in remote storage\\n        final List<Integer> remoteLeaderEpochs = new ArrayList<>(epochsSet);\\n        Collections.sort(remoteLeaderEpochs);\\n        LeaderEpochFileCache leaderEpochCache = leaderEpochCacheOption.get();\\n        // Build the leader epoch map by filtering the epochs that do not have any records.\\n        NavigableMap<Integer, Long> epochWithOffsets = buildFilteredLeaderEpochMap(leaderEpochCache.epochWithOffsets());\\n        long logStartOffset = log.logStartOffset();\\n        long logEndOffset = log.logEndOffset();\\n        Optional<RetentionSizeData> retentionSizeData = buildRetentionSizeData(log.config().retentionSize, log.onlyLocalLogSegmentsSize(), logEndOffset, epochWithOffsets);\\n        Optional<RetentionTimeData> retentionTimeData = buildRetentionTimeData(log.config().retentionMs);\\n        RemoteLogRetentionHandler remoteLogRetentionHandler = new RemoteLogRetentionHandler(retentionSizeData, retentionTimeData);\\n        Iterator<Integer> epochIterator = epochWithOffsets.navigableKeySet().iterator();\\n        boolean canProcess = true;\\n        List<RemoteLogSegmentMetadata> segmentsToDelete = new ArrayList<>();\\n        long sizeOfDeletableSegmentsBytes = 0L;\\n        while (canProcess && epochIterator.hasNext()) {\\n            Integer epoch = epochIterator.next();\\n            Iterator<RemoteLogSegmentMetadata> segmentsIterator = remoteLogMetadataManager.listRemoteLogSegments(topicIdPartition, epoch);\\n            while (canProcess && segmentsIterator.hasNext()) {\\n                if (isCancelled() || !isLeader()) {\\n                    logger.info(\"Returning from remote log segments cleanup for the remaining segments as the task state is changed.\");\\n                    return;\\n                }\\n                RemoteLogSegmentMetadata metadata = segmentsIterator.next();\\n                if (RemoteLogSegmentState.DELETE_SEGMENT_FINISHED.equals(metadata.state())) {\\n                    continue;\\n                }\\n                if (segmentsToDelete.contains(metadata)) {\\n                    continue;\\n                }\\n                // When the log-start-offset is moved by the user, the leader-epoch-checkpoint file gets truncated\\n                // as per the log-start-offset. Until the rlm-cleaner-thread runs in the next iteration, those\\n                // remote log segments won\\'t be removed. The `isRemoteSegmentWithinLeaderEpoch` validates whether\\n                // the epochs present in the segment lies in the checkpoint file. It will always return false\\n                // since the checkpoint file was already truncated.\\n                boolean shouldDeleteSegment = remoteLogRetentionHandler.isSegmentBreachByLogStartOffset(metadata, logStartOffset, epochWithOffsets);\\n                boolean isValidSegment = false;\\n                if (!shouldDeleteSegment) {\\n                    // check whether the segment contains the required epoch range with in the current leader epoch lineage.\\n                    isValidSegment = isRemoteSegmentWithinLeaderEpochs(metadata, logEndOffset, epochWithOffsets);\\n                    if (isValidSegment) {\\n                        shouldDeleteSegment = remoteLogRetentionHandler.isSegmentBreachedByRetentionTime(metadata) || remoteLogRetentionHandler.isSegmentBreachedByRetentionSize(metadata);\\n                    }\\n                }\\n                if (shouldDeleteSegment) {\\n                    segmentsToDelete.add(metadata);\\n                    sizeOfDeletableSegmentsBytes += metadata.segmentSizeInBytes();\\n                }\\n                canProcess = shouldDeleteSegment || !isValidSegment;\\n            }\\n        }\\n        // Update log start offset with the computed value after retention cleanup is done\\n        remoteLogRetentionHandler.logStartOffset.ifPresent(offset -> handleLogStartOffsetUpdate(topicIdPartition.topicPartition(), offset));\\n        // At this point in time we have updated the log start offsets, but not initiated a deletion.\\n        // Either a follower has picked up the changes to the log start offset, or they have not.\\n        // If the follower HAS picked up the changes, and they become the leader this replica won\\'t successfully complete\\n        // the deletion.\\n        // However, the new leader will correctly pick up all breaching segments as log start offset breaching ones\\n        // and delete them accordingly.\\n        // If the follower HAS NOT picked up the changes, and they become the leader then they will go through this process\\n        // again and delete them with the original deletion reason i.e. size, time or log start offset breach.\\n        int segmentsLeftToDelete = segmentsToDelete.size();\\n        updateRemoteDeleteLagWith(segmentsLeftToDelete, sizeOfDeletableSegmentsBytes);\\n        List<String> undeletedSegments = new ArrayList<>();\\n        for (RemoteLogSegmentMetadata segmentMetadata : segmentsToDelete) {\\n            if (!remoteLogRetentionHandler.deleteRemoteLogSegment(segmentMetadata, x -> !isCancelled() && isLeader())) {\\n                undeletedSegments.add(segmentMetadata.remoteLogSegmentId().toString());\\n            } else {\\n                sizeOfDeletableSegmentsBytes -= segmentMetadata.segmentSizeInBytes();\\n                segmentsLeftToDelete--;\\n                updateRemoteDeleteLagWith(segmentsLeftToDelete, sizeOfDeletableSegmentsBytes);\\n            }\\n        }\\n        if (!undeletedSegments.isEmpty()) {\\n            logger.info(\"The following remote segments could not be deleted: {}\", String.join(\",\", undeletedSegments));\\n        }\\n        // Remove the remote log segments whose segment-leader-epochs are less than the earliest-epoch known\\n        // to the leader. This will remove the unreferenced segments in the remote storage. This is needed for\\n        // unclean leader election scenarios as the remote storage can have epochs earlier to the current leader\\'s\\n        // earliest leader epoch.\\n        Optional<EpochEntry> earliestEpochEntryOptional = leaderEpochCache.earliestEntry();\\n        if (earliestEpochEntryOptional.isPresent()) {\\n            EpochEntry earliestEpochEntry = earliestEpochEntryOptional.get();\\n            Iterator<Integer> epochsToClean = remoteLeaderEpochs.stream().filter(remoteEpoch -> remoteEpoch < earliestEpochEntry.epoch).iterator();\\n            List<RemoteLogSegmentMetadata> listOfSegmentsToBeCleaned = new ArrayList<>();\\n            while (epochsToClean.hasNext()) {\\n                int epoch = epochsToClean.next();\\n                Iterator<RemoteLogSegmentMetadata> segmentsToBeCleaned = remoteLogMetadataManager.listRemoteLogSegments(topicIdPartition, epoch);\\n                while (segmentsToBeCleaned.hasNext()) {\\n                    if (!isCancelled() && isLeader()) {\\n                        RemoteLogSegmentMetadata nextSegmentMetadata = segmentsToBeCleaned.next();\\n                        sizeOfDeletableSegmentsBytes += nextSegmentMetadata.segmentSizeInBytes();\\n                        listOfSegmentsToBeCleaned.add(nextSegmentMetadata);\\n                    }\\n                }\\n            }\\n            segmentsLeftToDelete += listOfSegmentsToBeCleaned.size();\\n            updateRemoteDeleteLagWith(segmentsLeftToDelete, sizeOfDeletableSegmentsBytes);\\n            for (RemoteLogSegmentMetadata segmentMetadata : listOfSegmentsToBeCleaned) {\\n                if (!isCancelled() && isLeader()) {\\n                    // No need to update the log-start-offset even though the segment is deleted as these epochs/offsets are earlier to that value.\\n                    if (remoteLogRetentionHandler.deleteLogSegmentsDueToLeaderEpochCacheTruncation(earliestEpochEntry, segmentMetadata)) {\\n                        sizeOfDeletableSegmentsBytes -= segmentMetadata.segmentSizeInBytes();\\n                        segmentsLeftToDelete--;\\n                        updateRemoteDeleteLagWith(segmentsLeftToDelete, sizeOfDeletableSegmentsBytes);\\n                    }\\n                }\\n            }\\n        }\\n    }\\n\\n    private Optional<RetentionTimeData> buildRetentionTimeData(long retentionMs) {\\n        return retentionMs > -1 ? Optional.of(new RetentionTimeData(retentionMs, time.milliseconds() - retentionMs)) : Optional.empty();\\n    }\\n\\n    private Optional<RetentionSizeData> buildRetentionSizeData(long retentionSize, long onlyLocalLogSegmentsSize, long logEndOffset, NavigableMap<Integer, Long> epochEntries) throws RemoteStorageException {\\n        if (retentionSize > -1) {\\n            long startTimeMs = time.milliseconds();\\n            long remoteLogSizeBytes = 0L;\\n            Set<RemoteLogSegmentId> visitedSegmentIds = new HashSet<>();\\n            for (Integer epoch : epochEntries.navigableKeySet()) {\\n                // remoteLogSize(topicIdPartition, epochEntry.epoch) may not be completely accurate as the remote\\n                // log size may be computed for all the segments but not for segments with in the current\\n                // partition\\'s leader epoch lineage. Better to revisit this API.\\n                // remoteLogSizeBytes += remoteLogMetadataManager.remoteLogSize(topicIdPartition, epochEntry.epoch);\\n                Iterator<RemoteLogSegmentMetadata> segmentsIterator = remoteLogMetadataManager.listRemoteLogSegments(topicIdPartition, epoch);\\n                while (segmentsIterator.hasNext()) {\\n                    RemoteLogSegmentMetadata segmentMetadata = segmentsIterator.next();\\n                    RemoteLogSegmentId segmentId = segmentMetadata.remoteLogSegmentId();\\n                    if (!visitedSegmentIds.contains(segmentId) && isRemoteSegmentWithinLeaderEpochs(segmentMetadata, logEndOffset, epochEntries)) {\\n                        remoteLogSizeBytes += segmentMetadata.segmentSizeInBytes();\\n                        visitedSegmentIds.add(segmentId);\\n                    }\\n                }\\n            }\\n            brokerTopicStats.recordRemoteLogSizeComputationTime(topicIdPartition.topic(), topicIdPartition.partition(), time.milliseconds() - startTimeMs);\\n            // This is the total size of segments in local log that have their base-offset > local-log-start-offset\\n            // and size of the segments in remote storage which have their end-offset < local-log-start-offset.\\n            long totalSize = onlyLocalLogSegmentsSize + remoteLogSizeBytes;\\n            if (totalSize > retentionSize) {\\n                long remainingBreachedSize = totalSize - retentionSize;\\n                RetentionSizeData retentionSizeData = new RetentionSizeData(retentionSize, remainingBreachedSize);\\n                return Optional.of(retentionSizeData);\\n            }\\n        }\\n        return Optional.empty();\\n    }\\n\\n    public String toString() {\\n        return this.getClass() + \"[\" + topicIdPartition + \"]\";\\n    }\\n}\\n}\\n\\ncore/src/test/java/kafka/log/remote/RemoteLogManagerTest.java (Before)\\npublic class RemoteLogManagerTest {\\n@ParameterizedTest(name = \"testDeletionOnRetentionBreachedSegments retentionSize={0} retentionMs={1}\")\\n@CsvSource(value = { \"0, -1\", \"-1, 0\" })\\npublic void testDeletionOnRetentionBreachedSegments(long retentionSize, long retentionMs) throws RemoteStorageException, ExecutionException, InterruptedException {\\n    Map<String, Long> logProps = new HashMap<>();\\n    logProps.put(\"retention.bytes\", retentionSize);\\n    logProps.put(\"retention.ms\", retentionMs);\\n    LogConfig mockLogConfig = new LogConfig(logProps);\\n    when(mockLog.config()).thenReturn(mockLogConfig);\\n    List<EpochEntry> epochEntries = Collections.singletonList(epochEntry0);\\n    checkpoint.write(epochEntries);\\n    LeaderEpochFileCache cache = new LeaderEpochFileCache(tp, checkpoint, scheduler);\\n    when(mockLog.leaderEpochCache()).thenReturn(Option.apply(cache));\\n    when(mockLog.topicPartition()).thenReturn(leaderTopicIdPartition.topicPartition());\\n    when(mockLog.logEndOffset()).thenReturn(200L);\\n    List<RemoteLogSegmentMetadata> metadataList = listRemoteLogSegmentMetadata(leaderTopicIdPartition, 2, 100, 1024, epochEntries, RemoteLogSegmentState.COPY_SEGMENT_FINISHED);\\n    when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition)).thenReturn(metadataList.iterator());\\n    when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition, 0)).thenAnswer(ans -> metadataList.iterator());\\n    when(remoteLogMetadataManager.updateRemoteLogSegmentMetadata(any(RemoteLogSegmentMetadataUpdate.class))).thenReturn(CompletableFuture.runAsync(() -> {\\n    }));\\n    // Verify the metrics for remote deletes and for failures is zero before attempt to delete segments\\n    assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n    assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n    // Verify aggregate metrics\\n    assertEquals(0, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n    assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n    RemoteLogManager.RLMTask task = remoteLogManager.new RLMTask(leaderTopicIdPartition, 128);\\n    task.convertToLeader(0);\\n    task.cleanupExpiredRemoteLogSegments();\\n    assertEquals(200L, currentLogStartOffset.get());\\n    verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(0));\\n    verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(1));\\n    // Verify the metric for remote delete is updated correctly\\n    assertEquals(2, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n    // Verify we did not report any failure for remote deletes\\n    assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n    // Verify aggregate metrics\\n    assertEquals(2, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n    assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n}\\n@ParameterizedTest(name = \"testRemoteDeleteLagsOnRetentionBreachedSegments retentionSize={0} retentionMs={1}\")\\n@CsvSource(value = { \"0, -1\", \"-1, 0\" })\\npublic void testRemoteDeleteLagsOnRetentionBreachedSegments(long retentionSize, long retentionMs) throws RemoteStorageException, ExecutionException, InterruptedException {\\n    Map<String, Long> logProps = new HashMap<>();\\n    logProps.put(\"retention.bytes\", retentionSize);\\n    logProps.put(\"retention.ms\", retentionMs);\\n    LogConfig mockLogConfig = new LogConfig(logProps);\\n    when(mockLog.config()).thenReturn(mockLogConfig);\\n    List<EpochEntry> epochEntries = Collections.singletonList(epochEntry0);\\n    checkpoint.write(epochEntries);\\n    LeaderEpochFileCache cache = new LeaderEpochFileCache(tp, checkpoint, scheduler);\\n    when(mockLog.leaderEpochCache()).thenReturn(Option.apply(cache));\\n    when(mockLog.topicPartition()).thenReturn(leaderTopicIdPartition.topicPartition());\\n    when(mockLog.logEndOffset()).thenReturn(200L);\\n    List<RemoteLogSegmentMetadata> metadataList = listRemoteLogSegmentMetadata(leaderTopicIdPartition, 2, 100, 1024, epochEntries, RemoteLogSegmentState.COPY_SEGMENT_FINISHED);\\n    when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition)).thenReturn(metadataList.iterator());\\n    when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition, 0)).thenAnswer(ans -> metadataList.iterator());\\n    when(remoteLogMetadataManager.updateRemoteLogSegmentMetadata(any(RemoteLogSegmentMetadataUpdate.class))).thenReturn(CompletableFuture.runAsync(() -> {\\n    }));\\n    doAnswer(ans -> {\\n        assertEquals(2048, safeLongYammerMetricValue(\"RemoteDeleteLagBytes\"), String.format(\"Expected to find 2048 for RemoteDeleteLagBytes metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagBytes\")));\\n        assertEquals(2048, safeLongYammerMetricValue(\"RemoteDeleteLagBytes,topic=\" + leaderTopic), String.format(\"Expected to find 2048 for RemoteDeleteLagBytes for \\'Leader\\' topic metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagBytes,topic=\" + leaderTopic)));\\n        assertEquals(2, safeLongYammerMetricValue(\"RemoteDeleteLagSegments\"), String.format(\"Expected to find 2 for RemoteDeleteLagSegments metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagSegments\")));\\n        assertEquals(2, safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic), String.format(\"Expected to find 2 for RemoteDeleteLagSegments for \\'Leader\\' topic metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic)));\\n        return Optional.empty();\\n    }).doAnswer(ans -> {\\n        assertEquals(1024, safeLongYammerMetricValue(\"RemoteDeleteLagBytes\"), String.format(\"Expected to find 1024 for RemoteDeleteLagBytes metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagBytes\")));\\n        assertEquals(1, safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic), String.format(\"Expected to find 1 for RemoteDeleteLagSegments for \\'Leader\\' topic metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic)));\\n        assertEquals(1024, safeLongYammerMetricValue(\"RemoteDeleteLagBytes\"), String.format(\"Expected to find 1024 for RemoteDeleteLagBytes metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagBytes\")));\\n        assertEquals(1, safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic), String.format(\"Expected to find 1 for RemoteDeleteLagSegments for \\'Leader\\' topic metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic)));\\n        return Optional.empty();\\n    }).when(remoteStorageManager).deleteLogSegmentData(any(RemoteLogSegmentMetadata.class));\\n    RemoteLogManager.RLMTask task = remoteLogManager.new RLMTask(leaderTopicIdPartition, 128);\\n    assertEquals(0L, yammerMetricValue(\"RemoteDeleteLagBytes\"));\\n    assertEquals(0L, yammerMetricValue(\"RemoteDeleteLagSegments\"));\\n    assertEquals(0L, safeLongYammerMetricValue(\"RemoteDeleteLagBytes,topic=\" + leaderTopic));\\n    assertEquals(0L, safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic));\\n    task.convertToLeader(0);\\n    task.cleanupExpiredRemoteLogSegments();\\n    assertEquals(200L, currentLogStartOffset.get());\\n    verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(0));\\n    verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(1));\\n}\\n}\\n\\ncore/src/test/java/kafka/log/remote/RemoteLogManagerTest.java (After)\\npublic class RemoteLogManagerTest {\\n@ParameterizedTest(name = \"testDeletionOnRetentionBreachedSegments retentionSize={0} retentionMs={1}\")\\n@CsvSource(value = { \"0, -1\", \"-1, 0\" })\\npublic void testDeletionOnRetentionBreachedSegments(long retentionSize, long retentionMs) throws RemoteStorageException, ExecutionException, InterruptedException {\\n    Map<String, Long> logProps = new HashMap<>();\\n    logProps.put(\"retention.bytes\", retentionSize);\\n    logProps.put(\"retention.ms\", retentionMs);\\n    LogConfig mockLogConfig = new LogConfig(logProps);\\n    when(mockLog.config()).thenReturn(mockLogConfig);\\n    List<EpochEntry> epochEntries = Collections.singletonList(epochEntry0);\\n    checkpoint.write(epochEntries);\\n    LeaderEpochFileCache cache = new LeaderEpochFileCache(tp, checkpoint, scheduler);\\n    when(mockLog.leaderEpochCache()).thenReturn(Option.apply(cache));\\n    when(mockLog.topicPartition()).thenReturn(leaderTopicIdPartition.topicPartition());\\n    when(mockLog.logEndOffset()).thenReturn(200L);\\n    List<RemoteLogSegmentMetadata> metadataList = listRemoteLogSegmentMetadata(leaderTopicIdPartition, 2, 100, 1024, epochEntries, RemoteLogSegmentState.COPY_SEGMENT_FINISHED);\\n    when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition)).thenReturn(metadataList.iterator());\\n    when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition, 0)).thenAnswer(ans -> metadataList.iterator());\\n    when(remoteLogMetadataManager.updateRemoteLogSegmentMetadata(any(RemoteLogSegmentMetadataUpdate.class))).thenReturn(CompletableFuture.runAsync(() -> {\\n    }));\\n    // Verify the metrics for remote deletes and for failures is zero before attempt to delete segments\\n    assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n    assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n    // Verify aggregate metrics\\n    assertEquals(0, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n    assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n    RemoteLogManager.RLMTask task = remoteLogManager.new RLMTask(leaderTopicIdPartition, 128);\\n    task.convertToLeader(0);\\n    task.cleanupExpiredRemoteLogSegments();\\n    assertEquals(200L, currentLogStartOffset.get());\\n    verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(0));\\n    verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(1));\\n    // Verify the metric for remote delete is updated correctly\\n    assertEquals(2, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n    // Verify we did not report any failure for remote deletes\\n    assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n    // Verify aggregate metrics\\n    assertEquals(2, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n    assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n}\\n@ParameterizedTest(name = \"testDeletionOnOverlappingRetentionBreachedSegments retentionSize={0} retentionMs={1}\")\\n@CsvSource(value = { \"0, -1\", \"-1, 0\" })\\npublic void testDeletionOnOverlappingRetentionBreachedSegments(long retentionSize, long retentionMs) throws RemoteStorageException, ExecutionException, InterruptedException {\\n    Map<String, Long> logProps = new HashMap<>();\\n    logProps.put(\"retention.bytes\", retentionSize);\\n    logProps.put(\"retention.ms\", retentionMs);\\n    LogConfig mockLogConfig = new LogConfig(logProps);\\n    when(mockLog.config()).thenReturn(mockLogConfig);\\n    List<EpochEntry> epochEntries = Collections.singletonList(epochEntry0);\\n    checkpoint.write(epochEntries);\\n    LeaderEpochFileCache cache = new LeaderEpochFileCache(tp, checkpoint, scheduler);\\n    when(mockLog.leaderEpochCache()).thenReturn(Option.apply(cache));\\n    when(mockLog.topicPartition()).thenReturn(leaderTopicIdPartition.topicPartition());\\n    when(mockLog.logEndOffset()).thenReturn(200L);\\n    RemoteLogSegmentMetadata metadata1 = listRemoteLogSegmentMetadata(leaderTopicIdPartition, 1, 100, 1024, epochEntries, RemoteLogSegmentState.COPY_SEGMENT_FINISHED).get(0);\\n    // overlapping segment\\n    RemoteLogSegmentMetadata metadata2 = new RemoteLogSegmentMetadata(new RemoteLogSegmentId(leaderTopicIdPartition, Uuid.randomUuid()), metadata1.startOffset(), metadata1.endOffset() + 5, metadata1.maxTimestampMs(), metadata1.brokerId() + 1, metadata1.eventTimestampMs(), metadata1.segmentSizeInBytes() + 128, metadata1.customMetadata(), metadata1.state(), metadata1.segmentLeaderEpochs());\\n    // When there are overlapping/duplicate segments, the RemoteLogMetadataManager#listRemoteLogSegments\\n    // returns the segments in order of (valid ++ unreferenced) segments:\\n    // (eg) B0 uploaded segment S0 with offsets 0-100 and B1 uploaded segment S1 with offsets 0-200.\\n    //      We will mark the segment S0 as duplicate and add it to unreferencedSegmentIds.\\n    //      The order of segments returned by listRemoteLogSegments will be S1, S0.\\n    // While computing the next-log-start-offset, taking the max of deleted segment\\'s end-offset + 1.\\n    List<RemoteLogSegmentMetadata> metadataList = new ArrayList<>();\\n    metadataList.add(metadata2);\\n    metadataList.add(metadata1);\\n    when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition)).thenReturn(metadataList.iterator());\\n    when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition, 0)).thenAnswer(ans -> metadataList.iterator());\\n    when(remoteLogMetadataManager.updateRemoteLogSegmentMetadata(any(RemoteLogSegmentMetadataUpdate.class))).thenReturn(CompletableFuture.runAsync(() -> {\\n    }));\\n    // Verify the metrics for remote deletes and for failures is zero before attempt to delete segments\\n    assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n    assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n    // Verify aggregate metrics\\n    assertEquals(0, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n    assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n    RemoteLogManager.RLMTask task = remoteLogManager.new RLMTask(leaderTopicIdPartition, 128);\\n    task.convertToLeader(0);\\n    task.cleanupExpiredRemoteLogSegments();\\n    assertEquals(metadata2.endOffset() + 1, currentLogStartOffset.get());\\n    verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(0));\\n    verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(1));\\n    // Verify the metric for remote delete is updated correctly\\n    assertEquals(2, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n    // Verify we did not report any failure for remote deletes\\n    assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n    // Verify aggregate metrics\\n    assertEquals(2, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n    assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n}\\n@ParameterizedTest(name = \"testRemoteDeleteLagsOnRetentionBreachedSegments retentionSize={0} retentionMs={1}\")\\n@CsvSource(value = { \"0, -1\", \"-1, 0\" })\\npublic void testRemoteDeleteLagsOnRetentionBreachedSegments(long retentionSize, long retentionMs) throws RemoteStorageException, ExecutionException, InterruptedException {\\n    Map<String, Long> logProps = new HashMap<>();\\n    logProps.put(\"retention.bytes\", retentionSize);\\n    logProps.put(\"retention.ms\", retentionMs);\\n    LogConfig mockLogConfig = new LogConfig(logProps);\\n    when(mockLog.config()).thenReturn(mockLogConfig);\\n    List<EpochEntry> epochEntries = Collections.singletonList(epochEntry0);\\n    checkpoint.write(epochEntries);\\n    LeaderEpochFileCache cache = new LeaderEpochFileCache(tp, checkpoint, scheduler);\\n    when(mockLog.leaderEpochCache()).thenReturn(Option.apply(cache));\\n    when(mockLog.topicPartition()).thenReturn(leaderTopicIdPartition.topicPartition());\\n    when(mockLog.logEndOffset()).thenReturn(200L);\\n    List<RemoteLogSegmentMetadata> metadataList = listRemoteLogSegmentMetadata(leaderTopicIdPartition, 2, 100, 1024, epochEntries, RemoteLogSegmentState.COPY_SEGMENT_FINISHED);\\n    when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition)).thenReturn(metadataList.iterator());\\n    when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition, 0)).thenAnswer(ans -> metadataList.iterator());\\n    when(remoteLogMetadataManager.updateRemoteLogSegmentMetadata(any(RemoteLogSegmentMetadataUpdate.class))).thenReturn(CompletableFuture.runAsync(() -> {\\n    }));\\n    doAnswer(ans -> {\\n        assertEquals(2048, safeLongYammerMetricValue(\"RemoteDeleteLagBytes\"), String.format(\"Expected to find 2048 for RemoteDeleteLagBytes metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagBytes\")));\\n        assertEquals(2048, safeLongYammerMetricValue(\"RemoteDeleteLagBytes,topic=\" + leaderTopic), String.format(\"Expected to find 2048 for RemoteDeleteLagBytes for \\'Leader\\' topic metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagBytes,topic=\" + leaderTopic)));\\n        assertEquals(2, safeLongYammerMetricValue(\"RemoteDeleteLagSegments\"), String.format(\"Expected to find 2 for RemoteDeleteLagSegments metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagSegments\")));\\n        assertEquals(2, safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic), String.format(\"Expected to find 2 for RemoteDeleteLagSegments for \\'Leader\\' topic metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic)));\\n        return Optional.empty();\\n    }).doAnswer(ans -> {\\n        assertEquals(1024, safeLongYammerMetricValue(\"RemoteDeleteLagBytes\"), String.format(\"Expected to find 1024 for RemoteDeleteLagBytes metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagBytes\")));\\n        assertEquals(1, safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic), String.format(\"Expected to find 1 for RemoteDeleteLagSegments for \\'Leader\\' topic metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic)));\\n        assertEquals(1024, safeLongYammerMetricValue(\"RemoteDeleteLagBytes\"), String.format(\"Expected to find 1024 for RemoteDeleteLagBytes metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagBytes\")));\\n        assertEquals(1, safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic), String.format(\"Expected to find 1 for RemoteDeleteLagSegments for \\'Leader\\' topic metric value, but found %d\", safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic)));\\n        return Optional.empty();\\n    }).when(remoteStorageManager).deleteLogSegmentData(any(RemoteLogSegmentMetadata.class));\\n    RemoteLogManager.RLMTask task = remoteLogManager.new RLMTask(leaderTopicIdPartition, 128);\\n    assertEquals(0L, yammerMetricValue(\"RemoteDeleteLagBytes\"));\\n    assertEquals(0L, yammerMetricValue(\"RemoteDeleteLagSegments\"));\\n    assertEquals(0L, safeLongYammerMetricValue(\"RemoteDeleteLagBytes,topic=\" + leaderTopic));\\n    assertEquals(0L, safeLongYammerMetricValue(\"RemoteDeleteLagSegments,topic=\" + leaderTopic));\\n    task.convertToLeader(0);\\n    task.cleanupExpiredRemoteLogSegments();\\n    assertEquals(200L, currentLogStartOffset.get());\\n    verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(0));\\n    verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(1));\\n}\\n}\\n\\n\\nSummary:', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Given a Git diff and the relevant source code, write a concise summary of the code changes in a way that a non-technical person can understand. Summarize in maximum three concise sentences. \\n\\nAvoid adding any additional comments or annotations to the summary.\\n\\nGit diff:\\ndiff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java\\nindex 87949e36b34..e60e645b3bb 100644\\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java\\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java\\n@@ -65,6 +65,8 @@ public enum AzureServiceErrorCode {\\n   COPY_BLOB_ABORTED(\"CopyBlobAborted\", HttpURLConnection.HTTP_INTERNAL_ERROR, null),\\n   BLOB_OPERATION_NOT_SUPPORTED(\"BlobOperationNotSupported\", HttpURLConnection.HTTP_CONFLICT, null),\\n   INVALID_APPEND_OPERATION(\"InvalidAppendOperation\", HttpURLConnection.HTTP_CONFLICT, null),\\n+  UNAUTHORIZED_BLOB_OVERWRITE(\"UnauthorizedBlobOverwrite\", HttpURLConnection.HTTP_FORBIDDEN,\\n+          \"This request is not authorized to perform blob overwrites.\"),\\n   UNKNOWN(null, -1, null);\\n \\n   private final String errorCode;\\ndiff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java\\nindex 8505b533ce4..f571cb86cca 100644\\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java\\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java\\n@@ -42,6 +42,7 @@\\n \\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\\n import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\\n import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore;\\n import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\\n@@ -132,6 +133,8 @@\\n import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_RETAIN_UNCOMMITTED_DATA;\\n import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.RENAME_DESTINATION_PARENT_PATH_NOT_FOUND;\\n import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.SOURCE_PATH_NOT_FOUND;\\n+import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.UNAUTHORIZED_BLOB_OVERWRITE;\\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_FILE_ALREADY_EXISTS;\\n \\n /**\\n  * AbfsClient interacting with the DFS Endpoint.\\n@@ -702,6 +705,14 @@ public AbfsClientRenameResult renamePath(\\n         throw e;\\n       }\\n \\n+      // ref: HADOOP-19393. Write permission checks can occur before validating\\n+      // rename operation\\'s validity. If there is an existing destination path, it may be rejected\\n+      // with an authorization error. Catching and throwing FileAlreadyExistsException instead.\\n+      if (op.getResult().getStorageErrorCode()\\n+          .equals(UNAUTHORIZED_BLOB_OVERWRITE.getErrorCode())){\\n+        throw new FileAlreadyExistsException(ERR_FILE_ALREADY_EXISTS);\\n+      }\\n+\\n       // ref: HADOOP-18242. Rename failure occurring due to a rare case of\\n       // tracking metadata being in incomplete state.\\n       if (op.getResult().getStorageErrorCode()\\ndiff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java\\nindex a5864290b13..87c8869d6c6 100644\\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java\\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java\\n@@ -28,6 +28,7 @@\\n @InterfaceAudience.Public\\n @InterfaceStability.Evolving\\n public final class AbfsErrors {\\n+  public static final String ERR_FILE_ALREADY_EXISTS = \"File already exists.\";\\n   public static final String ERR_WRITE_WITHOUT_LEASE = \"Attempted to write to file without lease\";\\n   public static final String ERR_LEASE_EXPIRED = \"A lease ID was specified, but the lease for the resource has expired.\";\\n   public static final String ERR_LEASE_EXPIRED_BLOB = \"A lease ID was specified, but the lease for the blob has expired.\";\\ndiff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java\\nindex d9b6f0f123f..70e5b23eadd 100644\\n--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java\\n+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java\\n@@ -34,6 +34,7 @@\\n \\n import org.apache.hadoop.fs.FSDataInputStream;\\n import org.apache.hadoop.fs.FSDataOutputStream;\\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\\n import org.apache.hadoop.fs.FileStatus;\\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n@@ -52,6 +53,7 @@\\n \\n import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_SAS_TOKEN_PROVIDER_TYPE;\\n import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.AUTHORIZATION_PERMISSION_MISS_MATCH;\\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_FILE_ALREADY_EXISTS;\\n import static org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers.aclEntry;\\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathDoesNotExist;\\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathExists;\\n@@ -213,6 +215,18 @@ public void testReadAndWrite() throws Exception {\\n     }\\n   }\\n \\n+  @Test\\n+  public void checkExceptionForRenameOverwrites() throws Exception {\\n+    final AzureBlobFileSystem fs = getFileSystem();\\n+\\n+    Path src = new Path(\"a/b/f1.txt\");\\n+    Path dest = new Path(\"a/b/f2.txt\");\\n+    touch(src);\\n+    touch(dest);\\n+\\n+    intercept(FileAlreadyExistsException.class, ERR_FILE_ALREADY_EXISTS, () -> fs.rename(src, dest));\\n+  }\\n+\\n   @Test\\n   // Test rename file and rename folder\\n   public void testRename() throws Exception {\\n\\n\\nSource code:\\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java (Before)\\n/**\\n * Azure service error codes.\\n */\\n@InterfaceAudience.Public\\n@InterfaceStability.Evolving\\npublic enum AzureServiceErrorCode {\\n\\n    FILE_SYSTEM_ALREADY_EXISTS(\"FilesystemAlreadyExists\", HttpURLConnection.HTTP_CONFLICT, null),\\n    PATH_ALREADY_EXISTS(\"PathAlreadyExists\", HttpURLConnection.HTTP_CONFLICT, null),\\n    BLOB_ALREADY_EXISTS(\"BlobAlreadyExists\", HttpURLConnection.HTTP_CONFLICT, null),\\n    INTERNAL_OPERATION_ABORT(\"InternalOperationAbortError\", HttpURLConnection.HTTP_CONFLICT, null),\\n    PATH_CONFLICT(\"PathConflict\", HttpURLConnection.HTTP_CONFLICT, null),\\n    FILE_SYSTEM_NOT_FOUND(\"FilesystemNotFound\", HttpURLConnection.HTTP_NOT_FOUND, null),\\n    PATH_NOT_FOUND(\"PathNotFound\", HttpURLConnection.HTTP_NOT_FOUND, null),\\n    BLOB_PATH_NOT_FOUND(\"BlobNotFound\", HttpURLConnection.HTTP_NOT_FOUND, null),\\n    PRE_CONDITION_FAILED(\"PreconditionFailed\", HttpURLConnection.HTTP_PRECON_FAILED, null),\\n    SOURCE_PATH_NOT_FOUND(\"SourcePathNotFound\", HttpURLConnection.HTTP_NOT_FOUND, null),\\n    INVALID_SOURCE_OR_DESTINATION_RESOURCE_TYPE(\"InvalidSourceOrDestinationResourceType\", HttpURLConnection.HTTP_CONFLICT, null),\\n    RENAME_DESTINATION_PARENT_PATH_NOT_FOUND(\"RenameDestinationParentPathNotFound\", HttpURLConnection.HTTP_NOT_FOUND, null),\\n    INVALID_RENAME_SOURCE_PATH(\"InvalidRenameSourcePath\", HttpURLConnection.HTTP_CONFLICT, null),\\n    NON_EMPTY_DIRECTORY_DELETE(\"DirectoryNotEmpty\", HttpURLConnection.HTTP_CONFLICT, \"The recursive query parameter value must be true to delete a non-empty directory\"),\\n    INGRESS_OVER_ACCOUNT_LIMIT(\"ServerBusy\", HttpURLConnection.HTTP_UNAVAILABLE, \"Ingress is over the account limit.\"),\\n    EGRESS_OVER_ACCOUNT_LIMIT(\"ServerBusy\", HttpURLConnection.HTTP_UNAVAILABLE, \"Egress is over the account limit.\"),\\n    TPS_OVER_ACCOUNT_LIMIT(\"ServerBusy\", HttpURLConnection.HTTP_UNAVAILABLE, \"Operations per second is over the account limit.\"),\\n    OTHER_SERVER_THROTTLING(\"ServerBusy\", HttpURLConnection.HTTP_UNAVAILABLE, \"The server is currently unable to receive requests. Please retry your request.\"),\\n    INVALID_QUERY_PARAMETER_VALUE(\"InvalidQueryParameterValue\", HttpURLConnection.HTTP_BAD_REQUEST, null),\\n    INVALID_RENAME_DESTINATION(\"InvalidRenameDestinationPath\", HttpURLConnection.HTTP_BAD_REQUEST, null),\\n    AUTHORIZATION_PERMISSION_MISS_MATCH(\"AuthorizationPermissionMismatch\", HttpURLConnection.HTTP_FORBIDDEN, null),\\n    ACCOUNT_REQUIRES_HTTPS(\"AccountRequiresHttps\", HttpURLConnection.HTTP_BAD_REQUEST, null),\\n    MD5_MISMATCH(\"Md5Mismatch\", HttpURLConnection.HTTP_BAD_REQUEST, \"The MD5 value specified in the request did not match with the MD5 value calculated by the server.\"),\\n    COPY_BLOB_FAILED(\"CopyBlobFailed\", HttpURLConnection.HTTP_INTERNAL_ERROR, null),\\n    COPY_BLOB_ABORTED(\"CopyBlobAborted\", HttpURLConnection.HTTP_INTERNAL_ERROR, null),\\n    BLOB_OPERATION_NOT_SUPPORTED(\"BlobOperationNotSupported\", HttpURLConnection.HTTP_CONFLICT, null),\\n    INVALID_APPEND_OPERATION(\"InvalidAppendOperation\", HttpURLConnection.HTTP_CONFLICT, null),\\n    UNKNOWN(null, -1, null);\\n\\n    private final String errorCode;\\n\\n    private final int httpStatusCode;\\n\\n    private final String errorMessage;\\n\\n    private static final Logger LOG1 = LoggerFactory.getLogger(AzureServiceErrorCode.class);\\n\\n    AzureServiceErrorCode(String errorCode, int httpStatusCodes, String errorMessage) {\\n        this.errorCode = errorCode;\\n        this.httpStatusCode = httpStatusCodes;\\n        this.errorMessage = errorMessage;\\n    }\\n\\n    public int getStatusCode() {\\n        return this.httpStatusCode;\\n    }\\n\\n    public String getErrorCode() {\\n        return this.errorCode;\\n    }\\n\\n    public String getErrorMessage() {\\n        return this.errorMessage;\\n    }\\n\\n    public static List<AzureServiceErrorCode> getAzureServiceCode(int httpStatusCode) {\\n        List<AzureServiceErrorCode> errorCodes = new ArrayList<>();\\n        if (httpStatusCode == UNKNOWN.httpStatusCode) {\\n            errorCodes.add(UNKNOWN);\\n            return errorCodes;\\n        }\\n        for (AzureServiceErrorCode azureServiceErrorCode : AzureServiceErrorCode.values()) {\\n            if (azureServiceErrorCode.httpStatusCode == httpStatusCode) {\\n                errorCodes.add(azureServiceErrorCode);\\n            }\\n        }\\n        return errorCodes;\\n    }\\n\\n    public static AzureServiceErrorCode getAzureServiceCode(int httpStatusCode, String errorCode) {\\n        if (errorCode == null || errorCode.isEmpty() || httpStatusCode == UNKNOWN.httpStatusCode) {\\n            return UNKNOWN;\\n        }\\n        for (AzureServiceErrorCode azureServiceErrorCode : AzureServiceErrorCode.values()) {\\n            if (errorCode.equalsIgnoreCase(azureServiceErrorCode.errorCode) && azureServiceErrorCode.httpStatusCode == httpStatusCode) {\\n                return azureServiceErrorCode;\\n            }\\n        }\\n        return UNKNOWN;\\n    }\\n\\n    public static AzureServiceErrorCode getAzureServiceCode(int httpStatusCode, String errorCode, final String errorMessage) {\\n        if (errorCode == null || errorCode.isEmpty() || httpStatusCode == UNKNOWN.httpStatusCode || errorMessage == null || errorMessage.isEmpty()) {\\n            return UNKNOWN;\\n        }\\n        String[] errorMessages = errorMessage.split(System.lineSeparator(), 2);\\n        for (AzureServiceErrorCode azureServiceErrorCode : AzureServiceErrorCode.values()) {\\n            if (azureServiceErrorCode.getStatusCode() == httpStatusCode && azureServiceErrorCode.getErrorCode().equalsIgnoreCase(errorCode) && azureServiceErrorCode.getErrorMessage().equalsIgnoreCase(errorMessages[0])) {\\n                return azureServiceErrorCode;\\n            }\\n        }\\n        return UNKNOWN;\\n    }\\n}\\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java (After)\\n/**\\n * Azure service error codes.\\n */\\n@InterfaceAudience.Public\\n@InterfaceStability.Evolving\\npublic enum AzureServiceErrorCode {\\n\\n    FILE_SYSTEM_ALREADY_EXISTS(\"FilesystemAlreadyExists\", HttpURLConnection.HTTP_CONFLICT, null),\\n    PATH_ALREADY_EXISTS(\"PathAlreadyExists\", HttpURLConnection.HTTP_CONFLICT, null),\\n    BLOB_ALREADY_EXISTS(\"BlobAlreadyExists\", HttpURLConnection.HTTP_CONFLICT, null),\\n    INTERNAL_OPERATION_ABORT(\"InternalOperationAbortError\", HttpURLConnection.HTTP_CONFLICT, null),\\n    PATH_CONFLICT(\"PathConflict\", HttpURLConnection.HTTP_CONFLICT, null),\\n    FILE_SYSTEM_NOT_FOUND(\"FilesystemNotFound\", HttpURLConnection.HTTP_NOT_FOUND, null),\\n    PATH_NOT_FOUND(\"PathNotFound\", HttpURLConnection.HTTP_NOT_FOUND, null),\\n    BLOB_PATH_NOT_FOUND(\"BlobNotFound\", HttpURLConnection.HTTP_NOT_FOUND, null),\\n    PRE_CONDITION_FAILED(\"PreconditionFailed\", HttpURLConnection.HTTP_PRECON_FAILED, null),\\n    SOURCE_PATH_NOT_FOUND(\"SourcePathNotFound\", HttpURLConnection.HTTP_NOT_FOUND, null),\\n    INVALID_SOURCE_OR_DESTINATION_RESOURCE_TYPE(\"InvalidSourceOrDestinationResourceType\", HttpURLConnection.HTTP_CONFLICT, null),\\n    RENAME_DESTINATION_PARENT_PATH_NOT_FOUND(\"RenameDestinationParentPathNotFound\", HttpURLConnection.HTTP_NOT_FOUND, null),\\n    INVALID_RENAME_SOURCE_PATH(\"InvalidRenameSourcePath\", HttpURLConnection.HTTP_CONFLICT, null),\\n    NON_EMPTY_DIRECTORY_DELETE(\"DirectoryNotEmpty\", HttpURLConnection.HTTP_CONFLICT, \"The recursive query parameter value must be true to delete a non-empty directory\"),\\n    INGRESS_OVER_ACCOUNT_LIMIT(\"ServerBusy\", HttpURLConnection.HTTP_UNAVAILABLE, \"Ingress is over the account limit.\"),\\n    EGRESS_OVER_ACCOUNT_LIMIT(\"ServerBusy\", HttpURLConnection.HTTP_UNAVAILABLE, \"Egress is over the account limit.\"),\\n    TPS_OVER_ACCOUNT_LIMIT(\"ServerBusy\", HttpURLConnection.HTTP_UNAVAILABLE, \"Operations per second is over the account limit.\"),\\n    OTHER_SERVER_THROTTLING(\"ServerBusy\", HttpURLConnection.HTTP_UNAVAILABLE, \"The server is currently unable to receive requests. Please retry your request.\"),\\n    INVALID_QUERY_PARAMETER_VALUE(\"InvalidQueryParameterValue\", HttpURLConnection.HTTP_BAD_REQUEST, null),\\n    INVALID_RENAME_DESTINATION(\"InvalidRenameDestinationPath\", HttpURLConnection.HTTP_BAD_REQUEST, null),\\n    AUTHORIZATION_PERMISSION_MISS_MATCH(\"AuthorizationPermissionMismatch\", HttpURLConnection.HTTP_FORBIDDEN, null),\\n    ACCOUNT_REQUIRES_HTTPS(\"AccountRequiresHttps\", HttpURLConnection.HTTP_BAD_REQUEST, null),\\n    MD5_MISMATCH(\"Md5Mismatch\", HttpURLConnection.HTTP_BAD_REQUEST, \"The MD5 value specified in the request did not match with the MD5 value calculated by the server.\"),\\n    COPY_BLOB_FAILED(\"CopyBlobFailed\", HttpURLConnection.HTTP_INTERNAL_ERROR, null),\\n    COPY_BLOB_ABORTED(\"CopyBlobAborted\", HttpURLConnection.HTTP_INTERNAL_ERROR, null),\\n    BLOB_OPERATION_NOT_SUPPORTED(\"BlobOperationNotSupported\", HttpURLConnection.HTTP_CONFLICT, null),\\n    INVALID_APPEND_OPERATION(\"InvalidAppendOperation\", HttpURLConnection.HTTP_CONFLICT, null),\\n    UNAUTHORIZED_BLOB_OVERWRITE(\"UnauthorizedBlobOverwrite\", HttpURLConnection.HTTP_FORBIDDEN, \"This request is not authorized to perform blob overwrites.\"),\\n    UNKNOWN(null, -1, null);\\n\\n    private final String errorCode;\\n\\n    private final int httpStatusCode;\\n\\n    private final String errorMessage;\\n\\n    private static final Logger LOG1 = LoggerFactory.getLogger(AzureServiceErrorCode.class);\\n\\n    AzureServiceErrorCode(String errorCode, int httpStatusCodes, String errorMessage) {\\n        this.errorCode = errorCode;\\n        this.httpStatusCode = httpStatusCodes;\\n        this.errorMessage = errorMessage;\\n    }\\n\\n    public int getStatusCode() {\\n        return this.httpStatusCode;\\n    }\\n\\n    public String getErrorCode() {\\n        return this.errorCode;\\n    }\\n\\n    public String getErrorMessage() {\\n        return this.errorMessage;\\n    }\\n\\n    public static List<AzureServiceErrorCode> getAzureServiceCode(int httpStatusCode) {\\n        List<AzureServiceErrorCode> errorCodes = new ArrayList<>();\\n        if (httpStatusCode == UNKNOWN.httpStatusCode) {\\n            errorCodes.add(UNKNOWN);\\n            return errorCodes;\\n        }\\n        for (AzureServiceErrorCode azureServiceErrorCode : AzureServiceErrorCode.values()) {\\n            if (azureServiceErrorCode.httpStatusCode == httpStatusCode) {\\n                errorCodes.add(azureServiceErrorCode);\\n            }\\n        }\\n        return errorCodes;\\n    }\\n\\n    public static AzureServiceErrorCode getAzureServiceCode(int httpStatusCode, String errorCode) {\\n        if (errorCode == null || errorCode.isEmpty() || httpStatusCode == UNKNOWN.httpStatusCode) {\\n            return UNKNOWN;\\n        }\\n        for (AzureServiceErrorCode azureServiceErrorCode : AzureServiceErrorCode.values()) {\\n            if (errorCode.equalsIgnoreCase(azureServiceErrorCode.errorCode) && azureServiceErrorCode.httpStatusCode == httpStatusCode) {\\n                return azureServiceErrorCode;\\n            }\\n        }\\n        return UNKNOWN;\\n    }\\n\\n    public static AzureServiceErrorCode getAzureServiceCode(int httpStatusCode, String errorCode, final String errorMessage) {\\n        if (errorCode == null || errorCode.isEmpty() || httpStatusCode == UNKNOWN.httpStatusCode || errorMessage == null || errorMessage.isEmpty()) {\\n            return UNKNOWN;\\n        }\\n        String[] errorMessages = errorMessage.split(System.lineSeparator(), 2);\\n        for (AzureServiceErrorCode azureServiceErrorCode : AzureServiceErrorCode.values()) {\\n            if (azureServiceErrorCode.getStatusCode() == httpStatusCode && azureServiceErrorCode.getErrorCode().equalsIgnoreCase(errorCode) && azureServiceErrorCode.getErrorMessage().equalsIgnoreCase(errorMessages[0])) {\\n                return azureServiceErrorCode;\\n            }\\n        }\\n        return UNKNOWN;\\n    }\\n}\\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java (Before)\\nimport org.apache.hadoop.fs.FileSystem;\\nimport org.apache.hadoop.fs.Path;\\nimport org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\\nimport org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore;\\nimport org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\\nimport static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_RETAIN_UNCOMMITTED_DATA;\\nimport static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.RENAME_DESTINATION_PARENT_PATH_NOT_FOUND;\\nimport static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.SOURCE_PATH_NOT_FOUND;\\npublic class AbfsDfsClient extends AbfsClient {\\n/**\\n * Get Rest Operation for API\\n * <a href=\"https://learn.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\">\\n *   Path - Create</a>.\\n * @param source path to source file\\n * @param destination destination of rename.\\n * @param continuation continuation.\\n * @param tracingContext for tracing the server calls.\\n * @param sourceEtag etag of source file. may be null or empty\\n * @param isMetadataIncompleteState was there a rename failure due to incomplete metadata state\\n * @return executed rest operation containing response from server.\\n * @throws IOException if rest operation fails.\\n */\\n@Override\\npublic AbfsClientRenameResult renamePath(final String source, final String destination, final String continuation, final TracingContext tracingContext, String sourceEtag, boolean isMetadataIncompleteState) throws IOException {\\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\\n    final boolean hasEtag = !isEmpty(sourceEtag);\\n    boolean shouldAttemptRecovery = isRenameResilience() && getIsNamespaceEnabled();\\n    if (!hasEtag && shouldAttemptRecovery) {\\n        // in case eTag is already not supplied to the API\\n        // and rename resilience is expected and it is an HNS enabled account\\n        // fetch the source etag to be used later in recovery\\n        try {\\n            final AbfsRestOperation srcStatusOp = getPathStatus(source, false, tracingContext, null);\\n            if (srcStatusOp.hasResult()) {\\n                final AbfsHttpOperation result = srcStatusOp.getResult();\\n                sourceEtag = extractEtagHeader(result);\\n                // and update the directory status.\\n                boolean isDir = checkIsDir(result);\\n                shouldAttemptRecovery = !isDir;\\n                LOG.debug(\"Retrieved etag of source for rename recovery: {}; isDir={}\", sourceEtag, isDir);\\n            }\\n        } catch (AbfsRestOperationException e) {\\n            throw new AbfsRestOperationException(e.getStatusCode(), SOURCE_PATH_NOT_FOUND.getErrorCode(), e.getMessage(), e);\\n        }\\n    }\\n    String encodedRenameSource = urlEncode(FORWARD_SLASH + this.getFileSystem() + source);\\n    if (getAuthType() == AuthType.SAS) {\\n        final AbfsUriQueryBuilder srcQueryBuilder = new AbfsUriQueryBuilder();\\n        appendSASTokenToQuery(source, SASTokenProvider.RENAME_SOURCE_OPERATION, srcQueryBuilder);\\n        encodedRenameSource += srcQueryBuilder.toString();\\n    }\\n    LOG.trace(\"Rename source queryparam added {}\", encodedRenameSource);\\n    requestHeaders.add(new AbfsHttpHeader(X_MS_RENAME_SOURCE, encodedRenameSource));\\n    requestHeaders.add(new AbfsHttpHeader(IF_NONE_MATCH, STAR));\\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_CONTINUATION, continuation);\\n    appendSASTokenToQuery(destination, SASTokenProvider.RENAME_DESTINATION_OPERATION, abfsUriQueryBuilder);\\n    final URL url = createRequestUrl(destination, abfsUriQueryBuilder.toString());\\n    final AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\\n    try {\\n        incrementAbfsRenamePath();\\n        op.execute(tracingContext);\\n        // AbfsClientResult contains the AbfsOperation, If recovery happened or\\n        // not, and the incompleteMetaDataState is true or false.\\n        // If we successfully rename a path and isMetadataIncompleteState was\\n        // true, then rename was recovered, else it didn\\'t, this is why\\n        // isMetadataIncompleteState is used for renameRecovery(as the 2nd param).\\n        return new AbfsClientRenameResult(op, isMetadataIncompleteState, isMetadataIncompleteState);\\n    } catch (AzureBlobFileSystemException e) {\\n        // If we have no HTTP response, throw the original exception.\\n        if (!op.hasResult()) {\\n            throw e;\\n        }\\n        // ref: HADOOP-18242. Rename failure occurring due to a rare case of\\n        // tracking metadata being in incomplete state.\\n        if (op.getResult().getStorageErrorCode().equals(RENAME_DESTINATION_PARENT_PATH_NOT_FOUND.getErrorCode()) && !isMetadataIncompleteState) {\\n            //Logging\\n            ABFS_METADATA_INCOMPLETE_RENAME_FAILURE.info(\"Rename Failure attempting to resolve tracking metadata state and retrying.\");\\n            // rename recovery should be attempted in this case also\\n            shouldAttemptRecovery = true;\\n            isMetadataIncompleteState = true;\\n            String sourceEtagAfterFailure = sourceEtag;\\n            if (isEmpty(sourceEtagAfterFailure)) {\\n                // Doing a HEAD call resolves the incomplete metadata state and\\n                // then we can retry the rename operation.\\n                AbfsRestOperation sourceStatusOp = getPathStatus(source, false, tracingContext, null);\\n                isMetadataIncompleteState = true;\\n                // Extract the sourceEtag, using the status Op, and set it\\n                // for future rename recovery.\\n                AbfsHttpOperation sourceStatusResult = sourceStatusOp.getResult();\\n                sourceEtagAfterFailure = extractEtagHeader(sourceStatusResult);\\n            }\\n            renamePath(source, destination, continuation, tracingContext, sourceEtagAfterFailure, isMetadataIncompleteState);\\n        }\\n        // if we get out of the condition without a successful rename, then\\n        // it isn\\'t metadata incomplete state issue.\\n        isMetadataIncompleteState = false;\\n        // setting default rename recovery success to false\\n        boolean etagCheckSucceeded = false;\\n        if (shouldAttemptRecovery) {\\n            etagCheckSucceeded = renameIdempotencyCheckOp(source, sourceEtag, op, destination, tracingContext);\\n        }\\n        if (!etagCheckSucceeded) {\\n            // idempotency did not return different result\\n            // throw back the exception\\n            throw e;\\n        }\\n        return new AbfsClientRenameResult(op, true, isMetadataIncompleteState);\\n    }\\n}\\n}\\n\\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java (After)\\nimport org.apache.hadoop.fs.FileSystem;\\nimport org.apache.hadoop.fs.Path;\\nimport org.apache.hadoop.fs.FileAlreadyExistsException;\\nimport org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\\nimport org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore;\\nimport org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\\nimport static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_RETAIN_UNCOMMITTED_DATA;\\nimport static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.RENAME_DESTINATION_PARENT_PATH_NOT_FOUND;\\nimport static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.SOURCE_PATH_NOT_FOUND;\\nimport static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.UNAUTHORIZED_BLOB_OVERWRITE;\\nimport static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_FILE_ALREADY_EXISTS;\\npublic class AbfsDfsClient extends AbfsClient {\\n/**\\n * Get Rest Operation for API\\n * <a href=\"https://learn.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\">\\n *   Path - Create</a>.\\n * @param source path to source file\\n * @param destination destination of rename.\\n * @param continuation continuation.\\n * @param tracingContext for tracing the server calls.\\n * @param sourceEtag etag of source file. may be null or empty\\n * @param isMetadataIncompleteState was there a rename failure due to incomplete metadata state\\n * @return executed rest operation containing response from server.\\n * @throws IOException if rest operation fails.\\n */\\n@Override\\npublic AbfsClientRenameResult renamePath(final String source, final String destination, final String continuation, final TracingContext tracingContext, String sourceEtag, boolean isMetadataIncompleteState) throws IOException {\\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\\n    final boolean hasEtag = !isEmpty(sourceEtag);\\n    boolean shouldAttemptRecovery = isRenameResilience() && getIsNamespaceEnabled();\\n    if (!hasEtag && shouldAttemptRecovery) {\\n        // in case eTag is already not supplied to the API\\n        // and rename resilience is expected and it is an HNS enabled account\\n        // fetch the source etag to be used later in recovery\\n        try {\\n            final AbfsRestOperation srcStatusOp = getPathStatus(source, false, tracingContext, null);\\n            if (srcStatusOp.hasResult()) {\\n                final AbfsHttpOperation result = srcStatusOp.getResult();\\n                sourceEtag = extractEtagHeader(result);\\n                // and update the directory status.\\n                boolean isDir = checkIsDir(result);\\n                shouldAttemptRecovery = !isDir;\\n                LOG.debug(\"Retrieved etag of source for rename recovery: {}; isDir={}\", sourceEtag, isDir);\\n            }\\n        } catch (AbfsRestOperationException e) {\\n            throw new AbfsRestOperationException(e.getStatusCode(), SOURCE_PATH_NOT_FOUND.getErrorCode(), e.getMessage(), e);\\n        }\\n    }\\n    String encodedRenameSource = urlEncode(FORWARD_SLASH + this.getFileSystem() + source);\\n    if (getAuthType() == AuthType.SAS) {\\n        final AbfsUriQueryBuilder srcQueryBuilder = new AbfsUriQueryBuilder();\\n        appendSASTokenToQuery(source, SASTokenProvider.RENAME_SOURCE_OPERATION, srcQueryBuilder);\\n        encodedRenameSource += srcQueryBuilder.toString();\\n    }\\n    LOG.trace(\"Rename source queryparam added {}\", encodedRenameSource);\\n    requestHeaders.add(new AbfsHttpHeader(X_MS_RENAME_SOURCE, encodedRenameSource));\\n    requestHeaders.add(new AbfsHttpHeader(IF_NONE_MATCH, STAR));\\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_CONTINUATION, continuation);\\n    appendSASTokenToQuery(destination, SASTokenProvider.RENAME_DESTINATION_OPERATION, abfsUriQueryBuilder);\\n    final URL url = createRequestUrl(destination, abfsUriQueryBuilder.toString());\\n    final AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\\n    try {\\n        incrementAbfsRenamePath();\\n        op.execute(tracingContext);\\n        // AbfsClientResult contains the AbfsOperation, If recovery happened or\\n        // not, and the incompleteMetaDataState is true or false.\\n        // If we successfully rename a path and isMetadataIncompleteState was\\n        // true, then rename was recovered, else it didn\\'t, this is why\\n        // isMetadataIncompleteState is used for renameRecovery(as the 2nd param).\\n        return new AbfsClientRenameResult(op, isMetadataIncompleteState, isMetadataIncompleteState);\\n    } catch (AzureBlobFileSystemException e) {\\n        // If we have no HTTP response, throw the original exception.\\n        if (!op.hasResult()) {\\n            throw e;\\n        }\\n        // ref: HADOOP-19393. Write permission checks can occur before validating\\n        // rename operation\\'s validity. If there is an existing destination path, it may be rejected\\n        // with an authorization error. Catching and throwing FileAlreadyExistsException instead.\\n        if (op.getResult().getStorageErrorCode().equals(UNAUTHORIZED_BLOB_OVERWRITE.getErrorCode())) {\\n            throw new FileAlreadyExistsException(ERR_FILE_ALREADY_EXISTS);\\n        }\\n        // ref: HADOOP-18242. Rename failure occurring due to a rare case of\\n        // tracking metadata being in incomplete state.\\n        if (op.getResult().getStorageErrorCode().equals(RENAME_DESTINATION_PARENT_PATH_NOT_FOUND.getErrorCode()) && !isMetadataIncompleteState) {\\n            //Logging\\n            ABFS_METADATA_INCOMPLETE_RENAME_FAILURE.info(\"Rename Failure attempting to resolve tracking metadata state and retrying.\");\\n            // rename recovery should be attempted in this case also\\n            shouldAttemptRecovery = true;\\n            isMetadataIncompleteState = true;\\n            String sourceEtagAfterFailure = sourceEtag;\\n            if (isEmpty(sourceEtagAfterFailure)) {\\n                // Doing a HEAD call resolves the incomplete metadata state and\\n                // then we can retry the rename operation.\\n                AbfsRestOperation sourceStatusOp = getPathStatus(source, false, tracingContext, null);\\n                isMetadataIncompleteState = true;\\n                // Extract the sourceEtag, using the status Op, and set it\\n                // for future rename recovery.\\n                AbfsHttpOperation sourceStatusResult = sourceStatusOp.getResult();\\n                sourceEtagAfterFailure = extractEtagHeader(sourceStatusResult);\\n            }\\n            renamePath(source, destination, continuation, tracingContext, sourceEtagAfterFailure, isMetadataIncompleteState);\\n        }\\n        // if we get out of the condition without a successful rename, then\\n        // it isn\\'t metadata incomplete state issue.\\n        isMetadataIncompleteState = false;\\n        // setting default rename recovery success to false\\n        boolean etagCheckSucceeded = false;\\n        if (shouldAttemptRecovery) {\\n            etagCheckSucceeded = renameIdempotencyCheckOp(source, sourceEtag, op, destination, tracingContext);\\n        }\\n        if (!etagCheckSucceeded) {\\n            // idempotency did not return different result\\n            // throw back the exception\\n            throw e;\\n        }\\n        return new AbfsClientRenameResult(op, true, isMetadataIncompleteState);\\n    }\\n}\\n}\\n\\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java (Before)\\npublic  final class AbfsErrors {\\npublic static final String ERR_WRITE_WITHOUT_LEASE = \"Attempted to write to file without lease\";\\npublic static final String ERR_LEASE_EXPIRED = \"A lease ID was specified, but the lease for the resource has expired.\";\\npublic static final String ERR_LEASE_EXPIRED_BLOB = \"A lease ID was specified, but the lease for the blob has expired.\";\\n}\\n\\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java (After)\\npublic  final class AbfsErrors {\\npublic static final String ERR_FILE_ALREADY_EXISTS = \"File already exists.\";\\npublic static final String ERR_WRITE_WITHOUT_LEASE = \"Attempted to write to file without lease\";\\npublic static final String ERR_LEASE_EXPIRED = \"A lease ID was specified, but the lease for the resource has expired.\";\\npublic static final String ERR_LEASE_EXPIRED_BLOB = \"A lease ID was specified, but the lease for the blob has expired.\";\\n}\\n\\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java (Before)\\nimport org.apache.hadoop.fs.FSDataInputStream;\\nimport org.apache.hadoop.fs.FSDataOutputStream;\\nimport org.apache.hadoop.fs.FileStatus;\\nimport org.apache.hadoop.fs.FileSystem;\\nimport org.apache.hadoop.fs.Path;\\nimport static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_SAS_TOKEN_PROVIDER_TYPE;\\nimport static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.AUTHORIZATION_PERMISSION_MISS_MATCH;\\nimport static org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers.aclEntry;\\nimport static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathDoesNotExist;\\nimport static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathExists;\\npublic class ITestAzureBlobFileSystemDelegationSAS extends AbstractAbfsIntegrationTest {\\n@Test\\npublic // Test input stream operation, read\\nvoid testReadAndWrite() throws Exception {\\n    final AzureBlobFileSystem fs = getFileSystem();\\n    Path reqPath = new Path(UUID.randomUUID().toString());\\n    final String msg1 = \"purple\";\\n    final String msg2 = \"yellow\";\\n    int expectedFileLength = msg1.length() * 2;\\n    byte[] readBuffer = new byte[1024];\\n    // create file with content \"purplepurple\"\\n    try (FSDataOutputStream stream = fs.create(reqPath)) {\\n        stream.writeBytes(msg1);\\n        stream.hflush();\\n        stream.writeBytes(msg1);\\n    }\\n    // open file and verify content is \"purplepurple\"\\n    try (FSDataInputStream stream = fs.open(reqPath)) {\\n        int bytesRead = stream.read(readBuffer, 0, readBuffer.length);\\n        assertEquals(expectedFileLength, bytesRead);\\n        String fileContent = new String(readBuffer, 0, bytesRead, StandardCharsets.UTF_8);\\n        assertEquals(msg1 + msg1, fileContent);\\n    }\\n    // overwrite file with content \"yellowyellow\"\\n    try (FSDataOutputStream stream = fs.create(reqPath)) {\\n        stream.writeBytes(msg2);\\n        stream.hflush();\\n        stream.writeBytes(msg2);\\n    }\\n    // open file and verify content is \"yellowyellow\"\\n    try (FSDataInputStream stream = fs.open(reqPath)) {\\n        int bytesRead = stream.read(readBuffer, 0, readBuffer.length);\\n        assertEquals(expectedFileLength, bytesRead);\\n        String fileContent = new String(readBuffer, 0, bytesRead, StandardCharsets.UTF_8);\\n        assertEquals(msg2 + msg2, fileContent);\\n    }\\n    // append to file so final content is \"yellowyellowpurplepurple\"\\n    try (FSDataOutputStream stream = fs.append(reqPath)) {\\n        stream.writeBytes(msg1);\\n        stream.hflush();\\n        stream.writeBytes(msg1);\\n    }\\n    // open file and verify content is \"yellowyellowpurplepurple\"\\n    try (FSDataInputStream stream = fs.open(reqPath)) {\\n        int bytesRead = stream.read(readBuffer, 0, readBuffer.length);\\n        assertEquals(2 * expectedFileLength, bytesRead);\\n        String fileContent = new String(readBuffer, 0, bytesRead, StandardCharsets.UTF_8);\\n        assertEquals(msg2 + msg2 + msg1 + msg1, fileContent);\\n    }\\n}\\n@Test\\npublic // Test rename file and rename folder\\nvoid testRename() throws Exception {\\n    final AzureBlobFileSystem fs = getFileSystem();\\n    Path sourceDir = new Path(UUID.randomUUID().toString());\\n    Path sourcePath = new Path(sourceDir, UUID.randomUUID().toString());\\n    Path destinationPath = new Path(sourceDir, UUID.randomUUID().toString());\\n    Path destinationDir = new Path(UUID.randomUUID().toString());\\n    // create file with content \"hello\"\\n    try (FSDataOutputStream stream = fs.create(sourcePath)) {\\n        stream.writeBytes(\"hello\");\\n    }\\n    assertPathDoesNotExist(fs, \"This path should not exist\", destinationPath);\\n    fs.rename(sourcePath, destinationPath);\\n    assertPathDoesNotExist(fs, \"This path should not exist\", sourcePath);\\n    assertPathExists(fs, \"This path should exist\", destinationPath);\\n    assertPathDoesNotExist(fs, \"This path should not exist\", destinationDir);\\n    fs.rename(sourceDir, destinationDir);\\n    assertPathDoesNotExist(fs, \"This path should not exist\", sourceDir);\\n    assertPathExists(fs, \"This path should exist\", destinationDir);\\n}\\n}\\n\\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java (After)\\nimport org.apache.hadoop.fs.FSDataInputStream;\\nimport org.apache.hadoop.fs.FSDataOutputStream;\\nimport org.apache.hadoop.fs.FileAlreadyExistsException;\\nimport org.apache.hadoop.fs.FileStatus;\\nimport org.apache.hadoop.fs.FileSystem;\\nimport org.apache.hadoop.fs.Path;\\nimport static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_SAS_TOKEN_PROVIDER_TYPE;\\nimport static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.AUTHORIZATION_PERMISSION_MISS_MATCH;\\nimport static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_FILE_ALREADY_EXISTS;\\nimport static org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers.aclEntry;\\nimport static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathDoesNotExist;\\nimport static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathExists;\\npublic class ITestAzureBlobFileSystemDelegationSAS extends AbstractAbfsIntegrationTest {\\n@Test\\npublic // Test input stream operation, read\\nvoid testReadAndWrite() throws Exception {\\n    final AzureBlobFileSystem fs = getFileSystem();\\n    Path reqPath = new Path(UUID.randomUUID().toString());\\n    final String msg1 = \"purple\";\\n    final String msg2 = \"yellow\";\\n    int expectedFileLength = msg1.length() * 2;\\n    byte[] readBuffer = new byte[1024];\\n    // create file with content \"purplepurple\"\\n    try (FSDataOutputStream stream = fs.create(reqPath)) {\\n        stream.writeBytes(msg1);\\n        stream.hflush();\\n        stream.writeBytes(msg1);\\n    }\\n    // open file and verify content is \"purplepurple\"\\n    try (FSDataInputStream stream = fs.open(reqPath)) {\\n        int bytesRead = stream.read(readBuffer, 0, readBuffer.length);\\n        assertEquals(expectedFileLength, bytesRead);\\n        String fileContent = new String(readBuffer, 0, bytesRead, StandardCharsets.UTF_8);\\n        assertEquals(msg1 + msg1, fileContent);\\n    }\\n    // overwrite file with content \"yellowyellow\"\\n    try (FSDataOutputStream stream = fs.create(reqPath)) {\\n        stream.writeBytes(msg2);\\n        stream.hflush();\\n        stream.writeBytes(msg2);\\n    }\\n    // open file and verify content is \"yellowyellow\"\\n    try (FSDataInputStream stream = fs.open(reqPath)) {\\n        int bytesRead = stream.read(readBuffer, 0, readBuffer.length);\\n        assertEquals(expectedFileLength, bytesRead);\\n        String fileContent = new String(readBuffer, 0, bytesRead, StandardCharsets.UTF_8);\\n        assertEquals(msg2 + msg2, fileContent);\\n    }\\n    // append to file so final content is \"yellowyellowpurplepurple\"\\n    try (FSDataOutputStream stream = fs.append(reqPath)) {\\n        stream.writeBytes(msg1);\\n        stream.hflush();\\n        stream.writeBytes(msg1);\\n    }\\n    // open file and verify content is \"yellowyellowpurplepurple\"\\n    try (FSDataInputStream stream = fs.open(reqPath)) {\\n        int bytesRead = stream.read(readBuffer, 0, readBuffer.length);\\n        assertEquals(2 * expectedFileLength, bytesRead);\\n        String fileContent = new String(readBuffer, 0, bytesRead, StandardCharsets.UTF_8);\\n        assertEquals(msg2 + msg2 + msg1 + msg1, fileContent);\\n    }\\n}\\n@Test\\npublic void checkExceptionForRenameOverwrites() throws Exception {\\n    final AzureBlobFileSystem fs = getFileSystem();\\n    Path src = new Path(\"a/b/f1.txt\");\\n    Path dest = new Path(\"a/b/f2.txt\");\\n    touch(src);\\n    touch(dest);\\n    intercept(FileAlreadyExistsException.class, ERR_FILE_ALREADY_EXISTS, () -> fs.rename(src, dest));\\n}\\n@Test\\npublic // Test rename file and rename folder\\nvoid testRename() throws Exception {\\n    final AzureBlobFileSystem fs = getFileSystem();\\n    Path sourceDir = new Path(UUID.randomUUID().toString());\\n    Path sourcePath = new Path(sourceDir, UUID.randomUUID().toString());\\n    Path destinationPath = new Path(sourceDir, UUID.randomUUID().toString());\\n    Path destinationDir = new Path(UUID.randomUUID().toString());\\n    // create file with content \"hello\"\\n    try (FSDataOutputStream stream = fs.create(sourcePath)) {\\n        stream.writeBytes(\"hello\");\\n    }\\n    assertPathDoesNotExist(fs, \"This path should not exist\", destinationPath);\\n    fs.rename(sourcePath, destinationPath);\\n    assertPathDoesNotExist(fs, \"This path should not exist\", sourcePath);\\n    assertPathExists(fs, \"This path should exist\", destinationPath);\\n    assertPathDoesNotExist(fs, \"This path should not exist\", destinationDir);\\n    fs.rename(sourceDir, destinationDir);\\n    assertPathDoesNotExist(fs, \"This path should not exist\", sourceDir);\\n    assertPathExists(fs, \"This path should exist\", destinationDir);\\n}\\n}\\n\\n\\nSummary:', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Given a Git diff and the relevant source code, write a concise summary of the code changes in a way that a non-technical person can understand. Summarize in maximum three concise sentences. \\n\\nAvoid adding any additional comments or annotations to the summary.\\n\\nGit diff:\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\\nindex 42d91d536b..470d052d89 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\\n@@ -1039,7 +1039,8 @@ public static List<String> mergeUniqElems(List<String> src, List<String> dest) {\\n     return src;\\n   }\\n \\n-  private static final String tmpPrefix = \"_tmp.\";\\n+  private static final String hadoopTmpPrefix = \"_tmp.\";\\n+  private static final String tmpPrefix = \"-tmp.\";\\n   private static final String taskTmpPrefix = \"_task_tmp.\";\\n \\n   public static Path toTaskTempPath(Path orig) {\\n@@ -1070,7 +1071,7 @@ private static boolean isTempPath(FileStatus file) {\\n     String name = file.getPath().getName();\\n     // in addition to detecting hive temporary files, we also check hadoop\\n     // temporary folders that used to show up in older releases\\n-    return (name.startsWith(\"_task\") || name.startsWith(tmpPrefix));\\n+    return (name.startsWith(\"_task\") || name.startsWith(tmpPrefix) || name.startsWith(hadoopTmpPrefix));\\n   }\\n \\n   /**\\n@@ -1393,7 +1394,7 @@ private static String replaceTaskIdFromFilename(String filename, String oldTaskI\\n   }\\n \\n \\n-  private static boolean shouldAvoidRename(FileSinkDesc conf, Configuration hConf) {\\n+  public static boolean shouldAvoidRename(FileSinkDesc conf, Configuration hConf) {\\n     // we are avoiding rename/move only if following conditions are met\\n     //  * execution engine is tez\\n     //  * if it is select query\\n@@ -3524,6 +3525,9 @@ public static List<Path> getInputPaths(JobConf job, MapWork work, Path hiveScrat\\n     Set<Path> pathsProcessed = new HashSet<Path>();\\n     List<Path> pathsToAdd = new LinkedList<Path>();\\n     DriverState driverState = DriverState.getDriverState();\\n+    if (work.isUseInputPathsDirectly() && work.getInputPaths() != null) {\\n+      return work.getInputPaths();\\n+    }\\n     // AliasToWork contains all the aliases\\n     Collection<String> aliasToWork = work.getAliasToWork().keySet();\\n     if (!skipDummy) {\\n@@ -4555,7 +4559,7 @@ private static Path getManifestDir(Path specPath, long writeId, int stmtId, Stri\\n     if (isDelete) {\\n       deltaDir = AcidUtils.deleteDeltaSubdir(writeId, writeId, stmtId);\\n     }\\n-    Path manifestPath = new Path(manifestRoot, \"_tmp.\" + deltaDir);\\n+    Path manifestPath = new Path(manifestRoot, Utilities.toTempPath(deltaDir));\\n \\n     if (isInsertOverwrite) {\\n       // When doing a multi-statement insert overwrite query with dynamic partitioning, the\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java\\nindex 594289eda4..53a4f3a843 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java\\n@@ -46,7 +46,6 @@\\n public class MergeFileWork extends MapWork {\\n \\n   private static final Logger LOG = LoggerFactory.getLogger(MergeFileWork.class);\\n-  private List<Path> inputPaths;\\n   private Path outputDir;\\n   private boolean hasDynamicPartitions;\\n   private boolean isListBucketingAlterTableConcatenate;\\n@@ -84,14 +83,6 @@ public MergeFileWork(List<Path> inputPaths, Path outputDir,\\n     this.isListBucketingAlterTableConcatenate = false;\\n   }\\n \\n-  public List<Path> getInputPaths() {\\n-    return inputPaths;\\n-  }\\n-\\n-  public void setInputPaths(List<Path> inputPaths) {\\n-    this.inputPaths = inputPaths;\\n-  }\\n-\\n   public Path getOutputDir() {\\n     return outputDir;\\n   }\\n@@ -137,8 +128,6 @@ public void resolveDynamicPartitionStoredAsSubDirsMerge(HiveConf conf,\\n         aliases, partDesc);\\n     // set internal input format for all partition descriptors\\n     partDesc.setInputFileFormatClass(internalInputFormat);\\n-    // Add the DP path to the list of input paths\\n-    inputPaths.add(path);\\n   }\\n \\n   /**\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java\\nindex da20eceba3..0e6816ae40 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java\\n@@ -20,18 +20,27 @@\\n \\n import java.io.IOException;\\n import java.io.Serializable;\\n+import java.nio.charset.Charset;\\n import java.util.ArrayList;\\n+import java.util.HashMap;\\n import java.util.HashSet;\\n import java.util.LinkedHashMap;\\n import java.util.List;\\n+import java.util.LongSummaryStatistics;\\n import java.util.Map;\\n+import java.util.stream.Collectors;\\n \\n+import com.google.common.collect.Lists;\\n+import org.apache.commons.io.IOUtils;\\n+import org.apache.hadoop.fs.FSDataInputStream;\\n import org.apache.hadoop.fs.FileStatus;\\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n import org.apache.hadoop.hive.common.HiveStatsUtils;\\n import org.apache.hadoop.hive.conf.HiveConf;\\n import org.apache.hadoop.hive.metastore.Warehouse;\\n+import org.apache.hadoop.hive.metastore.utils.FileUtils;\\n+import org.apache.hadoop.hive.ql.exec.Operator;\\n import org.apache.hadoop.hive.ql.exec.Task;\\n import org.apache.hadoop.hive.ql.exec.Utilities;\\n import org.slf4j.Logger;\\n@@ -151,6 +160,11 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n         }\\n \\n         int lbLevel = (ctx.getLbCtx() == null) ? 0 : ctx.getLbCtx().calculateListBucketingLevel();\\n+        boolean manifestFilePresent = false;\\n+        FileSystem manifestFs = dirPath.getFileSystem(conf);\\n+        if (manifestFs.exists(new Path(dirPath, Utilities.BLOB_MANIFEST_FILE))) {\\n+          manifestFilePresent = true;\\n+        }\\n \\n         /**\\n          * In order to make code easier to read, we write the following in the way:\\n@@ -168,15 +182,25 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n           int dpLbLevel = numDPCols + lbLevel;\\n \\n           generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask,\\n-              mrAndMvTask, dirPath, inpFs, ctx, work, dpLbLevel);\\n+              mrAndMvTask, dirPath, inpFs, ctx, work, dpLbLevel, manifestFilePresent);\\n         } else { // no dynamic partitions\\n           if(lbLevel == 0) {\\n             // static partition without list bucketing\\n-            long totalSz = getMergeSize(inpFs, dirPath, avgConditionSize);\\n-            Utilities.FILE_OP_LOGGER.debug(\"merge resolve simple case - totalSz \" + totalSz + \" from \" + dirPath);\\n+            List<FileStatus> manifestFilePaths = new ArrayList<>();\\n+            long totalSize;\\n+            if (manifestFilePresent) {\\n+              manifestFilePaths = getManifestFilePaths(conf, dirPath);\\n+              totalSize = getMergeSize(manifestFilePaths, avgConditionSize);\\n+            } else {\\n+              totalSize = getMergeSize(inpFs, dirPath, avgConditionSize);\\n+              Utilities.FILE_OP_LOGGER.debug(\"merge resolve simple case - totalSize \" + totalSize + \" from \" + dirPath);\\n+            }\\n \\n-            if (totalSz >= 0) { // add the merge job\\n-              setupMapRedWork(conf, work, trgtSize, totalSz);\\n+            if (totalSize >= 0) { // add the merge job\\n+              if (manifestFilePresent) {\\n+                setupWorkWhenUsingManifestFile(work, manifestFilePaths, dirPath, true);\\n+              }\\n+              setupMapRedWork(conf, work, trgtSize, totalSize);\\n               resTsks.add(mrTask);\\n             } else { // don\\'t need to merge, add the move job\\n               resTsks.add(mvTask);\\n@@ -184,7 +208,7 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n           } else {\\n             // static partition and list bucketing\\n             generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask,\\n-                mrAndMvTask, dirPath, inpFs, ctx, work, lbLevel);\\n+                mrAndMvTask, dirPath, inpFs, ctx, work, lbLevel, manifestFilePresent);\\n           }\\n         }\\n       } else {\\n@@ -229,11 +253,22 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n   private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n       long trgtSize, long avgConditionSize, Task<?> mvTask,\\n       Task<?> mrTask, Task<?> mrAndMvTask, Path dirPath,\\n-      FileSystem inpFs, ConditionalResolverMergeFilesCtx ctx, MapWork work, int dpLbLevel)\\n+      FileSystem inpFs, ConditionalResolverMergeFilesCtx ctx, MapWork work, int dpLbLevel,\\n+      boolean manifestFilePresent)\\n       throws IOException {\\n     DynamicPartitionCtx dpCtx = ctx.getDPCtx();\\n-    // get list of dynamic partitions\\n-    List<FileStatus> statusList = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);\\n+    List<FileStatus> statusList;\\n+    Map<FileStatus, List<FileStatus>> manifestDirToFile = new HashMap<>();\\n+    if (manifestFilePresent) {\\n+      // Get the list of files from manifest file.\\n+      List<FileStatus> fileStatuses = getManifestFilePaths(conf, dirPath);\\n+      // Setup the work to include all the files present in the manifest.\\n+      setupWorkWhenUsingManifestFile(work, fileStatuses, dirPath, false);\\n+      manifestDirToFile = getManifestDirs(inpFs, fileStatuses);\\n+      statusList = new ArrayList<>(manifestDirToFile.keySet());\\n+    } else {\\n+      statusList = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);\\n+    }\\n     FileStatus[] status = statusList.toArray(new FileStatus[statusList.size()]);\\n \\n     // cleanup pathToPartitionInfo\\n@@ -253,15 +288,21 @@ private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n     work.removePathToAlias(path); // the root path is not useful anymore\\n \\n     // populate pathToPartitionInfo and pathToAliases w/ DP paths\\n-    long totalSz = 0;\\n+    long totalSize = 0;\\n     boolean doMerge = false;\\n     // list of paths that don\\'t need to merge but need to move to the dest location\\n-    List<Path> toMove = new ArrayList<Path>();\\n+    List<Path> toMove = new ArrayList<>();\\n+    List<Path> toMerge = new ArrayList<>();\\n     for (int i = 0; i < status.length; ++i) {\\n-      long len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);\\n+      long len;\\n+      if (manifestFilePresent) {\\n+        len = getMergeSize(manifestDirToFile.get(status[i]), avgConditionSize);\\n+      } else {\\n+        len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);\\n+      }\\n       if (len >= 0) {\\n         doMerge = true;\\n-        totalSz += len;\\n+        totalSize += len;\\n         PartitionDesc pDesc = (dpCtx != null) ? generateDPFullPartSpec(dpCtx, status, tblDesc, i)\\n             : partDesc;\\n         if (pDesc == null) {\\n@@ -271,6 +312,13 @@ private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n         Utilities.FILE_OP_LOGGER.debug(\"merge resolver will merge \" + status[i].getPath());\\n         work.resolveDynamicPartitionStoredAsSubDirsMerge(conf, status[i].getPath(), tblDesc,\\n             aliases, pDesc);\\n+        // Do not add input file since its already added when the manifest file is present.\\n+        if (manifestFilePresent) {\\n+          toMerge.addAll(manifestDirToFile.get(status[i])\\n+              .stream().map(FileStatus::getPath).collect(Collectors.toList()));\\n+        } else {\\n+          toMerge.add(status[i].getPath());\\n+        }\\n       } else {\\n         Utilities.FILE_OP_LOGGER.debug(\"merge resolver will move \" + status[i].getPath());\\n \\n@@ -278,8 +326,13 @@ private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n       }\\n     }\\n     if (doMerge) {\\n+      // Set paths appropriately.\\n+      if (work.getInputPaths() != null && !work.getInputPaths().isEmpty()) {\\n+        toMerge.addAll(work.getInputPaths());\\n+      }\\n+      work.setInputPaths(toMerge);\\n       // add the merge MR job\\n-      setupMapRedWork(conf, work, trgtSize, totalSz);\\n+      setupMapRedWork(conf, work, trgtSize, totalSize);\\n \\n       // add the move task for those partitions that do not need merging\\n       if (toMove.size() > 0) {\\n@@ -359,11 +412,11 @@ private void setupMapRedWork(HiveConf conf, MapWork mWork, long targetSize, long\\n     mWork.setIsMergeFromResolver(true);\\n   }\\n \\n-  private static class AverageSize {\\n+  private static class FileSummary {\\n     private final long totalSize;\\n-    private final int numFiles;\\n+    private final long numFiles;\\n \\n-    public AverageSize(long totalSize, int numFiles) {\\n+    public FileSummary(long totalSize, long numFiles) {\\n       this.totalSize = totalSize;\\n       this.numFiles  = numFiles;\\n     }\\n@@ -372,64 +425,106 @@ public long getTotalSize() {\\n       return totalSize;\\n     }\\n \\n-    public int getNumFiles() {\\n+    public long getNumFiles() {\\n       return numFiles;\\n     }\\n   }\\n \\n-  private AverageSize getAverageSize(FileSystem inpFs, Path dirPath) {\\n-    AverageSize error = new AverageSize(-1, -1);\\n-    try {\\n-      FileStatus[] fStats = inpFs.listStatus(dirPath);\\n-\\n-      long totalSz = 0;\\n-      int numFiles = 0;\\n-      for (FileStatus fStat : fStats) {\\n-        Utilities.FILE_OP_LOGGER.debug(\"Resolver looking at \" + fStat.getPath());\\n-        if (fStat.isDir()) {\\n-          AverageSize avgSzDir = getAverageSize(inpFs, fStat.getPath());\\n-          if (avgSzDir.getTotalSize() < 0) {\\n-            return error;\\n-          }\\n-          totalSz += avgSzDir.getTotalSize();\\n-          numFiles += avgSzDir.getNumFiles();\\n-        }\\n-        else {\\n-          totalSz += fStat.getLen();\\n-          numFiles++;\\n-        }\\n-      }\\n+  private FileSummary getFileSummary(List<FileStatus> fileStatusList) {\\n+    LongSummaryStatistics stats = fileStatusList.stream().filter(FileStatus::isFile)\\n+        .mapToLong(FileStatus::getLen).summaryStatistics();\\n+    return new FileSummary(stats.getSum(), stats.getCount());\\n+  }\\n \\n-      return new AverageSize(totalSz, numFiles);\\n-    } catch (IOException e) {\\n-      return error;\\n+  private List<FileStatus> getManifestFilePaths(HiveConf conf, Path dirPath) throws IOException {\\n+    FileSystem manifestFs = dirPath.getFileSystem(conf);\\n+    List<String> filesKept;\\n+    List<FileStatus> pathsKept = new ArrayList<>();\\n+    try (FSDataInputStream inStream = manifestFs.open(new Path(dirPath, Utilities.BLOB_MANIFEST_FILE))) {\\n+      String paths = IOUtils.toString(inStream, Charset.defaultCharset());\\n+      filesKept = Lists.newArrayList(paths.split(System.lineSeparator()));\\n     }\\n+    // The first string contains the directory information. Not useful.\\n+    filesKept.remove(0);\\n+\\n+    for (String file : filesKept) {\\n+      pathsKept.add(manifestFs.getFileStatus(new Path(file)));\\n+    }\\n+    return pathsKept;\\n+  }\\n+\\n+  private long getMergeSize(FileSystem inpFs, Path dirPath, long avgSize) {\\n+    List<FileStatus> result = FileUtils.getFileStatusRecurse(dirPath, inpFs);\\n+    return getMergeSize(result, avgSize);\\n   }\\n \\n   /**\\n    * Whether to merge files inside directory given the threshold of the average file size.\\n    *\\n-   * @param inpFs input file system.\\n-   * @param dirPath input file directory.\\n+   * @param fileStatuses a list of FileStatus instances.\\n    * @param avgSize threshold of average file size.\\n    * @return -1 if not need to merge (either because of there is only 1 file or the\\n    * average size is larger than avgSize). Otherwise the size of the total size of files.\\n    * If return value is 0 that means there are multiple files each of which is an empty file.\\n    * This could be true when the table is bucketized and all buckets are empty.\\n    */\\n-  private long getMergeSize(FileSystem inpFs, Path dirPath, long avgSize) {\\n-    AverageSize averageSize = getAverageSize(inpFs, dirPath);\\n-    if (averageSize.getTotalSize() < 0) {\\n+  private long getMergeSize(List<FileStatus> fileStatuses, long avgSize) {\\n+    FileSummary fileSummary = getFileSummary(fileStatuses);\\n+    if (fileSummary.getTotalSize() <= 0) {\\n       return -1;\\n     }\\n \\n-    if (averageSize.getNumFiles() <= 1) {\\n+    if (fileSummary.getNumFiles() <= 1) {\\n       return -1;\\n     }\\n \\n-    if (averageSize.getTotalSize()/averageSize.getNumFiles() < avgSize) {\\n-      return averageSize.getTotalSize();\\n+    if (fileSummary.getTotalSize() / fileSummary.getNumFiles() < avgSize) {\\n+      return fileSummary.getTotalSize();\\n     }\\n     return -1;\\n   }\\n+\\n+  private void setupWorkWhenUsingManifestFile(MapWork mapWork, List<FileStatus> fileStatuses, Path dirPath,\\n+                                              boolean isTblLevel) {\\n+    Map<String, Operator<? extends OperatorDesc>> aliasToWork = mapWork.getAliasToWork();\\n+    Map<Path, PartitionDesc> pathToPartitionInfo = mapWork.getPathToPartitionInfo();\\n+    Operator<? extends OperatorDesc> op = aliasToWork.get(dirPath.toString());\\n+    PartitionDesc partitionDesc = pathToPartitionInfo.get(dirPath);\\n+    Path tmpDirPath = Utilities.toTempPath(dirPath);\\n+    if (op != null) {\\n+      aliasToWork.remove(dirPath.toString());\\n+      aliasToWork.put(tmpDirPath.toString(), op);\\n+      mapWork.setAliasToWork(aliasToWork);\\n+    }\\n+    if (partitionDesc != null) {\\n+      pathToPartitionInfo.remove(dirPath);\\n+      pathToPartitionInfo.put(tmpDirPath, partitionDesc);\\n+      mapWork.setPathToPartitionInfo(pathToPartitionInfo);\\n+    }\\n+    mapWork.removePathToAlias(dirPath);\\n+    mapWork.addPathToAlias(tmpDirPath, tmpDirPath.toString());\\n+    if (isTblLevel) {\\n+      List<Path> inputPaths = fileStatuses.stream()\\n+          .filter(FileStatus::isFile)\\n+          .map(FileStatus::getPath).collect(Collectors.toList());\\n+      mapWork.setInputPaths(inputPaths);\\n+    }\\n+    mapWork.setUseInputPathsDirectly(true);\\n+  }\\n+\\n+  private Map<FileStatus, List<FileStatus>> getManifestDirs(FileSystem inpFs, List<FileStatus> fileStatuses)\\n+      throws IOException {\\n+    Map<FileStatus, List<FileStatus>> manifestDirsToPaths = new HashMap<>();\\n+    for (FileStatus fileStatus : fileStatuses) {\\n+      if (!fileStatus.isDirectory()) {\\n+        FileStatus parentDir = inpFs.getFileStatus(fileStatus.getPath().getParent());\\n+        List<FileStatus> fileStatusList = Lists.newArrayList(fileStatus);\\n+        manifestDirsToPaths.merge(parentDir, fileStatusList, (oldValue, newValue) -> {\\n+          oldValue.addAll(newValue);\\n+          return oldValue;\\n+        });\\n+      }\\n+    }\\n+    return manifestDirsToPaths;\\n+  }\\n }\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\\nindex 17e105310c..076ef0a99b 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\\n@@ -180,6 +180,10 @@ public enum LlapIODescriptor {\\n \\n   private ProbeDecodeContext probeDecodeContext = null;\\n \\n+  protected List<Path> inputPaths;\\n+\\n+  private boolean useInputPathsDirectly;\\n+\\n   public MapWork() {}\\n \\n   public MapWork(String name) {\\n@@ -934,4 +938,20 @@ public MapExplainVectorization getMapExplainVectorization() {\\n     }\\n     return new MapExplainVectorization(this);\\n   }\\n+\\n+  public List<Path> getInputPaths() {\\n+    return inputPaths;\\n+  }\\n+\\n+  public void setInputPaths(List<Path> inputPaths) {\\n+    this.inputPaths = inputPaths;\\n+  }\\n+\\n+  public void setUseInputPathsDirectly(boolean useInputPathsDirectly) {\\n+    this.useInputPathsDirectly = useInputPathsDirectly;\\n+  }\\n+\\n+  public boolean isUseInputPathsDirectly() {\\n+    return useInputPathsDirectly;\\n+  }\\n }\\n\\n\\nSource code:\\nql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java (Before)\\npublic  final class Utilities {\\npublic static List<String> mergeUniqElems(List<String> src, List<String> dest) {\\n    if (dest == null) {\\n        return src;\\n    }\\n    if (src == null) {\\n        return dest;\\n    }\\n    int pos = 0;\\n    while (pos < dest.size()) {\\n        if (!src.contains(dest.get(pos))) {\\n            src.add(dest.get(pos));\\n        }\\n        pos++;\\n    }\\n    return src;\\n}\\nprivate static final String tmpPrefix = \"_tmp.\";\\nprivate static final String taskTmpPrefix = \"_task_tmp.\";\\npublic static Path toTaskTempPath(Path orig) {\\n    if (orig.getName().indexOf(taskTmpPrefix) == 0) {\\n        return orig;\\n    }\\n    return new Path(orig.getParent(), taskTmpPrefix + orig.getName());\\n}\\n/**\\n * Detect if the supplied file is a temporary path.\\n */\\nprivate static boolean isTempPath(FileStatus file) {\\n    String name = file.getPath().getName();\\n    // in addition to detecting hive temporary files, we also check hadoop\\n    // temporary folders that used to show up in older releases\\n    return (name.startsWith(\"_task\") || name.startsWith(tmpPrefix));\\n}\\n/**\\n * Replace the oldTaskId appearing in the filename by the newTaskId. The string oldTaskId could\\n * appear multiple times, we should only replace the last one.\\n *\\n * @param filename\\n * @param oldTaskId\\n * @param newTaskId\\n * @return\\n */\\nprivate static String replaceTaskIdFromFilename(String filename, String oldTaskId, String newTaskId) {\\n    String[] spl = filename.split(oldTaskId);\\n    if ((spl.length == 0) || (spl.length == 1)) {\\n        return filename.replaceAll(oldTaskId, newTaskId);\\n    }\\n    StringBuilder snew = new StringBuilder();\\n    for (int idx = 0; idx < spl.length - 1; idx++) {\\n        if (idx > 0) {\\n            snew.append(oldTaskId);\\n        }\\n        snew.append(spl[idx]);\\n    }\\n    snew.append(newTaskId);\\n    snew.append(spl[spl.length - 1]);\\n    return snew.toString();\\n}\\nprivate static boolean shouldAvoidRename(FileSinkDesc conf, Configuration hConf) {\\n    // we are avoiding rename/move only if following conditions are met\\n    //  * execution engine is tez\\n    //  * if it is select query\\n    if (conf != null && conf.getIsQuery() && conf.getFilesToFetch() != null && HiveConf.getVar(hConf, ConfVars.HIVE_EXECUTION_ENGINE).equalsIgnoreCase(\"tez\")) {\\n        return true;\\n    }\\n    return false;\\n}\\n/**\\n * Computes a list of all input paths needed to compute the given MapWork. All aliases\\n * are considered and a merged list of input paths is returned. If any input path points\\n * to an empty table or partition a dummy file in the scratch dir is instead created and\\n * added to the list. This is needed to avoid special casing the operator pipeline for\\n * these cases.\\n *\\n * @param job JobConf used to run the job\\n * @param work MapWork encapsulating the info about the task\\n * @param hiveScratchDir The tmp dir used to create dummy files if needed\\n * @param ctx Context object\\n * @return List of paths to process for the given MapWork\\n * @throws Exception\\n */\\npublic static List<Path> getInputPaths(JobConf job, MapWork work, Path hiveScratchDir, Context ctx, boolean skipDummy) throws Exception {\\n    PerfLogger perfLogger = SessionState.getPerfLogger();\\n    perfLogger.perfLogBegin(CLASS_NAME, PerfLogger.INPUT_PATHS);\\n    Set<Path> pathsProcessed = new HashSet<Path>();\\n    List<Path> pathsToAdd = new LinkedList<Path>();\\n    DriverState driverState = DriverState.getDriverState();\\n    // AliasToWork contains all the aliases\\n    Collection<String> aliasToWork = work.getAliasToWork().keySet();\\n    if (!skipDummy) {\\n        // ConcurrentModification otherwise if adding dummy.\\n        aliasToWork = new ArrayList<>(aliasToWork);\\n    }\\n    for (String alias : aliasToWork) {\\n        LOG.info(\"Processing alias {}\", alias);\\n        // The alias may not have any path\\n        Collection<Map.Entry<Path, List<String>>> pathToAliases = work.getPathToAliases().entrySet();\\n        if (!skipDummy) {\\n            // ConcurrentModification otherwise if adding dummy.\\n            pathToAliases = new ArrayList<>(pathToAliases);\\n        }\\n        boolean isEmptyTable = true;\\n        boolean hasLogged = false;\\n        for (Map.Entry<Path, List<String>> e : pathToAliases) {\\n            if (driverState != null && driverState.isAborted()) {\\n                throw new IOException(\"Operation is Canceled.\");\\n            }\\n            Path file = e.getKey();\\n            List<String> aliases = e.getValue();\\n            if (aliases.contains(alias)) {\\n                if (file != null) {\\n                    isEmptyTable = false;\\n                } else {\\n                    LOG.warn(\"Found a null path for alias {}\", alias);\\n                    continue;\\n                }\\n                // Multiple aliases can point to the same path - it should be\\n                // processed only once\\n                if (pathsProcessed.contains(file)) {\\n                    continue;\\n                }\\n                StringInternUtils.internUriStringsInPath(file);\\n                pathsProcessed.add(file);\\n                LOG.debug(\"Adding input file {}\", file);\\n                if (!hasLogged) {\\n                    hasLogged = true;\\n                    LOG.info(\"Adding {} inputs; the first input is {}\", work.getPathToAliases().size(), file);\\n                }\\n                pathsToAdd.add(file);\\n            }\\n        }\\n        // If the query references non-existent partitions\\n        // We need to add a empty file, it is not acceptable to change the\\n        // operator tree\\n        // Consider the query:\\n        // select * from (select count(1) from T union all select count(1) from\\n        // T2) x;\\n        // If T is empty and T2 contains 100 rows, the user expects: 0, 100 (2\\n        // rows)\\n        if (isEmptyTable && !skipDummy) {\\n            pathsToAdd.add(createDummyFileForEmptyTable(job, work, hiveScratchDir, alias));\\n        }\\n    }\\n    List<Path> finalPathsToAdd = new LinkedList<>();\\n    int numExecutors = getMaxExecutorsForInputListing(job, pathsToAdd.size());\\n    if (numExecutors > 1) {\\n        ExecutorService pool = Executors.newFixedThreadPool(numExecutors, new ThreadFactoryBuilder().setDaemon(true).setNameFormat(\"Get-Input-Paths-%d\").build());\\n        finalPathsToAdd.addAll(getInputPathsWithPool(job, work, hiveScratchDir, ctx, skipDummy, pathsToAdd, pool));\\n    } else {\\n        for (final Path path : pathsToAdd) {\\n            if (driverState != null && driverState.isAborted()) {\\n                throw new IOException(\"Operation is Canceled.\");\\n            }\\n            Path newPath = new GetInputPathsCallable(path, job, work, hiveScratchDir, ctx, skipDummy).call();\\n            updatePathForMapWork(newPath, work, path);\\n            finalPathsToAdd.add(newPath);\\n        }\\n    }\\n    perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.INPUT_PATHS);\\n    return finalPathsToAdd;\\n}\\nprivate static Path getManifestDir(Path specPath, long writeId, int stmtId, String unionSuffix, boolean isInsertOverwrite, String staticSpec, boolean isDelete) {\\n    Path manifestRoot = specPath;\\n    if (staticSpec != null) {\\n        String tableRoot = specPath.toString();\\n        tableRoot = tableRoot.substring(0, tableRoot.length() - staticSpec.length());\\n        manifestRoot = new Path(tableRoot);\\n    }\\n    String deltaDir = AcidUtils.baseOrDeltaSubdir(isInsertOverwrite, writeId, writeId, stmtId);\\n    if (isDelete) {\\n        deltaDir = AcidUtils.deleteDeltaSubdir(writeId, writeId, stmtId);\\n    }\\n    Path manifestPath = new Path(manifestRoot, \"_tmp.\" + deltaDir);\\n    if (isInsertOverwrite) {\\n        // When doing a multi-statement insert overwrite query with dynamic partitioning, the\\n        // generated manifest directory is the same for each FileSinkOperator.\\n        // To resolve this name collision, extending the manifest path with the statement id.\\n        manifestPath = new Path(manifestPath + \"_\" + stmtId);\\n    }\\n    return (unionSuffix == null) ? manifestPath : new Path(manifestPath, unionSuffix);\\n}\\n}\\n\\nql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java (After)\\npublic  final class Utilities {\\npublic static List<String> mergeUniqElems(List<String> src, List<String> dest) {\\n    if (dest == null) {\\n        return src;\\n    }\\n    if (src == null) {\\n        return dest;\\n    }\\n    int pos = 0;\\n    while (pos < dest.size()) {\\n        if (!src.contains(dest.get(pos))) {\\n            src.add(dest.get(pos));\\n        }\\n        pos++;\\n    }\\n    return src;\\n}\\nprivate static final String hadoopTmpPrefix = \"_tmp.\";\\nprivate static final String tmpPrefix = \"-tmp.\";\\nprivate static final String taskTmpPrefix = \"_task_tmp.\";\\npublic static Path toTaskTempPath(Path orig) {\\n    if (orig.getName().indexOf(taskTmpPrefix) == 0) {\\n        return orig;\\n    }\\n    return new Path(orig.getParent(), taskTmpPrefix + orig.getName());\\n}\\n/**\\n * Detect if the supplied file is a temporary path.\\n */\\nprivate static boolean isTempPath(FileStatus file) {\\n    String name = file.getPath().getName();\\n    // in addition to detecting hive temporary files, we also check hadoop\\n    // temporary folders that used to show up in older releases\\n    return (name.startsWith(\"_task\") || name.startsWith(tmpPrefix) || name.startsWith(hadoopTmpPrefix));\\n}\\n/**\\n * Replace the oldTaskId appearing in the filename by the newTaskId. The string oldTaskId could\\n * appear multiple times, we should only replace the last one.\\n *\\n * @param filename\\n * @param oldTaskId\\n * @param newTaskId\\n * @return\\n */\\nprivate static String replaceTaskIdFromFilename(String filename, String oldTaskId, String newTaskId) {\\n    String[] spl = filename.split(oldTaskId);\\n    if ((spl.length == 0) || (spl.length == 1)) {\\n        return filename.replaceAll(oldTaskId, newTaskId);\\n    }\\n    StringBuilder snew = new StringBuilder();\\n    for (int idx = 0; idx < spl.length - 1; idx++) {\\n        if (idx > 0) {\\n            snew.append(oldTaskId);\\n        }\\n        snew.append(spl[idx]);\\n    }\\n    snew.append(newTaskId);\\n    snew.append(spl[spl.length - 1]);\\n    return snew.toString();\\n}\\npublic static boolean shouldAvoidRename(FileSinkDesc conf, Configuration hConf) {\\n    // we are avoiding rename/move only if following conditions are met\\n    //  * execution engine is tez\\n    //  * if it is select query\\n    if (conf != null && conf.getIsQuery() && conf.getFilesToFetch() != null && HiveConf.getVar(hConf, ConfVars.HIVE_EXECUTION_ENGINE).equalsIgnoreCase(\"tez\")) {\\n        return true;\\n    }\\n    return false;\\n}\\n/**\\n * Computes a list of all input paths needed to compute the given MapWork. All aliases\\n * are considered and a merged list of input paths is returned. If any input path points\\n * to an empty table or partition a dummy file in the scratch dir is instead created and\\n * added to the list. This is needed to avoid special casing the operator pipeline for\\n * these cases.\\n *\\n * @param job JobConf used to run the job\\n * @param work MapWork encapsulating the info about the task\\n * @param hiveScratchDir The tmp dir used to create dummy files if needed\\n * @param ctx Context object\\n * @return List of paths to process for the given MapWork\\n * @throws Exception\\n */\\npublic static List<Path> getInputPaths(JobConf job, MapWork work, Path hiveScratchDir, Context ctx, boolean skipDummy) throws Exception {\\n    PerfLogger perfLogger = SessionState.getPerfLogger();\\n    perfLogger.perfLogBegin(CLASS_NAME, PerfLogger.INPUT_PATHS);\\n    Set<Path> pathsProcessed = new HashSet<Path>();\\n    List<Path> pathsToAdd = new LinkedList<Path>();\\n    DriverState driverState = DriverState.getDriverState();\\n    if (work.isUseInputPathsDirectly() && work.getInputPaths() != null) {\\n        return work.getInputPaths();\\n    }\\n    // AliasToWork contains all the aliases\\n    Collection<String> aliasToWork = work.getAliasToWork().keySet();\\n    if (!skipDummy) {\\n        // ConcurrentModification otherwise if adding dummy.\\n        aliasToWork = new ArrayList<>(aliasToWork);\\n    }\\n    for (String alias : aliasToWork) {\\n        LOG.info(\"Processing alias {}\", alias);\\n        // The alias may not have any path\\n        Collection<Map.Entry<Path, List<String>>> pathToAliases = work.getPathToAliases().entrySet();\\n        if (!skipDummy) {\\n            // ConcurrentModification otherwise if adding dummy.\\n            pathToAliases = new ArrayList<>(pathToAliases);\\n        }\\n        boolean isEmptyTable = true;\\n        boolean hasLogged = false;\\n        for (Map.Entry<Path, List<String>> e : pathToAliases) {\\n            if (driverState != null && driverState.isAborted()) {\\n                throw new IOException(\"Operation is Canceled.\");\\n            }\\n            Path file = e.getKey();\\n            List<String> aliases = e.getValue();\\n            if (aliases.contains(alias)) {\\n                if (file != null) {\\n                    isEmptyTable = false;\\n                } else {\\n                    LOG.warn(\"Found a null path for alias {}\", alias);\\n                    continue;\\n                }\\n                // Multiple aliases can point to the same path - it should be\\n                // processed only once\\n                if (pathsProcessed.contains(file)) {\\n                    continue;\\n                }\\n                StringInternUtils.internUriStringsInPath(file);\\n                pathsProcessed.add(file);\\n                LOG.debug(\"Adding input file {}\", file);\\n                if (!hasLogged) {\\n                    hasLogged = true;\\n                    LOG.info(\"Adding {} inputs; the first input is {}\", work.getPathToAliases().size(), file);\\n                }\\n                pathsToAdd.add(file);\\n            }\\n        }\\n        // If the query references non-existent partitions\\n        // We need to add a empty file, it is not acceptable to change the\\n        // operator tree\\n        // Consider the query:\\n        // select * from (select count(1) from T union all select count(1) from\\n        // T2) x;\\n        // If T is empty and T2 contains 100 rows, the user expects: 0, 100 (2\\n        // rows)\\n        if (isEmptyTable && !skipDummy) {\\n            pathsToAdd.add(createDummyFileForEmptyTable(job, work, hiveScratchDir, alias));\\n        }\\n    }\\n    List<Path> finalPathsToAdd = new LinkedList<>();\\n    int numExecutors = getMaxExecutorsForInputListing(job, pathsToAdd.size());\\n    if (numExecutors > 1) {\\n        ExecutorService pool = Executors.newFixedThreadPool(numExecutors, new ThreadFactoryBuilder().setDaemon(true).setNameFormat(\"Get-Input-Paths-%d\").build());\\n        finalPathsToAdd.addAll(getInputPathsWithPool(job, work, hiveScratchDir, ctx, skipDummy, pathsToAdd, pool));\\n    } else {\\n        for (final Path path : pathsToAdd) {\\n            if (driverState != null && driverState.isAborted()) {\\n                throw new IOException(\"Operation is Canceled.\");\\n            }\\n            Path newPath = new GetInputPathsCallable(path, job, work, hiveScratchDir, ctx, skipDummy).call();\\n            updatePathForMapWork(newPath, work, path);\\n            finalPathsToAdd.add(newPath);\\n        }\\n    }\\n    perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.INPUT_PATHS);\\n    return finalPathsToAdd;\\n}\\nprivate static Path getManifestDir(Path specPath, long writeId, int stmtId, String unionSuffix, boolean isInsertOverwrite, String staticSpec, boolean isDelete) {\\n    Path manifestRoot = specPath;\\n    if (staticSpec != null) {\\n        String tableRoot = specPath.toString();\\n        tableRoot = tableRoot.substring(0, tableRoot.length() - staticSpec.length());\\n        manifestRoot = new Path(tableRoot);\\n    }\\n    String deltaDir = AcidUtils.baseOrDeltaSubdir(isInsertOverwrite, writeId, writeId, stmtId);\\n    if (isDelete) {\\n        deltaDir = AcidUtils.deleteDeltaSubdir(writeId, writeId, stmtId);\\n    }\\n    Path manifestPath = new Path(manifestRoot, Utilities.toTempPath(deltaDir));\\n    if (isInsertOverwrite) {\\n        // When doing a multi-statement insert overwrite query with dynamic partitioning, the\\n        // generated manifest directory is the same for each FileSinkOperator.\\n        // To resolve this name collision, extending the manifest path with the statement id.\\n        manifestPath = new Path(manifestPath + \"_\" + stmtId);\\n    }\\n    return (unionSuffix == null) ? manifestPath : new Path(manifestPath, unionSuffix);\\n}\\n}\\n\\nql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java (Before)\\npublic class MergeFileWork extends MapWork {\\nprivate static final Logger LOG = LoggerFactory.getLogger(MergeFileWork.class);\\nprivate List<Path> inputPaths;\\nprivate Path outputDir;\\nprivate boolean hasDynamicPartitions;\\nprivate boolean isListBucketingAlterTableConcatenate;\\npublic MergeFileWork(List<Path> inputPaths, Path outputDir, boolean hasDynamicPartitions, String srcTblInputFormat, TableDesc tbl) {\\n    this.inputPaths = inputPaths;\\n    this.outputDir = outputDir;\\n    this.hasDynamicPartitions = hasDynamicPartitions;\\n    this.srcTblInputFormat = srcTblInputFormat;\\n    PartitionDesc partDesc = new PartitionDesc();\\n    if (srcTblInputFormat.equals(OrcInputFormat.class.getName())) {\\n        this.internalInputFormat = OrcFileStripeMergeInputFormat.class;\\n    } else if (srcTblInputFormat.equals(RCFileInputFormat.class.getName())) {\\n        this.internalInputFormat = RCFileBlockMergeInputFormat.class;\\n    }\\n    partDesc.setInputFileFormatClass(internalInputFormat);\\n    partDesc.setTableDesc(tbl);\\n    for (Path path : this.inputPaths) {\\n        this.addPathToPartitionInfo(path, partDesc);\\n    }\\n    this.isListBucketingAlterTableConcatenate = false;\\n}\\npublic List<Path> getInputPaths() {\\n    return inputPaths;\\n}\\npublic void setInputPaths(List<Path> inputPaths) {\\n    this.inputPaths = inputPaths;\\n}\\npublic Path getOutputDir() {\\n    return outputDir;\\n}\\n@Override\\npublic void resolveDynamicPartitionStoredAsSubDirsMerge(HiveConf conf, Path path, TableDesc tblDesc, List<String> aliases, PartitionDesc partDesc) {\\n    super.resolveDynamicPartitionStoredAsSubDirsMerge(conf, path, tblDesc, aliases, partDesc);\\n    // set internal input format for all partition descriptors\\n    partDesc.setInputFileFormatClass(internalInputFormat);\\n    // Add the DP path to the list of input paths\\n    inputPaths.add(path);\\n}\\n}\\n\\nql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java (After)\\npublic class MergeFileWork extends MapWork {\\nprivate static final Logger LOG = LoggerFactory.getLogger(MergeFileWork.class);\\nprivate Path outputDir;\\nprivate boolean hasDynamicPartitions;\\nprivate boolean isListBucketingAlterTableConcatenate;\\npublic MergeFileWork(List<Path> inputPaths, Path outputDir, boolean hasDynamicPartitions, String srcTblInputFormat, TableDesc tbl) {\\n    this.inputPaths = inputPaths;\\n    this.outputDir = outputDir;\\n    this.hasDynamicPartitions = hasDynamicPartitions;\\n    this.srcTblInputFormat = srcTblInputFormat;\\n    PartitionDesc partDesc = new PartitionDesc();\\n    if (srcTblInputFormat.equals(OrcInputFormat.class.getName())) {\\n        this.internalInputFormat = OrcFileStripeMergeInputFormat.class;\\n    } else if (srcTblInputFormat.equals(RCFileInputFormat.class.getName())) {\\n        this.internalInputFormat = RCFileBlockMergeInputFormat.class;\\n    }\\n    partDesc.setInputFileFormatClass(internalInputFormat);\\n    partDesc.setTableDesc(tbl);\\n    for (Path path : this.inputPaths) {\\n        this.addPathToPartitionInfo(path, partDesc);\\n    }\\n    this.isListBucketingAlterTableConcatenate = false;\\n}\\npublic Path getOutputDir() {\\n    return outputDir;\\n}\\n@Override\\npublic void resolveDynamicPartitionStoredAsSubDirsMerge(HiveConf conf, Path path, TableDesc tblDesc, List<String> aliases, PartitionDesc partDesc) {\\n    super.resolveDynamicPartitionStoredAsSubDirsMerge(conf, path, tblDesc, aliases, partDesc);\\n    // set internal input format for all partition descriptors\\n    partDesc.setInputFileFormatClass(internalInputFormat);\\n}\\n}\\n\\nql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java (Before)\\nimport java.io.IOException;\\nimport java.io.Serializable;\\nimport java.util.ArrayList;\\nimport java.util.HashSet;\\nimport java.util.LinkedHashMap;\\nimport java.util.List;\\nimport java.util.Map;\\nimport org.apache.hadoop.fs.FileStatus;\\nimport org.apache.hadoop.fs.FileSystem;\\nimport org.apache.hadoop.fs.Path;\\nimport org.apache.hadoop.hive.common.HiveStatsUtils;\\nimport org.apache.hadoop.hive.conf.HiveConf;\\nimport org.apache.hadoop.hive.metastore.Warehouse;\\nimport org.apache.hadoop.hive.ql.exec.Task;\\nimport org.apache.hadoop.hive.ql.exec.Utilities;\\nimport org.slf4j.Logger;\\npublic class ConditionalResolverMergeFiles implements ConditionalResolver, Serializable {\\npublic List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n    ConditionalResolverMergeFilesCtx ctx = (ConditionalResolverMergeFilesCtx) objCtx;\\n    String dirName = ctx.getDir();\\n    List<Task<?>> resTsks = new ArrayList<Task<?>>();\\n    // check if a map-reduce job is needed to merge the files\\n    // If the current size is smaller than the target, merge\\n    long trgtSize = conf.getLongVar(HiveConf.ConfVars.HIVEMERGEMAPFILESSIZE);\\n    long avgConditionSize = conf.getLongVar(HiveConf.ConfVars.HIVEMERGEMAPFILESAVGSIZE);\\n    trgtSize = Math.max(trgtSize, avgConditionSize);\\n    Task<?> mvTask = ctx.getListTasks().get(0);\\n    Task<?> mrTask = ctx.getListTasks().get(1);\\n    Task<?> mrAndMvTask = ctx.getListTasks().get(2);\\n    try {\\n        Path dirPath = new Path(dirName);\\n        FileSystem inpFs = dirPath.getFileSystem(conf);\\n        DynamicPartitionCtx dpCtx = ctx.getDPCtx();\\n        if (inpFs.exists(dirPath)) {\\n            // For each dynamic partition, check if it needs to be merged.\\n            MapWork work;\\n            if (mrTask.getWork() instanceof MapredWork) {\\n                work = ((MapredWork) mrTask.getWork()).getMapWork();\\n            } else if (mrTask.getWork() instanceof TezWork) {\\n                work = (MapWork) ((TezWork) mrTask.getWork()).getAllWork().get(0);\\n            } else {\\n                work = (MapWork) mrTask.getWork();\\n            }\\n            int lbLevel = (ctx.getLbCtx() == null) ? 0 : ctx.getLbCtx().calculateListBucketingLevel();\\n            /**\\n             * In order to make code easier to read, we write the following in the way:\\n             * 1. the first if clause to differ dynamic partition and static partition\\n             * 2. with static partition, we differ list bucketing from non-list bucketing.\\n             * Another way to write it is to merge static partition w/ LB wit DP. In that way,\\n             * we still need to further differ them, since one uses lbLevel and\\n             * another lbLevel+numDPCols.\\n             * The first one is selected mainly for easy to read.\\n             */\\n            // Dynamic partition: replace input path (root to dp paths) with dynamic partition\\n            // input paths.\\n            if (dpCtx != null && dpCtx.getNumDPCols() > 0) {\\n                int numDPCols = dpCtx.getNumDPCols();\\n                int dpLbLevel = numDPCols + lbLevel;\\n                generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask, mrAndMvTask, dirPath, inpFs, ctx, work, dpLbLevel);\\n            } else {\\n                // no dynamic partitions\\n                if (lbLevel == 0) {\\n                    // static partition without list bucketing\\n                    long totalSz = getMergeSize(inpFs, dirPath, avgConditionSize);\\n                    Utilities.FILE_OP_LOGGER.debug(\"merge resolve simple case - totalSz \" + totalSz + \" from \" + dirPath);\\n                    if (totalSz >= 0) {\\n                        // add the merge job\\n                        setupMapRedWork(conf, work, trgtSize, totalSz);\\n                        resTsks.add(mrTask);\\n                    } else {\\n                        // don\\'t need to merge, add the move job\\n                        resTsks.add(mvTask);\\n                    }\\n                } else {\\n                    // static partition and list bucketing\\n                    generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask, mrAndMvTask, dirPath, inpFs, ctx, work, lbLevel);\\n                }\\n            }\\n        } else {\\n            Utilities.FILE_OP_LOGGER.info(\"Resolver returning movetask for \" + dirPath);\\n            resTsks.add(mvTask);\\n        }\\n    } catch (IOException e) {\\n        LOG.warn(\"Exception while getting tasks\", e);\\n    }\\n    // Only one of the tasks should ever be added to resTsks\\n    assert (resTsks.size() == 1);\\n    return resTsks;\\n}\\n/**\\n * This method generates actual task for conditional tasks. It could be\\n * 1. move task only\\n * 2. merge task only\\n * 3. merge task followed by a move task.\\n * It used to be true for dynamic partition only since static partition doesn\\'t have #3.\\n * It changes w/ list bucketing. Static partition has #3 since it has sub-directories.\\n * For example, if a static partition is defined as skewed and stored-as-directories,\\n * instead of all files in one directory, it will create a sub-dir per skewed value plus\\n * default directory. So #3 is required for static partition.\\n * So, we move it to a method so that it can be used by both SP and DP.\\n * @param conf\\n * @param resTsks\\n * @param trgtSize\\n * @param avgConditionSize\\n * @param mvTask\\n * @param mrTask\\n * @param mrAndMvTask\\n * @param dirPath\\n * @param inpFs\\n * @param ctx\\n * @param work\\n * @param dpLbLevel\\n * @throws IOException\\n */\\nprivate void generateActualTasks(HiveConf conf, List<Task<?>> resTsks, long trgtSize, long avgConditionSize, Task<?> mvTask, Task<?> mrTask, Task<?> mrAndMvTask, Path dirPath, FileSystem inpFs, ConditionalResolverMergeFilesCtx ctx, MapWork work, int dpLbLevel) throws IOException {\\n    DynamicPartitionCtx dpCtx = ctx.getDPCtx();\\n    // get list of dynamic partitions\\n    List<FileStatus> statusList = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);\\n    FileStatus[] status = statusList.toArray(new FileStatus[statusList.size()]);\\n    // cleanup pathToPartitionInfo\\n    Map<Path, PartitionDesc> ptpi = work.getPathToPartitionInfo();\\n    assert ptpi.size() == 1;\\n    Path path = ptpi.keySet().iterator().next();\\n    PartitionDesc partDesc = ptpi.get(path);\\n    TableDesc tblDesc = partDesc.getTableDesc();\\n    Utilities.FILE_OP_LOGGER.debug(\"merge resolver removing \" + path);\\n    // the root path is not useful anymore\\n    work.removePathToPartitionInfo(path);\\n    // cleanup pathToAliases\\n    Map<Path, List<String>> pta = work.getPathToAliases();\\n    assert pta.size() == 1;\\n    path = pta.keySet().iterator().next();\\n    List<String> aliases = pta.get(path);\\n    // the root path is not useful anymore\\n    work.removePathToAlias(path);\\n    // populate pathToPartitionInfo and pathToAliases w/ DP paths\\n    long totalSz = 0;\\n    boolean doMerge = false;\\n    // list of paths that don\\'t need to merge but need to move to the dest location\\n    List<Path> toMove = new ArrayList<Path>();\\n    for (int i = 0; i < status.length; ++i) {\\n        long len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);\\n        if (len >= 0) {\\n            doMerge = true;\\n            totalSz += len;\\n            PartitionDesc pDesc = (dpCtx != null) ? generateDPFullPartSpec(dpCtx, status, tblDesc, i) : partDesc;\\n            if (pDesc == null) {\\n                Utilities.FILE_OP_LOGGER.warn(\"merger ignoring invalid DP path \" + status[i].getPath());\\n                continue;\\n            }\\n            Utilities.FILE_OP_LOGGER.debug(\"merge resolver will merge \" + status[i].getPath());\\n            work.resolveDynamicPartitionStoredAsSubDirsMerge(conf, status[i].getPath(), tblDesc, aliases, pDesc);\\n        } else {\\n            Utilities.FILE_OP_LOGGER.debug(\"merge resolver will move \" + status[i].getPath());\\n            toMove.add(status[i].getPath());\\n        }\\n    }\\n    if (doMerge) {\\n        // add the merge MR job\\n        setupMapRedWork(conf, work, trgtSize, totalSz);\\n        // add the move task for those partitions that do not need merging\\n        if (toMove.size() > 0) {\\n            // Note: this path should be specific to concatenate; never executed in a select query.\\n            // modify the existing move task as it is already in the candidate running tasks\\n            // running the MoveTask and MR task in parallel may\\n            // cause the mvTask write to /ds=1 and MR task write\\n            // to /ds=1_1 for the same partition.\\n            // make the MoveTask as the child of the MR Task\\n            resTsks.add(mrAndMvTask);\\n            // Originally the mvTask and the child move task of the mrAndMvTask contain the same\\n            // MoveWork object.\\n            // If the blobstore optimizations are on and the input/output paths are merged\\n            // in the move only MoveWork, the mvTask and the child move task of the mrAndMvTask\\n            // will contain different MoveWork objects, which causes problems.\\n            // Not just in this case, but also in general the child move task of the mrAndMvTask should\\n            // be used, because that is the correct move task for the \"merge and move\" use case.\\n            Task<?> mergeAndMoveMoveTask = mrAndMvTask.getChildTasks().get(0);\\n            MoveWork mvWork = (MoveWork) mergeAndMoveMoveTask.getWork();\\n            LoadFileDesc lfd = mvWork.getLoadFileWork();\\n            Path targetDir = lfd.getTargetDir();\\n            List<Path> targetDirs = new ArrayList<Path>(toMove.size());\\n            for (int i = 0; i < toMove.size(); i++) {\\n                // Here directly the path name is used, instead of the uri because the uri contains the\\n                // serialized version of the path. For dynamic partition, we need the non serialized\\n                // version of the path as this value is used directly as partition name to create the partition.\\n                // For example, if the dp name is \"part=2022-01-16 04:35:56.732\" then the uri\\n                // will contain \"part=2022-01-162022-01-16%2004%253A35%253A56.732\". When we convert it to\\n                // partition name, it will come as \"part=2022-01-16 04%3A35%3A56.732\". But the path will have\\n                // value \"part=2022-01-16 04%3A35%3A56.732\", which will get converted to proper name by\\n                // function escapePathName.\\n                String[] moveStrSplits = toMove.get(i).toString().split(Path.SEPARATOR);\\n                int dpIndex = moveStrSplits.length - dpLbLevel;\\n                Path target = targetDir;\\n                while (dpIndex < moveStrSplits.length) {\\n                    target = new Path(target, moveStrSplits[dpIndex]);\\n                    dpIndex++;\\n                }\\n                targetDirs.add(target);\\n            }\\n            LoadMultiFilesDesc lmfd = new LoadMultiFilesDesc(toMove, targetDirs, lfd.getIsDfsDir(), lfd.getColumns(), lfd.getColumnTypes());\\n            mvWork.setLoadFileWork(null);\\n            mvWork.setLoadTableWork(null);\\n            mvWork.setMultiFilesDesc(lmfd);\\n        } else {\\n            resTsks.add(mrTask);\\n        }\\n    } else {\\n        // add the move task\\n        resTsks.add(mvTask);\\n    }\\n}\\nprivate void setupMapRedWork(HiveConf conf, MapWork mWork, long targetSize, long totalSize) {\\n    mWork.setMaxSplitSize(targetSize);\\n    mWork.setMinSplitSize(targetSize);\\n    mWork.setMinSplitSizePerNode(targetSize);\\n    mWork.setMinSplitSizePerRack(targetSize);\\n    mWork.setIsMergeFromResolver(true);\\n}\\nprivate static class AverageSize {\\n\\n    private final long totalSize;\\n\\n    private final int numFiles;\\n\\n    public AverageSize(long totalSize, int numFiles) {\\n        this.totalSize = totalSize;\\n        this.numFiles = numFiles;\\n    }\\n\\n    public long getTotalSize() {\\n        return totalSize;\\n    }\\n\\n    public int getNumFiles() {\\n        return numFiles;\\n    }\\n}\\nprivate AverageSize getAverageSize(FileSystem inpFs, Path dirPath) {\\n    AverageSize error = new AverageSize(-1, -1);\\n    try {\\n        FileStatus[] fStats = inpFs.listStatus(dirPath);\\n        long totalSz = 0;\\n        int numFiles = 0;\\n        for (FileStatus fStat : fStats) {\\n            Utilities.FILE_OP_LOGGER.debug(\"Resolver looking at \" + fStat.getPath());\\n            if (fStat.isDir()) {\\n                AverageSize avgSzDir = getAverageSize(inpFs, fStat.getPath());\\n                if (avgSzDir.getTotalSize() < 0) {\\n                    return error;\\n                }\\n                totalSz += avgSzDir.getTotalSize();\\n                numFiles += avgSzDir.getNumFiles();\\n            } else {\\n                totalSz += fStat.getLen();\\n                numFiles++;\\n            }\\n        }\\n        return new AverageSize(totalSz, numFiles);\\n    } catch (IOException e) {\\n        return error;\\n    }\\n}\\n/**\\n * Whether to merge files inside directory given the threshold of the average file size.\\n *\\n * @param inpFs input file system.\\n * @param dirPath input file directory.\\n * @param avgSize threshold of average file size.\\n * @return -1 if not need to merge (either because of there is only 1 file or the\\n * average size is larger than avgSize). Otherwise the size of the total size of files.\\n * If return value is 0 that means there are multiple files each of which is an empty file.\\n * This could be true when the table is bucketized and all buckets are empty.\\n */\\nprivate long getMergeSize(FileSystem inpFs, Path dirPath, long avgSize) {\\n    AverageSize averageSize = getAverageSize(inpFs, dirPath);\\n    if (averageSize.getTotalSize() < 0) {\\n        return -1;\\n    }\\n    if (averageSize.getNumFiles() <= 1) {\\n        return -1;\\n    }\\n    if (averageSize.getTotalSize() / averageSize.getNumFiles() < avgSize) {\\n        return averageSize.getTotalSize();\\n    }\\n    return -1;\\n}\\n}\\n\\nql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java (After)\\nimport java.io.IOException;\\nimport java.io.Serializable;\\nimport java.nio.charset.Charset;\\nimport java.util.ArrayList;\\nimport java.util.HashMap;\\nimport java.util.HashSet;\\nimport java.util.LinkedHashMap;\\nimport java.util.List;\\nimport java.util.LongSummaryStatistics;\\nimport java.util.Map;\\nimport java.util.stream.Collectors;\\nimport com.google.common.collect.Lists;\\nimport org.apache.commons.io.IOUtils;\\nimport org.apache.hadoop.fs.FSDataInputStream;\\nimport org.apache.hadoop.fs.FileStatus;\\nimport org.apache.hadoop.fs.FileSystem;\\nimport org.apache.hadoop.fs.Path;\\nimport org.apache.hadoop.hive.common.HiveStatsUtils;\\nimport org.apache.hadoop.hive.conf.HiveConf;\\nimport org.apache.hadoop.hive.metastore.Warehouse;\\nimport org.apache.hadoop.hive.metastore.utils.FileUtils;\\nimport org.apache.hadoop.hive.ql.exec.Operator;\\nimport org.apache.hadoop.hive.ql.exec.Task;\\nimport org.apache.hadoop.hive.ql.exec.Utilities;\\nimport org.slf4j.Logger;\\npublic class ConditionalResolverMergeFiles implements ConditionalResolver, Serializable {\\npublic List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n    ConditionalResolverMergeFilesCtx ctx = (ConditionalResolverMergeFilesCtx) objCtx;\\n    String dirName = ctx.getDir();\\n    List<Task<?>> resTsks = new ArrayList<Task<?>>();\\n    // check if a map-reduce job is needed to merge the files\\n    // If the current size is smaller than the target, merge\\n    long trgtSize = conf.getLongVar(HiveConf.ConfVars.HIVEMERGEMAPFILESSIZE);\\n    long avgConditionSize = conf.getLongVar(HiveConf.ConfVars.HIVEMERGEMAPFILESAVGSIZE);\\n    trgtSize = Math.max(trgtSize, avgConditionSize);\\n    Task<?> mvTask = ctx.getListTasks().get(0);\\n    Task<?> mrTask = ctx.getListTasks().get(1);\\n    Task<?> mrAndMvTask = ctx.getListTasks().get(2);\\n    try {\\n        Path dirPath = new Path(dirName);\\n        FileSystem inpFs = dirPath.getFileSystem(conf);\\n        DynamicPartitionCtx dpCtx = ctx.getDPCtx();\\n        if (inpFs.exists(dirPath)) {\\n            // For each dynamic partition, check if it needs to be merged.\\n            MapWork work;\\n            if (mrTask.getWork() instanceof MapredWork) {\\n                work = ((MapredWork) mrTask.getWork()).getMapWork();\\n            } else if (mrTask.getWork() instanceof TezWork) {\\n                work = (MapWork) ((TezWork) mrTask.getWork()).getAllWork().get(0);\\n            } else {\\n                work = (MapWork) mrTask.getWork();\\n            }\\n            int lbLevel = (ctx.getLbCtx() == null) ? 0 : ctx.getLbCtx().calculateListBucketingLevel();\\n            boolean manifestFilePresent = false;\\n            FileSystem manifestFs = dirPath.getFileSystem(conf);\\n            if (manifestFs.exists(new Path(dirPath, Utilities.BLOB_MANIFEST_FILE))) {\\n                manifestFilePresent = true;\\n            }\\n            /**\\n             * In order to make code easier to read, we write the following in the way:\\n             * 1. the first if clause to differ dynamic partition and static partition\\n             * 2. with static partition, we differ list bucketing from non-list bucketing.\\n             * Another way to write it is to merge static partition w/ LB wit DP. In that way,\\n             * we still need to further differ them, since one uses lbLevel and\\n             * another lbLevel+numDPCols.\\n             * The first one is selected mainly for easy to read.\\n             */\\n            // Dynamic partition: replace input path (root to dp paths) with dynamic partition\\n            // input paths.\\n            if (dpCtx != null && dpCtx.getNumDPCols() > 0) {\\n                int numDPCols = dpCtx.getNumDPCols();\\n                int dpLbLevel = numDPCols + lbLevel;\\n                generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask, mrAndMvTask, dirPath, inpFs, ctx, work, dpLbLevel, manifestFilePresent);\\n            } else {\\n                // no dynamic partitions\\n                if (lbLevel == 0) {\\n                    // static partition without list bucketing\\n                    List<FileStatus> manifestFilePaths = new ArrayList<>();\\n                    long totalSize;\\n                    if (manifestFilePresent) {\\n                        manifestFilePaths = getManifestFilePaths(conf, dirPath);\\n                        totalSize = getMergeSize(manifestFilePaths, avgConditionSize);\\n                    } else {\\n                        totalSize = getMergeSize(inpFs, dirPath, avgConditionSize);\\n                        Utilities.FILE_OP_LOGGER.debug(\"merge resolve simple case - totalSize \" + totalSize + \" from \" + dirPath);\\n                    }\\n                    if (totalSize >= 0) {\\n                        // add the merge job\\n                        if (manifestFilePresent) {\\n                            setupWorkWhenUsingManifestFile(work, manifestFilePaths, dirPath, true);\\n                        }\\n                        setupMapRedWork(conf, work, trgtSize, totalSize);\\n                        resTsks.add(mrTask);\\n                    } else {\\n                        // don\\'t need to merge, add the move job\\n                        resTsks.add(mvTask);\\n                    }\\n                } else {\\n                    // static partition and list bucketing\\n                    generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask, mrAndMvTask, dirPath, inpFs, ctx, work, lbLevel, manifestFilePresent);\\n                }\\n            }\\n        } else {\\n            Utilities.FILE_OP_LOGGER.info(\"Resolver returning movetask for \" + dirPath);\\n            resTsks.add(mvTask);\\n        }\\n    } catch (IOException e) {\\n        LOG.warn(\"Exception while getting tasks\", e);\\n    }\\n    // Only one of the tasks should ever be added to resTsks\\n    assert (resTsks.size() == 1);\\n    return resTsks;\\n}\\n/**\\n * This method generates actual task for conditional tasks. It could be\\n * 1. move task only\\n * 2. merge task only\\n * 3. merge task followed by a move task.\\n * It used to be true for dynamic partition only since static partition doesn\\'t have #3.\\n * It changes w/ list bucketing. Static partition has #3 since it has sub-directories.\\n * For example, if a static partition is defined as skewed and stored-as-directories,\\n * instead of all files in one directory, it will create a sub-dir per skewed value plus\\n * default directory. So #3 is required for static partition.\\n * So, we move it to a method so that it can be used by both SP and DP.\\n * @param conf\\n * @param resTsks\\n * @param trgtSize\\n * @param avgConditionSize\\n * @param mvTask\\n * @param mrTask\\n * @param mrAndMvTask\\n * @param dirPath\\n * @param inpFs\\n * @param ctx\\n * @param work\\n * @param dpLbLevel\\n * @throws IOException\\n */\\nprivate void generateActualTasks(HiveConf conf, List<Task<?>> resTsks, long trgtSize, long avgConditionSize, Task<?> mvTask, Task<?> mrTask, Task<?> mrAndMvTask, Path dirPath, FileSystem inpFs, ConditionalResolverMergeFilesCtx ctx, MapWork work, int dpLbLevel, boolean manifestFilePresent) throws IOException {\\n    DynamicPartitionCtx dpCtx = ctx.getDPCtx();\\n    List<FileStatus> statusList;\\n    Map<FileStatus, List<FileStatus>> manifestDirToFile = new HashMap<>();\\n    if (manifestFilePresent) {\\n        // Get the list of files from manifest file.\\n        List<FileStatus> fileStatuses = getManifestFilePaths(conf, dirPath);\\n        // Setup the work to include all the files present in the manifest.\\n        setupWorkWhenUsingManifestFile(work, fileStatuses, dirPath, false);\\n        manifestDirToFile = getManifestDirs(inpFs, fileStatuses);\\n        statusList = new ArrayList<>(manifestDirToFile.keySet());\\n    } else {\\n        statusList = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);\\n    }\\n    FileStatus[] status = statusList.toArray(new FileStatus[statusList.size()]);\\n    // cleanup pathToPartitionInfo\\n    Map<Path, PartitionDesc> ptpi = work.getPathToPartitionInfo();\\n    assert ptpi.size() == 1;\\n    Path path = ptpi.keySet().iterator().next();\\n    PartitionDesc partDesc = ptpi.get(path);\\n    TableDesc tblDesc = partDesc.getTableDesc();\\n    Utilities.FILE_OP_LOGGER.debug(\"merge resolver removing \" + path);\\n    // the root path is not useful anymore\\n    work.removePathToPartitionInfo(path);\\n    // cleanup pathToAliases\\n    Map<Path, List<String>> pta = work.getPathToAliases();\\n    assert pta.size() == 1;\\n    path = pta.keySet().iterator().next();\\n    List<String> aliases = pta.get(path);\\n    // the root path is not useful anymore\\n    work.removePathToAlias(path);\\n    // populate pathToPartitionInfo and pathToAliases w/ DP paths\\n    long totalSize = 0;\\n    boolean doMerge = false;\\n    // list of paths that don\\'t need to merge but need to move to the dest location\\n    List<Path> toMove = new ArrayList<>();\\n    List<Path> toMerge = new ArrayList<>();\\n    for (int i = 0; i < status.length; ++i) {\\n        long len;\\n        if (manifestFilePresent) {\\n            len = getMergeSize(manifestDirToFile.get(status[i]), avgConditionSize);\\n        } else {\\n            len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);\\n        }\\n        if (len >= 0) {\\n            doMerge = true;\\n            totalSize += len;\\n            PartitionDesc pDesc = (dpCtx != null) ? generateDPFullPartSpec(dpCtx, status, tblDesc, i) : partDesc;\\n            if (pDesc == null) {\\n                Utilities.FILE_OP_LOGGER.warn(\"merger ignoring invalid DP path \" + status[i].getPath());\\n                continue;\\n            }\\n            Utilities.FILE_OP_LOGGER.debug(\"merge resolver will merge \" + status[i].getPath());\\n            work.resolveDynamicPartitionStoredAsSubDirsMerge(conf, status[i].getPath(), tblDesc, aliases, pDesc);\\n            // Do not add input file since its already added when the manifest file is present.\\n            if (manifestFilePresent) {\\n                toMerge.addAll(manifestDirToFile.get(status[i]).stream().map(FileStatus::getPath).collect(Collectors.toList()));\\n            } else {\\n                toMerge.add(status[i].getPath());\\n            }\\n        } else {\\n            Utilities.FILE_OP_LOGGER.debug(\"merge resolver will move \" + status[i].getPath());\\n            toMove.add(status[i].getPath());\\n        }\\n    }\\n    if (doMerge) {\\n        // Set paths appropriately.\\n        if (work.getInputPaths() != null && !work.getInputPaths().isEmpty()) {\\n            toMerge.addAll(work.getInputPaths());\\n        }\\n        work.setInputPaths(toMerge);\\n        // add the merge MR job\\n        setupMapRedWork(conf, work, trgtSize, totalSize);\\n        // add the move task for those partitions that do not need merging\\n        if (toMove.size() > 0) {\\n            // Note: this path should be specific to concatenate; never executed in a select query.\\n            // modify the existing move task as it is already in the candidate running tasks\\n            // running the MoveTask and MR task in parallel may\\n            // cause the mvTask write to /ds=1 and MR task write\\n            // to /ds=1_1 for the same partition.\\n            // make the MoveTask as the child of the MR Task\\n            resTsks.add(mrAndMvTask);\\n            // Originally the mvTask and the child move task of the mrAndMvTask contain the same\\n            // MoveWork object.\\n            // If the blobstore optimizations are on and the input/output paths are merged\\n            // in the move only MoveWork, the mvTask and the child move task of the mrAndMvTask\\n            // will contain different MoveWork objects, which causes problems.\\n            // Not just in this case, but also in general the child move task of the mrAndMvTask should\\n            // be used, because that is the correct move task for the \"merge and move\" use case.\\n            Task<?> mergeAndMoveMoveTask = mrAndMvTask.getChildTasks().get(0);\\n            MoveWork mvWork = (MoveWork) mergeAndMoveMoveTask.getWork();\\n            LoadFileDesc lfd = mvWork.getLoadFileWork();\\n            Path targetDir = lfd.getTargetDir();\\n            List<Path> targetDirs = new ArrayList<Path>(toMove.size());\\n            for (int i = 0; i < toMove.size(); i++) {\\n                // Here directly the path name is used, instead of the uri because the uri contains the\\n                // serialized version of the path. For dynamic partition, we need the non serialized\\n                // version of the path as this value is used directly as partition name to create the partition.\\n                // For example, if the dp name is \"part=2022-01-16 04:35:56.732\" then the uri\\n                // will contain \"part=2022-01-162022-01-16%2004%253A35%253A56.732\". When we convert it to\\n                // partition name, it will come as \"part=2022-01-16 04%3A35%3A56.732\". But the path will have\\n                // value \"part=2022-01-16 04%3A35%3A56.732\", which will get converted to proper name by\\n                // function escapePathName.\\n                String[] moveStrSplits = toMove.get(i).toString().split(Path.SEPARATOR);\\n                int dpIndex = moveStrSplits.length - dpLbLevel;\\n                Path target = targetDir;\\n                while (dpIndex < moveStrSplits.length) {\\n                    target = new Path(target, moveStrSplits[dpIndex]);\\n                    dpIndex++;\\n                }\\n                targetDirs.add(target);\\n            }\\n            LoadMultiFilesDesc lmfd = new LoadMultiFilesDesc(toMove, targetDirs, lfd.getIsDfsDir(), lfd.getColumns(), lfd.getColumnTypes());\\n            mvWork.setLoadFileWork(null);\\n            mvWork.setLoadTableWork(null);\\n            mvWork.setMultiFilesDesc(lmfd);\\n        } else {\\n            resTsks.add(mrTask);\\n        }\\n    } else {\\n        // add the move task\\n        resTsks.add(mvTask);\\n    }\\n}\\nprivate void setupMapRedWork(HiveConf conf, MapWork mWork, long targetSize, long totalSize) {\\n    mWork.setMaxSplitSize(targetSize);\\n    mWork.setMinSplitSize(targetSize);\\n    mWork.setMinSplitSizePerNode(targetSize);\\n    mWork.setMinSplitSizePerRack(targetSize);\\n    mWork.setIsMergeFromResolver(true);\\n}\\nprivate static class FileSummary {\\n\\n    private final long totalSize;\\n\\n    private final long numFiles;\\n\\n    public FileSummary(long totalSize, long numFiles) {\\n        this.totalSize = totalSize;\\n        this.numFiles = numFiles;\\n    }\\n\\n    public long getTotalSize() {\\n        return totalSize;\\n    }\\n\\n    public long getNumFiles() {\\n        return numFiles;\\n    }\\n}\\nprivate FileSummary getFileSummary(List<FileStatus> fileStatusList) {\\n    LongSummaryStatistics stats = fileStatusList.stream().filter(FileStatus::isFile).mapToLong(FileStatus::getLen).summaryStatistics();\\n    return new FileSummary(stats.getSum(), stats.getCount());\\n}\\nprivate List<FileStatus> getManifestFilePaths(HiveConf conf, Path dirPath) throws IOException {\\n    FileSystem manifestFs = dirPath.getFileSystem(conf);\\n    List<String> filesKept;\\n    List<FileStatus> pathsKept = new ArrayList<>();\\n    try (FSDataInputStream inStream = manifestFs.open(new Path(dirPath, Utilities.BLOB_MANIFEST_FILE))) {\\n        String paths = IOUtils.toString(inStream, Charset.defaultCharset());\\n        filesKept = Lists.newArrayList(paths.split(System.lineSeparator()));\\n    }\\n    // The first string contains the directory information. Not useful.\\n    filesKept.remove(0);\\n    for (String file : filesKept) {\\n        pathsKept.add(manifestFs.getFileStatus(new Path(file)));\\n    }\\n    return pathsKept;\\n}\\nprivate long getMergeSize(FileSystem inpFs, Path dirPath, long avgSize) {\\n    List<FileStatus> result = FileUtils.getFileStatusRecurse(dirPath, inpFs);\\n    return getMergeSize(result, avgSize);\\n}\\n/**\\n * Whether to merge files inside directory given the threshold of the average file size.\\n *\\n * @param fileStatuses a list of FileStatus instances.\\n * @param avgSize threshold of average file size.\\n * @return -1 if not need to merge (either because of there is only 1 file or the\\n * average size is larger than avgSize). Otherwise the size of the total size of files.\\n * If return value is 0 that means there are multiple files each of which is an empty file.\\n * This could be true when the table is bucketized and all buckets are empty.\\n */\\nprivate long getMergeSize(List<FileStatus> fileStatuses, long avgSize) {\\n    FileSummary fileSummary = getFileSummary(fileStatuses);\\n    if (fileSummary.getTotalSize() <= 0) {\\n        return -1;\\n    }\\n    if (fileSummary.getNumFiles() <= 1) {\\n        return -1;\\n    }\\n    if (fileSummary.getTotalSize() / fileSummary.getNumFiles() < avgSize) {\\n        return fileSummary.getTotalSize();\\n    }\\n    return -1;\\n}\\nprivate void setupWorkWhenUsingManifestFile(MapWork mapWork, List<FileStatus> fileStatuses, Path dirPath, boolean isTblLevel) {\\n    Map<String, Operator<? extends OperatorDesc>> aliasToWork = mapWork.getAliasToWork();\\n    Map<Path, PartitionDesc> pathToPartitionInfo = mapWork.getPathToPartitionInfo();\\n    Operator<? extends OperatorDesc> op = aliasToWork.get(dirPath.toString());\\n    PartitionDesc partitionDesc = pathToPartitionInfo.get(dirPath);\\n    Path tmpDirPath = Utilities.toTempPath(dirPath);\\n    if (op != null) {\\n        aliasToWork.remove(dirPath.toString());\\n        aliasToWork.put(tmpDirPath.toString(), op);\\n        mapWork.setAliasToWork(aliasToWork);\\n    }\\n    if (partitionDesc != null) {\\n        pathToPartitionInfo.remove(dirPath);\\n        pathToPartitionInfo.put(tmpDirPath, partitionDesc);\\n        mapWork.setPathToPartitionInfo(pathToPartitionInfo);\\n    }\\n    mapWork.removePathToAlias(dirPath);\\n    mapWork.addPathToAlias(tmpDirPath, tmpDirPath.toString());\\n    if (isTblLevel) {\\n        List<Path> inputPaths = fileStatuses.stream().filter(FileStatus::isFile).map(FileStatus::getPath).collect(Collectors.toList());\\n        mapWork.setInputPaths(inputPaths);\\n    }\\n    mapWork.setUseInputPathsDirectly(true);\\n}\\nprivate Map<FileStatus, List<FileStatus>> getManifestDirs(FileSystem inpFs, List<FileStatus> fileStatuses) throws IOException {\\n    Map<FileStatus, List<FileStatus>> manifestDirsToPaths = new HashMap<>();\\n    for (FileStatus fileStatus : fileStatuses) {\\n        if (!fileStatus.isDirectory()) {\\n            FileStatus parentDir = inpFs.getFileStatus(fileStatus.getPath().getParent());\\n            List<FileStatus> fileStatusList = Lists.newArrayList(fileStatus);\\n            manifestDirsToPaths.merge(parentDir, fileStatusList, (oldValue, newValue) -> {\\n                oldValue.addAll(newValue);\\n                return oldValue;\\n            });\\n        }\\n    }\\n    return manifestDirsToPaths;\\n}\\n}\\n\\nql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java (Before)\\npublic class MapWork extends BaseWork {\\nprivate ProbeDecodeContext probeDecodeContext = null;\\npublic MapWork() {\\n}\\npublic MapWork(String name) {\\n    super(name);\\n}\\n@Explain(vectorization = Vectorization.SUMMARY, displayName = \"Map Vectorization\", explainLevels = { Level.DEFAULT, Level.EXTENDED })\\npublic MapExplainVectorization getMapExplainVectorization() {\\n    if (!getVectorizationExamined()) {\\n        return null;\\n    }\\n    return new MapExplainVectorization(this);\\n}\\n}\\n\\nql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java (After)\\npublic class MapWork extends BaseWork {\\nprivate ProbeDecodeContext probeDecodeContext = null;\\nprotected List<Path> inputPaths;\\nprivate boolean useInputPathsDirectly;\\npublic MapWork() {\\n}\\npublic MapWork(String name) {\\n    super(name);\\n}\\n@Explain(vectorization = Vectorization.SUMMARY, displayName = \"Map Vectorization\", explainLevels = { Level.DEFAULT, Level.EXTENDED })\\npublic MapExplainVectorization getMapExplainVectorization() {\\n    if (!getVectorizationExamined()) {\\n        return null;\\n    }\\n    return new MapExplainVectorization(this);\\n}\\npublic List<Path> getInputPaths() {\\n    return inputPaths;\\n}\\npublic void setInputPaths(List<Path> inputPaths) {\\n    this.inputPaths = inputPaths;\\n}\\npublic void setUseInputPathsDirectly(boolean useInputPathsDirectly) {\\n    this.useInputPathsDirectly = useInputPathsDirectly;\\n}\\npublic boolean isUseInputPathsDirectly() {\\n    return useInputPathsDirectly;\\n}\\n}\\n\\n\\nSummary:', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Given a Git diff and the relevant source code, write a concise summary of the code changes in a way that a non-technical person can understand. Summarize in maximum three concise sentences. \\n\\nAvoid adding any additional comments or annotations to the summary.\\n\\nGit diff:\\ndiff --git a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java\\nindex e1b6f9352..ca2906d88 100644\\n--- a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java\\n+++ b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java\\n@@ -26,6 +26,7 @@ import java.util.Map;\\n \\n import javax.jms.JMSException;\\n \\n+import com.thoughtworks.xstream.io.json.JsonHierarchicalStreamDriver;\\n import org.apache.activemq.advisory.AdvisorySupport;\\n import org.apache.activemq.broker.BrokerContext;\\n import org.apache.activemq.broker.BrokerContextAware;\\n@@ -102,6 +103,7 @@ public class JmsFrameTranslator extends LegacyFrameTranslator implements\\n     @Override\\n     public StompFrame convertMessage(ProtocolConverter converter,\\n             ActiveMQMessage message) throws IOException, JMSException {\\n+\\n         if (message.getDataStructureType() == ActiveMQObjectMessage.DATA_STRUCTURE_TYPE) {\\n             StompFrame command = new StompFrame();\\n             command.setAction(Stomp.Responses.MESSAGE);\\n@@ -153,6 +155,10 @@ public class JmsFrameTranslator extends LegacyFrameTranslator implements\\n             FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(\\n                     converter, message, command, this);\\n \\n+            if (!headers.containsKey(Stomp.Headers.TRANSFORMATION)) {\\n+                headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_ADVISORY_JSON.toString());\\n+            }\\n+\\n             if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_XML.toString())) {\\n                 headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_ADVISORY_XML.toString());\\n             } else if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_JSON.toString())) {\\n@@ -274,4 +280,16 @@ public class JmsFrameTranslator extends LegacyFrameTranslator implements\\n     public void setBrokerContext(BrokerContext brokerContext) {\\n         this.brokerContext = brokerContext;\\n     }\\n+\\n+    /**\\n+     * Return an Advisory message as a JSON formatted string\\n+     * @param ds\\n+     * @return\\n+     */\\n+    protected String marshallAdvisory(final DataStructure ds) {\\n+        XStream xstream = new XStream(new JsonHierarchicalStreamDriver());\\n+        xstream.setMode(XStream.NO_REFERENCES);\\n+        xstream.aliasPackage(\"\", \"org.apache.activemq.command\");\\n+        return xstream.toXML(ds);\\n+    }\\n }\\ndiff --git a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java\\nindex 1d826a44e..8cfa1219e 100644\\n--- a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java\\n+++ b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java\\n@@ -16,25 +16,19 @@\\n  */\\n package org.apache.activemq.transport.stomp;\\n \\n-import java.io.DataOutputStream;\\n-import java.io.IOException;\\n-import java.util.HashMap;\\n-import java.util.Map;\\n-\\n-import javax.jms.Destination;\\n-import javax.jms.JMSException;\\n-\\n-import org.apache.activemq.advisory.AdvisorySupport;\\n import org.apache.activemq.command.ActiveMQBytesMessage;\\n import org.apache.activemq.command.ActiveMQDestination;\\n import org.apache.activemq.command.ActiveMQMessage;\\n import org.apache.activemq.command.ActiveMQTextMessage;\\n-import org.apache.activemq.command.DataStructure;\\n import org.apache.activemq.util.ByteArrayOutputStream;\\n import org.apache.activemq.util.ByteSequence;\\n \\n-import com.thoughtworks.xstream.XStream;\\n-import com.thoughtworks.xstream.io.json.JsonHierarchicalStreamDriver;\\n+import javax.jms.Destination;\\n+import javax.jms.JMSException;\\n+import java.io.DataOutputStream;\\n+import java.io.IOException;\\n+import java.util.HashMap;\\n+import java.util.Map;\\n \\n /**\\n  * Implements ActiveMQ 4.0 translations\\n@@ -127,15 +121,8 @@ public class LegacyFrameTranslator implements FrameTranslator {\\n \\n             headers.put(Stomp.Headers.CONTENT_LENGTH, Integer.toString(data.length));\\n             command.setContent(data);\\n-        } else if (message.getDataStructureType() == ActiveMQMessage.DATA_STRUCTURE_TYPE &&\\n-                AdvisorySupport.ADIVSORY_MESSAGE_TYPE.equals(message.getType())) {\\n-\\n-            FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(\\n-                    converter, message, command, this);\\n-\\n-            String body = marshallAdvisory(message.getDataStructure());\\n-            command.setContent(body.getBytes(\"UTF-8\"));\\n         }\\n+\\n         return command;\\n     }\\n \\n@@ -212,15 +199,5 @@ public class LegacyFrameTranslator implements FrameTranslator {\\n         }\\n     }\\n \\n-    /**\\n-     * Return an Advisory message as a JSON formatted string\\n-     * @param ds\\n-     * @return\\n-     */\\n-    protected String marshallAdvisory(final DataStructure ds) {\\n-        XStream xstream = new XStream(new JsonHierarchicalStreamDriver());\\n-        xstream.setMode(XStream.NO_REFERENCES);\\n-        xstream.aliasPackage(\"\", \"org.apache.activemq.command\");\\n-        return xstream.toXML(ds);\\n-    }\\n+\\n }\\ndiff --git a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java\\nindex a6a22f17f..f31aad1b6 100644\\n--- a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java\\n+++ b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java\\n@@ -31,6 +31,7 @@ import java.util.concurrent.atomic.AtomicBoolean;\\n import javax.jms.JMSException;\\n \\n import org.apache.activemq.ActiveMQPrefetchPolicy;\\n+import org.apache.activemq.advisory.AdvisorySupport;\\n import org.apache.activemq.broker.BrokerContext;\\n import org.apache.activemq.broker.BrokerContextAware;\\n import org.apache.activemq.command.ActiveMQDestination;\\n@@ -200,19 +201,28 @@ public class ProtocolConverter {\\n     }\\n \\n     protected FrameTranslator findTranslator(String header) {\\n+        return findTranslator(header, null);\\n+    }\\n+\\n+    protected FrameTranslator findTranslator(String header, ActiveMQDestination destination) {\\n         FrameTranslator translator = frameTranslator;\\n         try {\\n             if (header != null) {\\n                 translator = (FrameTranslator) FRAME_TRANSLATOR_FINDER\\n                         .newInstance(header);\\n-                if (translator instanceof BrokerContextAware) {\\n-                    ((BrokerContextAware)translator).setBrokerContext(brokerContext);\\n+            } else {\\n+                if (destination != null && AdvisorySupport.isAdvisoryTopic(destination)) {\\n+                    translator = new JmsFrameTranslator();\\n                 }\\n             }\\n         } catch (Exception ignore) {\\n             // if anything goes wrong use the default translator\\n         }\\n \\n+        if (translator instanceof BrokerContextAware) {\\n+            ((BrokerContextAware)translator).setBrokerContext(brokerContext);\\n+        }\\n+\\n         return translator;\\n     }\\n \\n@@ -879,7 +889,7 @@ public class ProtocolConverter {\\n         if (ignoreTransformation == true) {\\n             return frameTranslator.convertMessage(this, message);\\n         } else {\\n-            return findTranslator(message.getStringProperty(Stomp.Headers.TRANSFORMATION)).convertMessage(this, message);\\n+            return findTranslator(message.getStringProperty(Stomp.Headers.TRANSFORMATION), message.getDestination()).convertMessage(this, message);\\n         }\\n     }\\n \\n\\n\\nSource code:\\nactivemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java (Before)\\nimport javax.jms.JMSException;\\nimport org.apache.activemq.advisory.AdvisorySupport;\\nimport org.apache.activemq.broker.BrokerContext;\\nimport org.apache.activemq.broker.BrokerContextAware;\\npublic class JmsFrameTranslator extends LegacyFrameTranslator implements BrokerContextAware {\\n@Override\\npublic StompFrame convertMessage(ProtocolConverter converter, ActiveMQMessage message) throws IOException, JMSException {\\n    if (message.getDataStructureType() == ActiveMQObjectMessage.DATA_STRUCTURE_TYPE) {\\n        StompFrame command = new StompFrame();\\n        command.setAction(Stomp.Responses.MESSAGE);\\n        Map<String, String> headers = new HashMap<String, String>(25);\\n        command.setHeaders(headers);\\n        FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(converter, message, command, this);\\n        if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_XML.toString())) {\\n            headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_OBJECT_XML.toString());\\n        } else if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_JSON.toString())) {\\n            headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_OBJECT_JSON.toString());\\n        }\\n        ActiveMQObjectMessage msg = (ActiveMQObjectMessage) message.copy();\\n        command.setContent(marshall(msg.getObject(), headers.get(Stomp.Headers.TRANSFORMATION)).getBytes(\"UTF-8\"));\\n        return command;\\n    } else if (message.getDataStructureType() == ActiveMQMapMessage.DATA_STRUCTURE_TYPE) {\\n        StompFrame command = new StompFrame();\\n        command.setAction(Stomp.Responses.MESSAGE);\\n        Map<String, String> headers = new HashMap<String, String>(25);\\n        command.setHeaders(headers);\\n        FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(converter, message, command, this);\\n        if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_XML.toString())) {\\n            headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_MAP_XML.toString());\\n        } else if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_JSON.toString())) {\\n            headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_MAP_JSON.toString());\\n        }\\n        ActiveMQMapMessage msg = (ActiveMQMapMessage) message.copy();\\n        command.setContent(marshall((Serializable) msg.getContentMap(), headers.get(Stomp.Headers.TRANSFORMATION)).getBytes(\"UTF-8\"));\\n        return command;\\n    } else if (message.getDataStructureType() == ActiveMQMessage.DATA_STRUCTURE_TYPE && AdvisorySupport.ADIVSORY_MESSAGE_TYPE.equals(message.getType())) {\\n        StompFrame command = new StompFrame();\\n        command.setAction(Stomp.Responses.MESSAGE);\\n        Map<String, String> headers = new HashMap<String, String>(25);\\n        command.setHeaders(headers);\\n        FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(converter, message, command, this);\\n        if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_XML.toString())) {\\n            headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_ADVISORY_XML.toString());\\n        } else if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_JSON.toString())) {\\n            headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_ADVISORY_JSON.toString());\\n        }\\n        String body = marshallAdvisory(message.getDataStructure(), headers.get(Stomp.Headers.TRANSFORMATION));\\n        command.setContent(body.getBytes(\"UTF-8\"));\\n        return command;\\n    } else {\\n        return super.convertMessage(converter, message);\\n    }\\n}\\n@Override\\npublic void setBrokerContext(BrokerContext brokerContext) {\\n    this.brokerContext = brokerContext;\\n}\\n}\\n\\nactivemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java (After)\\nimport javax.jms.JMSException;\\nimport com.thoughtworks.xstream.io.json.JsonHierarchicalStreamDriver;\\nimport org.apache.activemq.advisory.AdvisorySupport;\\nimport org.apache.activemq.broker.BrokerContext;\\nimport org.apache.activemq.broker.BrokerContextAware;\\npublic class JmsFrameTranslator extends LegacyFrameTranslator implements BrokerContextAware {\\n@Override\\npublic StompFrame convertMessage(ProtocolConverter converter, ActiveMQMessage message) throws IOException, JMSException {\\n    if (message.getDataStructureType() == ActiveMQObjectMessage.DATA_STRUCTURE_TYPE) {\\n        StompFrame command = new StompFrame();\\n        command.setAction(Stomp.Responses.MESSAGE);\\n        Map<String, String> headers = new HashMap<String, String>(25);\\n        command.setHeaders(headers);\\n        FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(converter, message, command, this);\\n        if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_XML.toString())) {\\n            headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_OBJECT_XML.toString());\\n        } else if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_JSON.toString())) {\\n            headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_OBJECT_JSON.toString());\\n        }\\n        ActiveMQObjectMessage msg = (ActiveMQObjectMessage) message.copy();\\n        command.setContent(marshall(msg.getObject(), headers.get(Stomp.Headers.TRANSFORMATION)).getBytes(\"UTF-8\"));\\n        return command;\\n    } else if (message.getDataStructureType() == ActiveMQMapMessage.DATA_STRUCTURE_TYPE) {\\n        StompFrame command = new StompFrame();\\n        command.setAction(Stomp.Responses.MESSAGE);\\n        Map<String, String> headers = new HashMap<String, String>(25);\\n        command.setHeaders(headers);\\n        FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(converter, message, command, this);\\n        if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_XML.toString())) {\\n            headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_MAP_XML.toString());\\n        } else if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_JSON.toString())) {\\n            headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_MAP_JSON.toString());\\n        }\\n        ActiveMQMapMessage msg = (ActiveMQMapMessage) message.copy();\\n        command.setContent(marshall((Serializable) msg.getContentMap(), headers.get(Stomp.Headers.TRANSFORMATION)).getBytes(\"UTF-8\"));\\n        return command;\\n    } else if (message.getDataStructureType() == ActiveMQMessage.DATA_STRUCTURE_TYPE && AdvisorySupport.ADIVSORY_MESSAGE_TYPE.equals(message.getType())) {\\n        StompFrame command = new StompFrame();\\n        command.setAction(Stomp.Responses.MESSAGE);\\n        Map<String, String> headers = new HashMap<String, String>(25);\\n        command.setHeaders(headers);\\n        FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(converter, message, command, this);\\n        if (!headers.containsKey(Stomp.Headers.TRANSFORMATION)) {\\n            headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_ADVISORY_JSON.toString());\\n        }\\n        if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_XML.toString())) {\\n            headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_ADVISORY_XML.toString());\\n        } else if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_JSON.toString())) {\\n            headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_ADVISORY_JSON.toString());\\n        }\\n        String body = marshallAdvisory(message.getDataStructure(), headers.get(Stomp.Headers.TRANSFORMATION));\\n        command.setContent(body.getBytes(\"UTF-8\"));\\n        return command;\\n    } else {\\n        return super.convertMessage(converter, message);\\n    }\\n}\\n@Override\\npublic void setBrokerContext(BrokerContext brokerContext) {\\n    this.brokerContext = brokerContext;\\n}\\n/**\\n * Return an Advisory message as a JSON formatted string\\n * @param ds\\n * @return\\n */\\nprotected String marshallAdvisory(final DataStructure ds) {\\n    XStream xstream = new XStream(new JsonHierarchicalStreamDriver());\\n    xstream.setMode(XStream.NO_REFERENCES);\\n    xstream.aliasPackage(\"\", \"org.apache.activemq.command\");\\n    return xstream.toXML(ds);\\n}\\n}\\n\\nactivemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java (Before)\\npackage org.apache.activemq.transport.stomp;\\n\\nimport java.io.DataOutputStream;\\nimport java.io.IOException;\\nimport java.util.HashMap;\\nimport java.util.Map;\\nimport javax.jms.Destination;\\nimport javax.jms.JMSException;\\nimport org.apache.activemq.advisory.AdvisorySupport;\\nimport org.apache.activemq.command.ActiveMQBytesMessage;\\nimport org.apache.activemq.command.ActiveMQDestination;\\nimport org.apache.activemq.command.ActiveMQMessage;\\nimport org.apache.activemq.command.ActiveMQTextMessage;\\nimport org.apache.activemq.command.DataStructure;\\nimport org.apache.activemq.util.ByteArrayOutputStream;\\nimport org.apache.activemq.util.ByteSequence;\\nimport com.thoughtworks.xstream.XStream;\\nimport com.thoughtworks.xstream.io.json.JsonHierarchicalStreamDriver;\\npublic class LegacyFrameTranslator implements FrameTranslator {\\npublic StompFrame convertMessage(ProtocolConverter converter, ActiveMQMessage message) throws IOException, JMSException {\\n    StompFrame command = new StompFrame();\\n    command.setAction(Stomp.Responses.MESSAGE);\\n    Map<String, String> headers = new HashMap<String, String>(25);\\n    command.setHeaders(headers);\\n    FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(converter, message, command, this);\\n    if (message.getDataStructureType() == ActiveMQTextMessage.DATA_STRUCTURE_TYPE) {\\n        if (!message.isCompressed() && message.getContent() != null) {\\n            ByteSequence msgContent = message.getContent();\\n            if (msgContent.getLength() > 4) {\\n                byte[] content = new byte[msgContent.getLength() - 4];\\n                System.arraycopy(msgContent.data, 4, content, 0, content.length);\\n                command.setContent(content);\\n            }\\n        } else {\\n            ActiveMQTextMessage msg = (ActiveMQTextMessage) message.copy();\\n            String messageText = msg.getText();\\n            if (messageText != null) {\\n                command.setContent(msg.getText().getBytes(\"UTF-8\"));\\n            }\\n        }\\n    } else if (message.getDataStructureType() == ActiveMQBytesMessage.DATA_STRUCTURE_TYPE) {\\n        ActiveMQBytesMessage msg = (ActiveMQBytesMessage) message.copy();\\n        msg.setReadOnlyBody(true);\\n        byte[] data = new byte[(int) msg.getBodyLength()];\\n        msg.readBytes(data);\\n        headers.put(Stomp.Headers.CONTENT_LENGTH, Integer.toString(data.length));\\n        command.setContent(data);\\n    } else if (message.getDataStructureType() == ActiveMQMessage.DATA_STRUCTURE_TYPE && AdvisorySupport.ADIVSORY_MESSAGE_TYPE.equals(message.getType())) {\\n        FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(converter, message, command, this);\\n        String body = marshallAdvisory(message.getDataStructure());\\n        command.setContent(body.getBytes(\"UTF-8\"));\\n    }\\n    return command;\\n}\\npublic ActiveMQDestination convertDestination(ProtocolConverter converter, String name, boolean forceFallback) throws ProtocolException {\\n    if (name == null) {\\n        return null;\\n    }\\n    // in case of space padding by a client we trim for the initial detection, on fallback use\\n    // the un-trimmed value.\\n    String originalName = name;\\n    name = name.trim();\\n    if (name.startsWith(\"/queue/\")) {\\n        String qName = name.substring(\"/queue/\".length(), name.length());\\n        return ActiveMQDestination.createDestination(qName, ActiveMQDestination.QUEUE_TYPE);\\n    } else if (name.startsWith(\"/topic/\")) {\\n        String tName = name.substring(\"/topic/\".length(), name.length());\\n        return ActiveMQDestination.createDestination(tName, ActiveMQDestination.TOPIC_TYPE);\\n    } else if (name.startsWith(\"/remote-temp-queue/\")) {\\n        String tName = name.substring(\"/remote-temp-queue/\".length(), name.length());\\n        return ActiveMQDestination.createDestination(tName, ActiveMQDestination.TEMP_QUEUE_TYPE);\\n    } else if (name.startsWith(\"/remote-temp-topic/\")) {\\n        String tName = name.substring(\"/remote-temp-topic/\".length(), name.length());\\n        return ActiveMQDestination.createDestination(tName, ActiveMQDestination.TEMP_TOPIC_TYPE);\\n    } else if (name.startsWith(\"/temp-queue/\")) {\\n        return converter.createTempDestination(name, false);\\n    } else if (name.startsWith(\"/temp-topic/\")) {\\n        return converter.createTempDestination(name, true);\\n    } else {\\n        if (forceFallback) {\\n            try {\\n                ActiveMQDestination fallback = ActiveMQDestination.getUnresolvableDestinationTransformer().transform(originalName);\\n                if (fallback != null) {\\n                    return fallback;\\n                }\\n            } catch (JMSException e) {\\n                throw new ProtocolException(\"Illegal destination name: [\" + originalName + \"] -- ActiveMQ STOMP destinations \" + \"must begin with one of: /queue/ /topic/ /temp-queue/ /temp-topic/\", false, e);\\n            }\\n        }\\n        throw new ProtocolException(\"Illegal destination name: [\" + originalName + \"] -- ActiveMQ STOMP destinations \" + \"must begin with one of: /queue/ /topic/ /temp-queue/ /temp-topic/\");\\n    }\\n}\\n/**\\n * Return an Advisory message as a JSON formatted string\\n * @param ds\\n * @return\\n */\\nprotected String marshallAdvisory(final DataStructure ds) {\\n    XStream xstream = new XStream(new JsonHierarchicalStreamDriver());\\n    xstream.setMode(XStream.NO_REFERENCES);\\n    xstream.aliasPackage(\"\", \"org.apache.activemq.command\");\\n    return xstream.toXML(ds);\\n}\\n}\\n\\nactivemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java (After)\\npackage org.apache.activemq.transport.stomp;\\n\\nimport org.apache.activemq.command.ActiveMQBytesMessage;\\nimport org.apache.activemq.command.ActiveMQDestination;\\nimport org.apache.activemq.command.ActiveMQMessage;\\nimport org.apache.activemq.command.ActiveMQTextMessage;\\nimport org.apache.activemq.util.ByteArrayOutputStream;\\nimport org.apache.activemq.util.ByteSequence;\\nimport javax.jms.Destination;\\nimport javax.jms.JMSException;\\nimport java.io.DataOutputStream;\\nimport java.io.IOException;\\nimport java.util.HashMap;\\nimport java.util.Map;\\npublic class LegacyFrameTranslator implements FrameTranslator {\\npublic StompFrame convertMessage(ProtocolConverter converter, ActiveMQMessage message) throws IOException, JMSException {\\n    StompFrame command = new StompFrame();\\n    command.setAction(Stomp.Responses.MESSAGE);\\n    Map<String, String> headers = new HashMap<String, String>(25);\\n    command.setHeaders(headers);\\n    FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(converter, message, command, this);\\n    if (message.getDataStructureType() == ActiveMQTextMessage.DATA_STRUCTURE_TYPE) {\\n        if (!message.isCompressed() && message.getContent() != null) {\\n            ByteSequence msgContent = message.getContent();\\n            if (msgContent.getLength() > 4) {\\n                byte[] content = new byte[msgContent.getLength() - 4];\\n                System.arraycopy(msgContent.data, 4, content, 0, content.length);\\n                command.setContent(content);\\n            }\\n        } else {\\n            ActiveMQTextMessage msg = (ActiveMQTextMessage) message.copy();\\n            String messageText = msg.getText();\\n            if (messageText != null) {\\n                command.setContent(msg.getText().getBytes(\"UTF-8\"));\\n            }\\n        }\\n    } else if (message.getDataStructureType() == ActiveMQBytesMessage.DATA_STRUCTURE_TYPE) {\\n        ActiveMQBytesMessage msg = (ActiveMQBytesMessage) message.copy();\\n        msg.setReadOnlyBody(true);\\n        byte[] data = new byte[(int) msg.getBodyLength()];\\n        msg.readBytes(data);\\n        headers.put(Stomp.Headers.CONTENT_LENGTH, Integer.toString(data.length));\\n        command.setContent(data);\\n    }\\n    return command;\\n}\\npublic ActiveMQDestination convertDestination(ProtocolConverter converter, String name, boolean forceFallback) throws ProtocolException {\\n    if (name == null) {\\n        return null;\\n    }\\n    // in case of space padding by a client we trim for the initial detection, on fallback use\\n    // the un-trimmed value.\\n    String originalName = name;\\n    name = name.trim();\\n    if (name.startsWith(\"/queue/\")) {\\n        String qName = name.substring(\"/queue/\".length(), name.length());\\n        return ActiveMQDestination.createDestination(qName, ActiveMQDestination.QUEUE_TYPE);\\n    } else if (name.startsWith(\"/topic/\")) {\\n        String tName = name.substring(\"/topic/\".length(), name.length());\\n        return ActiveMQDestination.createDestination(tName, ActiveMQDestination.TOPIC_TYPE);\\n    } else if (name.startsWith(\"/remote-temp-queue/\")) {\\n        String tName = name.substring(\"/remote-temp-queue/\".length(), name.length());\\n        return ActiveMQDestination.createDestination(tName, ActiveMQDestination.TEMP_QUEUE_TYPE);\\n    } else if (name.startsWith(\"/remote-temp-topic/\")) {\\n        String tName = name.substring(\"/remote-temp-topic/\".length(), name.length());\\n        return ActiveMQDestination.createDestination(tName, ActiveMQDestination.TEMP_TOPIC_TYPE);\\n    } else if (name.startsWith(\"/temp-queue/\")) {\\n        return converter.createTempDestination(name, false);\\n    } else if (name.startsWith(\"/temp-topic/\")) {\\n        return converter.createTempDestination(name, true);\\n    } else {\\n        if (forceFallback) {\\n            try {\\n                ActiveMQDestination fallback = ActiveMQDestination.getUnresolvableDestinationTransformer().transform(originalName);\\n                if (fallback != null) {\\n                    return fallback;\\n                }\\n            } catch (JMSException e) {\\n                throw new ProtocolException(\"Illegal destination name: [\" + originalName + \"] -- ActiveMQ STOMP destinations \" + \"must begin with one of: /queue/ /topic/ /temp-queue/ /temp-topic/\", false, e);\\n            }\\n        }\\n        throw new ProtocolException(\"Illegal destination name: [\" + originalName + \"] -- ActiveMQ STOMP destinations \" + \"must begin with one of: /queue/ /topic/ /temp-queue/ /temp-topic/\");\\n    }\\n}\\n}\\n\\nactivemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java (Before)\\nimport javax.jms.JMSException;\\nimport org.apache.activemq.ActiveMQPrefetchPolicy;\\nimport org.apache.activemq.broker.BrokerContext;\\nimport org.apache.activemq.broker.BrokerContextAware;\\nimport org.apache.activemq.command.ActiveMQDestination;\\npublic class ProtocolConverter {\\nprotected void sendToStomp(StompFrame command) throws IOException {\\n    stompTransport.sendToStomp(command);\\n}\\nprotected FrameTranslator findTranslator(String header) {\\n    FrameTranslator translator = frameTranslator;\\n    try {\\n        if (header != null) {\\n            translator = (FrameTranslator) FRAME_TRANSLATOR_FINDER.newInstance(header);\\n            if (translator instanceof BrokerContextAware) {\\n                ((BrokerContextAware) translator).setBrokerContext(brokerContext);\\n            }\\n        }\\n    } catch (Exception ignore) {\\n        // if anything goes wrong use the default translator\\n    }\\n    return translator;\\n}\\npublic StompFrame convertMessage(ActiveMQMessage message, boolean ignoreTransformation) throws IOException, JMSException {\\n    if (ignoreTransformation == true) {\\n        return frameTranslator.convertMessage(this, message);\\n    } else {\\n        return findTranslator(message.getStringProperty(Stomp.Headers.TRANSFORMATION)).convertMessage(this, message);\\n    }\\n}\\n}\\n\\nactivemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java (After)\\nimport javax.jms.JMSException;\\nimport org.apache.activemq.ActiveMQPrefetchPolicy;\\nimport org.apache.activemq.advisory.AdvisorySupport;\\nimport org.apache.activemq.broker.BrokerContext;\\nimport org.apache.activemq.broker.BrokerContextAware;\\nimport org.apache.activemq.command.ActiveMQDestination;\\npublic class ProtocolConverter {\\nprotected void sendToStomp(StompFrame command) throws IOException {\\n    stompTransport.sendToStomp(command);\\n}\\nprotected FrameTranslator findTranslator(String header) {\\n    return findTranslator(header, null);\\n}\\nprotected FrameTranslator findTranslator(String header, ActiveMQDestination destination) {\\n    FrameTranslator translator = frameTranslator;\\n    try {\\n        if (header != null) {\\n            translator = (FrameTranslator) FRAME_TRANSLATOR_FINDER.newInstance(header);\\n        } else {\\n            if (destination != null && AdvisorySupport.isAdvisoryTopic(destination)) {\\n                translator = new JmsFrameTranslator();\\n            }\\n        }\\n    } catch (Exception ignore) {\\n        // if anything goes wrong use the default translator\\n    }\\n    if (translator instanceof BrokerContextAware) {\\n        ((BrokerContextAware) translator).setBrokerContext(brokerContext);\\n    }\\n    return translator;\\n}\\npublic StompFrame convertMessage(ActiveMQMessage message, boolean ignoreTransformation) throws IOException, JMSException {\\n    if (ignoreTransformation == true) {\\n        return frameTranslator.convertMessage(this, message);\\n    } else {\\n        return findTranslator(message.getStringProperty(Stomp.Headers.TRANSFORMATION), message.getDestination()).convertMessage(this, message);\\n    }\\n}\\n}\\n\\n\\nSummary:', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Given a Git diff and the relevant source code, write a concise summary of the code changes in a way that a non-technical person can understand. Summarize in maximum three concise sentences. \\n\\nAvoid adding any additional comments or annotations to the summary.\\n\\nGit diff:\\ndiff --git a/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java b/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\\nindex 52d0efbb58..2856f131f1 100644\\n--- a/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\\n+++ b/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\\n@@ -36,6 +36,7 @@ import org.apache.cassandra.utils.ByteBufferUtil;\\n import org.apache.cassandra.utils.MurmurHash;\\n import org.apache.cassandra.utils.ObjectSizes;\\n \\n+import com.google.common.annotations.VisibleForTesting;\\n import com.google.common.primitives.Longs;\\n \\n /**\\n@@ -207,6 +208,18 @@ public class Murmur3Partitioner implements IPartitioner\\n         {\\n             return new LongToken(token + 1);\\n         }\\n+\\n+        /**\\n+         * Reverses murmur3 to find a possible 16 byte key that generates a given token\\n+         */\\n+        @VisibleForTesting\\n+        public static ByteBuffer keyForToken(LongToken token)\\n+        {\\n+            ByteBuffer result = ByteBuffer.allocate(16);\\n+            long[] inv = MurmurHash.inv_hash3_x64_128(new long[] {token.token, 0L});\\n+            result.putLong(inv[0]).putLong(inv[1]).position(0);\\n+            return result;\\n+        }\\n     }\\n \\n     /**\\ndiff --git a/src/java/org/apache/cassandra/utils/ByteBufferUtil.java b/src/java/org/apache/cassandra/utils/ByteBufferUtil.java\\nindex ff3fb3d0a8..5300d9de1f 100644\\n--- a/src/java/org/apache/cassandra/utils/ByteBufferUtil.java\\n+++ b/src/java/org/apache/cassandra/utils/ByteBufferUtil.java\\n@@ -535,6 +535,8 @@ public class ByteBufferUtil\\n             return ByteBufferUtil.bytes((InetAddress) obj);\\n         else if (obj instanceof String)\\n             return ByteBufferUtil.bytes((String) obj);\\n+        else if (obj instanceof ByteBuffer)\\n+            return (ByteBuffer) obj;\\n         else\\n             throw new IllegalArgumentException(String.format(\"Cannot convert value %s of type %s\",\\n                                                              obj,\\ndiff --git a/src/java/org/apache/cassandra/utils/MurmurHash.java b/src/java/org/apache/cassandra/utils/MurmurHash.java\\nindex c02fdcc6dc..80cf5cd39f 100644\\n--- a/src/java/org/apache/cassandra/utils/MurmurHash.java\\n+++ b/src/java/org/apache/cassandra/utils/MurmurHash.java\\n@@ -18,6 +18,9 @@\\n package org.apache.cassandra.utils;\\n \\n import java.nio.ByteBuffer;\\n+import java.util.BitSet;\\n+\\n+import com.google.common.primitives.Longs;\\n \\n /**\\n  * This is a very fast, non-cryptographic hash suitable for general hash-based\\n@@ -146,7 +149,7 @@ public class MurmurHash\\n         return h64;\\n     }\\n \\n-    protected static long getblock(ByteBuffer key, int offset, int index)\\n+    protected static long getBlock(ByteBuffer key, int offset, int index)\\n     {\\n         int i_8 = index << 3;\\n         int blockOffset = offset + i_8;\\n@@ -187,8 +190,8 @@ public class MurmurHash\\n \\n         for(int i = 0; i < nblocks; i++)\\n         {\\n-            long k1 = getblock(key, offset, i*2+0);\\n-            long k2 = getblock(key, offset, i*2+1);\\n+            long k1 = getBlock(key, offset, i * 2 + 0);\\n+            long k2 = getBlock(key, offset, i * 2 + 1);\\n \\n             k1 *= c1; k1 = rotl64(k1,31); k1 *= c2; h1 ^= k1;\\n \\n@@ -248,4 +251,108 @@ public class MurmurHash\\n         result[1] = h2;\\n     }\\n \\n+    protected static long invRotl64(long v, int n)\\n+    {\\n+        return ((v >>> n) | (v << (64 - n)));\\n+    }\\n+\\n+    protected static long invRShiftXor(long value, int shift)\\n+    {\\n+        long output = 0;\\n+        long i = 0;\\n+        while (i * shift < 64)\\n+        {\\n+            long c = (0xffffffffffffffffL << (64 - shift)) >>> (shift * i);\\n+            long partOutput = value & c;\\n+            value ^= partOutput >>> shift;\\n+            output |= partOutput;\\n+            i += 1;\\n+        }\\n+        return output;\\n+    }\\n+\\n+    protected static long invFmix(long k)\\n+    {\\n+        k = invRShiftXor(k, 33);\\n+        k *= 0x9cb4b2f8129337dbL;\\n+        k = invRShiftXor(k, 33);\\n+        k *= 0x4f74430c22a54005L;\\n+        k = invRShiftXor(k, 33);\\n+        return k;\\n+    }\\n+\\n+    /**\\n+     * This gives a correct reversal of the tail byte flip which is needed if want a non mod16==0 byte hash inv or to\\n+     * target a hash for a given schema.\\n+     */\\n+    public static long invTailReverse(long num)\\n+    {\\n+        byte[] v = Longs.toByteArray(Long.reverseBytes(num));\\n+        for (int i = 0; i < 8; i++)\\n+        {\\n+            if (v[i] < 0 && i < 7)\\n+            {\\n+                BitSet bits = BitSet.valueOf(v);\\n+                bits.flip(8 * (i + 1), 64);\\n+                v = bits.toByteArray();\\n+            }\\n+        }\\n+        return Longs.fromByteArray(v);\\n+    }\\n+\\n+    public static long[] inv_hash3_x64_128(long[] result)\\n+    {\\n+        long c1 = 0xa98409e882ce4d7dL;\\n+        long c2 = 0xa81e14edd9de2c7fL;\\n+\\n+        long k1 = 0;\\n+        long k2 = 0;\\n+        long h1 = result[0];\\n+        long h2 = result[1];\\n+\\n+        //----------\\n+        // reverse finalization\\n+        h2 -= h1;\\n+        h1 -= h2;\\n+\\n+        h1 = invFmix(h1);\\n+        h2 = invFmix(h2);\\n+\\n+        h2 -= h1;\\n+        h1 -= h2;\\n+\\n+        h1 ^= 16;\\n+        h2 ^= 16;\\n+\\n+        //----------\\n+        // reverse body\\n+        h2 -= 0x38495ab5;\\n+        h2 *= 0xcccccccccccccccdL;\\n+        h2 -= h1;\\n+        h2 = invRotl64(h2, 31);\\n+        k2 = h2;\\n+        h2 = 0;\\n+\\n+        k2 *= c1;\\n+        k2 = invRotl64(k2, 33);\\n+        k2 *= c2;\\n+\\n+        h1 -= 0x52dce729;\\n+        h1 *= 0xcccccccccccccccdL;\\n+        //h1 -= h2;\\n+        h1 = invRotl64(h1, 27);\\n+\\n+        k1 = h1;\\n+\\n+        k1 *= c2;\\n+        k1 = invRotl64(k1, 31);\\n+        k1 *= c1;\\n+\\n+        // note that while this works for body block reversing the tail reverse requires `invTailReverse`\\n+        k1 = Long.reverseBytes(k1);\\n+        k2 = Long.reverseBytes(k2);\\n+\\n+        return new long[] {k1, k2};\\n+    }\\n+\\n }\\n\\n\\nSource code:\\nsrc/java/org/apache/cassandra/dht/Murmur3Partitioner.java (Before)\\nimport org.apache.cassandra.utils.MurmurHash;\\nimport org.apache.cassandra.utils.ObjectSizes;\\nimport com.google.common.primitives.Longs;\\npublic class Murmur3Partitioner implements IPartitioner {\\npublic static class LongToken extends Token {\\n\\n    static final long serialVersionUID = -5833580143318243006L;\\n\\n    final long token;\\n\\n    public LongToken(long token) {\\n        this.token = token;\\n    }\\n\\n    public String toString() {\\n        return Long.toString(token);\\n    }\\n\\n    public boolean equals(Object obj) {\\n        if (this == obj)\\n            return true;\\n        if (obj == null || this.getClass() != obj.getClass())\\n            return false;\\n        return token == (((LongToken) obj).token);\\n    }\\n\\n    public int hashCode() {\\n        return Longs.hashCode(token);\\n    }\\n\\n    public int compareTo(Token o) {\\n        return Long.compare(token, ((LongToken) o).token);\\n    }\\n\\n    @Override\\n    public IPartitioner getPartitioner() {\\n        return instance;\\n    }\\n\\n    @Override\\n    public long getHeapSize() {\\n        return HEAP_SIZE;\\n    }\\n\\n    @Override\\n    public Object getTokenValue() {\\n        return token;\\n    }\\n\\n    @Override\\n    public double size(Token next) {\\n        LongToken n = (LongToken) next;\\n        // Overflow acceptable and desired.\\n        long v = n.token - token;\\n        // Scale so that the full range is 1.\\n        double d = Math.scalb((double) v, -Long.SIZE);\\n        // Adjust for signed long, also making sure t.size(t) == 1.\\n        return d > 0.0 ? d : (d + 1.0);\\n    }\\n\\n    @Override\\n    public Token increaseSlightly() {\\n        return new LongToken(token + 1);\\n    }\\n}\\n}\\n\\nsrc/java/org/apache/cassandra/dht/Murmur3Partitioner.java (After)\\nimport org.apache.cassandra.utils.MurmurHash;\\nimport org.apache.cassandra.utils.ObjectSizes;\\nimport com.google.common.annotations.VisibleForTesting;\\nimport com.google.common.primitives.Longs;\\npublic class Murmur3Partitioner implements IPartitioner {\\npublic static class LongToken extends Token {\\n\\n    static final long serialVersionUID = -5833580143318243006L;\\n\\n    final long token;\\n\\n    public LongToken(long token) {\\n        this.token = token;\\n    }\\n\\n    public String toString() {\\n        return Long.toString(token);\\n    }\\n\\n    public boolean equals(Object obj) {\\n        if (this == obj)\\n            return true;\\n        if (obj == null || this.getClass() != obj.getClass())\\n            return false;\\n        return token == (((LongToken) obj).token);\\n    }\\n\\n    public int hashCode() {\\n        return Longs.hashCode(token);\\n    }\\n\\n    public int compareTo(Token o) {\\n        return Long.compare(token, ((LongToken) o).token);\\n    }\\n\\n    @Override\\n    public IPartitioner getPartitioner() {\\n        return instance;\\n    }\\n\\n    @Override\\n    public long getHeapSize() {\\n        return HEAP_SIZE;\\n    }\\n\\n    @Override\\n    public Object getTokenValue() {\\n        return token;\\n    }\\n\\n    @Override\\n    public double size(Token next) {\\n        LongToken n = (LongToken) next;\\n        // Overflow acceptable and desired.\\n        long v = n.token - token;\\n        // Scale so that the full range is 1.\\n        double d = Math.scalb((double) v, -Long.SIZE);\\n        // Adjust for signed long, also making sure t.size(t) == 1.\\n        return d > 0.0 ? d : (d + 1.0);\\n    }\\n\\n    @Override\\n    public Token increaseSlightly() {\\n        return new LongToken(token + 1);\\n    }\\n\\n    /**\\n     * Reverses murmur3 to find a possible 16 byte key that generates a given token\\n     */\\n    @VisibleForTesting\\n    public static ByteBuffer keyForToken(LongToken token) {\\n        ByteBuffer result = ByteBuffer.allocate(16);\\n        long[] inv = MurmurHash.inv_hash3_x64_128(new long[] { token.token, 0L });\\n        result.putLong(inv[0]).putLong(inv[1]).position(0);\\n        return result;\\n    }\\n}\\n}\\n\\nsrc/java/org/apache/cassandra/utils/ByteBufferUtil.java (Before)\\npublic class ByteBufferUtil {\\npublic static ByteBuffer objectToBytes(Object obj) {\\n    if (obj instanceof Integer)\\n        return ByteBufferUtil.bytes((int) obj);\\n    else if (obj instanceof Byte)\\n        return ByteBufferUtil.bytes((byte) obj);\\n    else if (obj instanceof Short)\\n        return ByteBufferUtil.bytes((short) obj);\\n    else if (obj instanceof Long)\\n        return ByteBufferUtil.bytes((long) obj);\\n    else if (obj instanceof Float)\\n        return ByteBufferUtil.bytes((float) obj);\\n    else if (obj instanceof Double)\\n        return ByteBufferUtil.bytes((double) obj);\\n    else if (obj instanceof UUID)\\n        return ByteBufferUtil.bytes((UUID) obj);\\n    else if (obj instanceof InetAddress)\\n        return ByteBufferUtil.bytes((InetAddress) obj);\\n    else if (obj instanceof String)\\n        return ByteBufferUtil.bytes((String) obj);\\n    else\\n        throw new IllegalArgumentException(String.format(\"Cannot convert value %s of type %s\", obj, obj.getClass()));\\n}\\n}\\n\\nsrc/java/org/apache/cassandra/utils/ByteBufferUtil.java (After)\\npublic class ByteBufferUtil {\\npublic static ByteBuffer objectToBytes(Object obj) {\\n    if (obj instanceof Integer)\\n        return ByteBufferUtil.bytes((int) obj);\\n    else if (obj instanceof Byte)\\n        return ByteBufferUtil.bytes((byte) obj);\\n    else if (obj instanceof Short)\\n        return ByteBufferUtil.bytes((short) obj);\\n    else if (obj instanceof Long)\\n        return ByteBufferUtil.bytes((long) obj);\\n    else if (obj instanceof Float)\\n        return ByteBufferUtil.bytes((float) obj);\\n    else if (obj instanceof Double)\\n        return ByteBufferUtil.bytes((double) obj);\\n    else if (obj instanceof UUID)\\n        return ByteBufferUtil.bytes((UUID) obj);\\n    else if (obj instanceof InetAddress)\\n        return ByteBufferUtil.bytes((InetAddress) obj);\\n    else if (obj instanceof String)\\n        return ByteBufferUtil.bytes((String) obj);\\n    else if (obj instanceof ByteBuffer)\\n        return (ByteBuffer) obj;\\n    else\\n        throw new IllegalArgumentException(String.format(\"Cannot convert value %s of type %s\", obj, obj.getClass()));\\n}\\n}\\n\\nsrc/java/org/apache/cassandra/utils/MurmurHash.java (Before)\\npackage org.apache.cassandra.utils;\\n\\nimport java.nio.ByteBuffer;\\npublic class MurmurHash {\\npublic static long hash2_64(ByteBuffer key, int offset, int length, long seed) {\\n    long m64 = 0xc6a4a7935bd1e995L;\\n    int r64 = 47;\\n    long h64 = (seed & 0xffffffffL) ^ (m64 * length);\\n    int lenLongs = length >> 3;\\n    for (int i = 0; i < lenLongs; ++i) {\\n        int i_8 = i << 3;\\n        long k64 = ((long) key.get(offset + i_8 + 0) & 0xff) + (((long) key.get(offset + i_8 + 1) & 0xff) << 8) + (((long) key.get(offset + i_8 + 2) & 0xff) << 16) + (((long) key.get(offset + i_8 + 3) & 0xff) << 24) + (((long) key.get(offset + i_8 + 4) & 0xff) << 32) + (((long) key.get(offset + i_8 + 5) & 0xff) << 40) + (((long) key.get(offset + i_8 + 6) & 0xff) << 48) + (((long) key.get(offset + i_8 + 7) & 0xff) << 56);\\n        k64 *= m64;\\n        k64 ^= k64 >>> r64;\\n        k64 *= m64;\\n        h64 ^= k64;\\n        h64 *= m64;\\n    }\\n    int rem = length & 0x7;\\n    switch(rem) {\\n        case 0:\\n            break;\\n        case 7:\\n            h64 ^= (long) key.get(offset + length - rem + 6) << 48;\\n        case 6:\\n            h64 ^= (long) key.get(offset + length - rem + 5) << 40;\\n        case 5:\\n            h64 ^= (long) key.get(offset + length - rem + 4) << 32;\\n        case 4:\\n            h64 ^= (long) key.get(offset + length - rem + 3) << 24;\\n        case 3:\\n            h64 ^= (long) key.get(offset + length - rem + 2) << 16;\\n        case 2:\\n            h64 ^= (long) key.get(offset + length - rem + 1) << 8;\\n        case 1:\\n            h64 ^= (long) key.get(offset + length - rem);\\n            h64 *= m64;\\n    }\\n    h64 ^= h64 >>> r64;\\n    h64 *= m64;\\n    h64 ^= h64 >>> r64;\\n    return h64;\\n}\\nprotected static long getblock(ByteBuffer key, int offset, int index) {\\n    int i_8 = index << 3;\\n    int blockOffset = offset + i_8;\\n    return ((long) key.get(blockOffset + 0) & 0xff) + (((long) key.get(blockOffset + 1) & 0xff) << 8) + (((long) key.get(blockOffset + 2) & 0xff) << 16) + (((long) key.get(blockOffset + 3) & 0xff) << 24) + (((long) key.get(blockOffset + 4) & 0xff) << 32) + (((long) key.get(blockOffset + 5) & 0xff) << 40) + (((long) key.get(blockOffset + 6) & 0xff) << 48) + (((long) key.get(blockOffset + 7) & 0xff) << 56);\\n}\\npublic static void hash3_x64_128(ByteBuffer key, int offset, int length, long seed, long[] result) {\\n    // Process as 128-bit blocks.\\n    final int nblocks = length >> 4;\\n    long h1 = seed;\\n    long h2 = seed;\\n    long c1 = 0x87c37b91114253d5L;\\n    long c2 = 0x4cf5ad432745937fL;\\n    //----------\\n    // body\\n    for (int i = 0; i < nblocks; i++) {\\n        long k1 = getblock(key, offset, i * 2 + 0);\\n        long k2 = getblock(key, offset, i * 2 + 1);\\n        k1 *= c1;\\n        k1 = rotl64(k1, 31);\\n        k1 *= c2;\\n        h1 ^= k1;\\n        h1 = rotl64(h1, 27);\\n        h1 += h2;\\n        h1 = h1 * 5 + 0x52dce729;\\n        k2 *= c2;\\n        k2 = rotl64(k2, 33);\\n        k2 *= c1;\\n        h2 ^= k2;\\n        h2 = rotl64(h2, 31);\\n        h2 += h1;\\n        h2 = h2 * 5 + 0x38495ab5;\\n    }\\n    //----------\\n    // tail\\n    // Advance offset to the unprocessed tail of the data.\\n    offset += nblocks * 16;\\n    long k1 = 0;\\n    long k2 = 0;\\n    switch(length & 15) {\\n        case 15:\\n            k2 ^= ((long) key.get(offset + 14)) << 48;\\n        case 14:\\n            k2 ^= ((long) key.get(offset + 13)) << 40;\\n        case 13:\\n            k2 ^= ((long) key.get(offset + 12)) << 32;\\n        case 12:\\n            k2 ^= ((long) key.get(offset + 11)) << 24;\\n        case 11:\\n            k2 ^= ((long) key.get(offset + 10)) << 16;\\n        case 10:\\n            k2 ^= ((long) key.get(offset + 9)) << 8;\\n        case 9:\\n            k2 ^= ((long) key.get(offset + 8)) << 0;\\n            k2 *= c2;\\n            k2 = rotl64(k2, 33);\\n            k2 *= c1;\\n            h2 ^= k2;\\n        case 8:\\n            k1 ^= ((long) key.get(offset + 7)) << 56;\\n        case 7:\\n            k1 ^= ((long) key.get(offset + 6)) << 48;\\n        case 6:\\n            k1 ^= ((long) key.get(offset + 5)) << 40;\\n        case 5:\\n            k1 ^= ((long) key.get(offset + 4)) << 32;\\n        case 4:\\n            k1 ^= ((long) key.get(offset + 3)) << 24;\\n        case 3:\\n            k1 ^= ((long) key.get(offset + 2)) << 16;\\n        case 2:\\n            k1 ^= ((long) key.get(offset + 1)) << 8;\\n        case 1:\\n            k1 ^= ((long) key.get(offset));\\n            k1 *= c1;\\n            k1 = rotl64(k1, 31);\\n            k1 *= c2;\\n            h1 ^= k1;\\n    }\\n    ;\\n    //----------\\n    // finalization\\n    h1 ^= length;\\n    h2 ^= length;\\n    h1 += h2;\\n    h2 += h1;\\n    h1 = fmix(h1);\\n    h2 = fmix(h2);\\n    h1 += h2;\\n    h2 += h1;\\n    result[0] = h1;\\n    result[1] = h2;\\n}\\n}\\n\\nsrc/java/org/apache/cassandra/utils/MurmurHash.java (After)\\npackage org.apache.cassandra.utils;\\n\\nimport java.nio.ByteBuffer;\\nimport java.util.BitSet;\\nimport com.google.common.primitives.Longs;\\npublic class MurmurHash {\\npublic static long hash2_64(ByteBuffer key, int offset, int length, long seed) {\\n    long m64 = 0xc6a4a7935bd1e995L;\\n    int r64 = 47;\\n    long h64 = (seed & 0xffffffffL) ^ (m64 * length);\\n    int lenLongs = length >> 3;\\n    for (int i = 0; i < lenLongs; ++i) {\\n        int i_8 = i << 3;\\n        long k64 = ((long) key.get(offset + i_8 + 0) & 0xff) + (((long) key.get(offset + i_8 + 1) & 0xff) << 8) + (((long) key.get(offset + i_8 + 2) & 0xff) << 16) + (((long) key.get(offset + i_8 + 3) & 0xff) << 24) + (((long) key.get(offset + i_8 + 4) & 0xff) << 32) + (((long) key.get(offset + i_8 + 5) & 0xff) << 40) + (((long) key.get(offset + i_8 + 6) & 0xff) << 48) + (((long) key.get(offset + i_8 + 7) & 0xff) << 56);\\n        k64 *= m64;\\n        k64 ^= k64 >>> r64;\\n        k64 *= m64;\\n        h64 ^= k64;\\n        h64 *= m64;\\n    }\\n    int rem = length & 0x7;\\n    switch(rem) {\\n        case 0:\\n            break;\\n        case 7:\\n            h64 ^= (long) key.get(offset + length - rem + 6) << 48;\\n        case 6:\\n            h64 ^= (long) key.get(offset + length - rem + 5) << 40;\\n        case 5:\\n            h64 ^= (long) key.get(offset + length - rem + 4) << 32;\\n        case 4:\\n            h64 ^= (long) key.get(offset + length - rem + 3) << 24;\\n        case 3:\\n            h64 ^= (long) key.get(offset + length - rem + 2) << 16;\\n        case 2:\\n            h64 ^= (long) key.get(offset + length - rem + 1) << 8;\\n        case 1:\\n            h64 ^= (long) key.get(offset + length - rem);\\n            h64 *= m64;\\n    }\\n    h64 ^= h64 >>> r64;\\n    h64 *= m64;\\n    h64 ^= h64 >>> r64;\\n    return h64;\\n}\\nprotected static long getBlock(ByteBuffer key, int offset, int index) {\\n    int i_8 = index << 3;\\n    int blockOffset = offset + i_8;\\n    return ((long) key.get(blockOffset + 0) & 0xff) + (((long) key.get(blockOffset + 1) & 0xff) << 8) + (((long) key.get(blockOffset + 2) & 0xff) << 16) + (((long) key.get(blockOffset + 3) & 0xff) << 24) + (((long) key.get(blockOffset + 4) & 0xff) << 32) + (((long) key.get(blockOffset + 5) & 0xff) << 40) + (((long) key.get(blockOffset + 6) & 0xff) << 48) + (((long) key.get(blockOffset + 7) & 0xff) << 56);\\n}\\npublic static void hash3_x64_128(ByteBuffer key, int offset, int length, long seed, long[] result) {\\n    // Process as 128-bit blocks.\\n    final int nblocks = length >> 4;\\n    long h1 = seed;\\n    long h2 = seed;\\n    long c1 = 0x87c37b91114253d5L;\\n    long c2 = 0x4cf5ad432745937fL;\\n    //----------\\n    // body\\n    for (int i = 0; i < nblocks; i++) {\\n        long k1 = getBlock(key, offset, i * 2 + 0);\\n        long k2 = getBlock(key, offset, i * 2 + 1);\\n        k1 *= c1;\\n        k1 = rotl64(k1, 31);\\n        k1 *= c2;\\n        h1 ^= k1;\\n        h1 = rotl64(h1, 27);\\n        h1 += h2;\\n        h1 = h1 * 5 + 0x52dce729;\\n        k2 *= c2;\\n        k2 = rotl64(k2, 33);\\n        k2 *= c1;\\n        h2 ^= k2;\\n        h2 = rotl64(h2, 31);\\n        h2 += h1;\\n        h2 = h2 * 5 + 0x38495ab5;\\n    }\\n    //----------\\n    // tail\\n    // Advance offset to the unprocessed tail of the data.\\n    offset += nblocks * 16;\\n    long k1 = 0;\\n    long k2 = 0;\\n    switch(length & 15) {\\n        case 15:\\n            k2 ^= ((long) key.get(offset + 14)) << 48;\\n        case 14:\\n            k2 ^= ((long) key.get(offset + 13)) << 40;\\n        case 13:\\n            k2 ^= ((long) key.get(offset + 12)) << 32;\\n        case 12:\\n            k2 ^= ((long) key.get(offset + 11)) << 24;\\n        case 11:\\n            k2 ^= ((long) key.get(offset + 10)) << 16;\\n        case 10:\\n            k2 ^= ((long) key.get(offset + 9)) << 8;\\n        case 9:\\n            k2 ^= ((long) key.get(offset + 8)) << 0;\\n            k2 *= c2;\\n            k2 = rotl64(k2, 33);\\n            k2 *= c1;\\n            h2 ^= k2;\\n        case 8:\\n            k1 ^= ((long) key.get(offset + 7)) << 56;\\n        case 7:\\n            k1 ^= ((long) key.get(offset + 6)) << 48;\\n        case 6:\\n            k1 ^= ((long) key.get(offset + 5)) << 40;\\n        case 5:\\n            k1 ^= ((long) key.get(offset + 4)) << 32;\\n        case 4:\\n            k1 ^= ((long) key.get(offset + 3)) << 24;\\n        case 3:\\n            k1 ^= ((long) key.get(offset + 2)) << 16;\\n        case 2:\\n            k1 ^= ((long) key.get(offset + 1)) << 8;\\n        case 1:\\n            k1 ^= ((long) key.get(offset));\\n            k1 *= c1;\\n            k1 = rotl64(k1, 31);\\n            k1 *= c2;\\n            h1 ^= k1;\\n    }\\n    ;\\n    //----------\\n    // finalization\\n    h1 ^= length;\\n    h2 ^= length;\\n    h1 += h2;\\n    h2 += h1;\\n    h1 = fmix(h1);\\n    h2 = fmix(h2);\\n    h1 += h2;\\n    h2 += h1;\\n    result[0] = h1;\\n    result[1] = h2;\\n}\\nprotected static long invRotl64(long v, int n) {\\n    return ((v >>> n) | (v << (64 - n)));\\n}\\nprotected static long invRShiftXor(long value, int shift) {\\n    long output = 0;\\n    long i = 0;\\n    while (i * shift < 64) {\\n        long c = (0xffffffffffffffffL << (64 - shift)) >>> (shift * i);\\n        long partOutput = value & c;\\n        value ^= partOutput >>> shift;\\n        output |= partOutput;\\n        i += 1;\\n    }\\n    return output;\\n}\\nprotected static long invFmix(long k) {\\n    k = invRShiftXor(k, 33);\\n    k *= 0x9cb4b2f8129337dbL;\\n    k = invRShiftXor(k, 33);\\n    k *= 0x4f74430c22a54005L;\\n    k = invRShiftXor(k, 33);\\n    return k;\\n}\\n/**\\n * This gives a correct reversal of the tail byte flip which is needed if want a non mod16==0 byte hash inv or to\\n * target a hash for a given schema.\\n */\\npublic static long invTailReverse(long num) {\\n    byte[] v = Longs.toByteArray(Long.reverseBytes(num));\\n    for (int i = 0; i < 8; i++) {\\n        if (v[i] < 0 && i < 7) {\\n            BitSet bits = BitSet.valueOf(v);\\n            bits.flip(8 * (i + 1), 64);\\n            v = bits.toByteArray();\\n        }\\n    }\\n    return Longs.fromByteArray(v);\\n}\\npublic static long[] inv_hash3_x64_128(long[] result) {\\n    long c1 = 0xa98409e882ce4d7dL;\\n    long c2 = 0xa81e14edd9de2c7fL;\\n    long k1 = 0;\\n    long k2 = 0;\\n    long h1 = result[0];\\n    long h2 = result[1];\\n    //----------\\n    // reverse finalization\\n    h2 -= h1;\\n    h1 -= h2;\\n    h1 = invFmix(h1);\\n    h2 = invFmix(h2);\\n    h2 -= h1;\\n    h1 -= h2;\\n    h1 ^= 16;\\n    h2 ^= 16;\\n    //----------\\n    // reverse body\\n    h2 -= 0x38495ab5;\\n    h2 *= 0xcccccccccccccccdL;\\n    h2 -= h1;\\n    h2 = invRotl64(h2, 31);\\n    k2 = h2;\\n    h2 = 0;\\n    k2 *= c1;\\n    k2 = invRotl64(k2, 33);\\n    k2 *= c2;\\n    h1 -= 0x52dce729;\\n    h1 *= 0xcccccccccccccccdL;\\n    //h1 -= h2;\\n    h1 = invRotl64(h1, 27);\\n    k1 = h1;\\n    k1 *= c2;\\n    k1 = invRotl64(k1, 31);\\n    k1 *= c1;\\n    // note that while this works for body block reversing the tail reverse requires `invTailReverse`\\n    k1 = Long.reverseBytes(k1);\\n    k2 = Long.reverseBytes(k2);\\n    return new long[] { k1, k2 };\\n}\\n}\\n\\n\\nSummary:', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Given a Git diff and the relevant source code, write a concise summary of the code changes in a way that a non-technical person can understand. Summarize in maximum three concise sentences. \\n\\nAvoid adding any additional comments or annotations to the summary.\\n\\nGit diff:\\ndiff --git a/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java b/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java\\nindex 6b96c07a966..aa502a2b47b 100644\\n--- a/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java\\n+++ b/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java\\n@@ -349,7 +349,9 @@ public class BeanExpression implements Expression, Predicate {\\n     private static Object invokeBean(BeanHolder beanHolder, String beanName, String methodName, Exchange exchange) {\\n         Object result;\\n \\n-        try (BeanExpressionProcessor processor = new BeanExpressionProcessor(beanHolder)) {\\n+        try {\\n+            // do not close BeanExpressionProcessor as beanHolder should not be closed\\n+            BeanExpressionProcessor processor = new BeanExpressionProcessor(beanHolder);\\n \\n             if (methodName != null) {\\n                 processor.setMethod(methodName);\\ndiff --git a/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java b/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java\\nindex fc076c25d4b..4cc2fc9c335 100644\\n--- a/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java\\n+++ b/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java\\n@@ -692,12 +692,12 @@ public class DefaultRoute extends ServiceSupport implements Route {\\n             services.add(service);\\n         }\\n         for (Processor p : onCompletions.values()) {\\n-            if (processor instanceof Service service) {\\n+            if (p instanceof Service service) {\\n                 services.add(service);\\n             }\\n         }\\n         for (Processor p : onExceptions.values()) {\\n-            if (processor instanceof Service service) {\\n+            if (p instanceof Service service) {\\n                 services.add(service);\\n             }\\n         }\\ndiff --git a/core/camel-core/src/test/java/org/apache/camel/processor/SupervisingRouteControllerSplitOnExceptionTest.java b/core/camel-core/src/test/java/org/apache/camel/processor/SupervisingRouteControllerSplitOnExceptionTest.java\\nnew file mode 100644\\nindex 00000000000..710f56f00d4\\n--- /dev/null\\n+++ b/core/camel-core/src/test/java/org/apache/camel/processor/SupervisingRouteControllerSplitOnExceptionTest.java\\n@@ -0,0 +1,87 @@\\n+/*\\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\\n+ * contributor license agreements.  See the NOTICE file distributed with\\n+ * this work for additional information regarding copyright ownership.\\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\\n+ * (the \"License\"); you may not use this file except in compliance with\\n+ * the License.  You may obtain a copy of the License at\\n+ *\\n+ *      http://www.apache.org/licenses/LICENSE-2.0\\n+ *\\n+ * Unless required by applicable law or agreed to in writing, software\\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+ * See the License for the specific language governing permissions and\\n+ * limitations under the License.\\n+ */\\n+package org.apache.camel.processor;\\n+\\n+import java.util.ArrayList;\\n+import java.util.List;\\n+\\n+import org.apache.camel.Body;\\n+import org.apache.camel.CamelContext;\\n+import org.apache.camel.ContextTestSupport;\\n+import org.apache.camel.Message;\\n+import org.apache.camel.RoutesBuilder;\\n+import org.apache.camel.builder.RouteBuilder;\\n+import org.apache.camel.spi.SupervisingRouteController;\\n+import org.junit.jupiter.api.Test;\\n+\\n+public class SupervisingRouteControllerSplitOnExceptionTest extends ContextTestSupport {\\n+\\n+    @Override\\n+    protected CamelContext createCamelContext() throws Exception {\\n+        CamelContext context = super.createCamelContext();\\n+\\n+        SupervisingRouteController src = context.getRouteController().supervising();\\n+        src.setBackOffDelay(25);\\n+        src.setBackOffMaxAttempts(3);\\n+        src.setInitialDelay(100);\\n+        src.setThreadPoolSize(1);\\n+\\n+        return context;\\n+    }\\n+\\n+    @Test\\n+    public void testSupervising() throws Exception {\\n+        getMockEndpoint(\"mock:error\").expectedMessageCount(1);\\n+        getMockEndpoint(\"mock:uk\").expectedMessageCount(0);\\n+        getMockEndpoint(\"mock:other\").expectedMessageCount(0);\\n+\\n+        template.sendBody(\"direct:start\", \"<hello>World\");\\n+\\n+        assertMockEndpointsSatisfied();\\n+    }\\n+\\n+    @Override\\n+    protected RoutesBuilder createRouteBuilder() throws Exception {\\n+        return new RouteBuilder() {\\n+            @Override\\n+            public void configure() throws Exception {\\n+                onException().handled(true).split().method(SupervisingRouteControllerSplitOnExceptionTest.class, \"mySplit\")\\n+                        .streaming().log(\"Exception occurred\").to(\"mock:error\");\\n+\\n+                from(\"direct:start\")\\n+                        .choice()\\n+                        .when(xpath(\"/person/city = \\'London\\'\"))\\n+                        .log(\"UK message\")\\n+                        .to(\"mock:uk\")\\n+                        .otherwise()\\n+                        .log(\"Other message\")\\n+                        .to(\"mock:other\");\\n+            }\\n+        };\\n+    }\\n+\\n+    public static List<Message> mySplit(@Body Message inputMessage) {\\n+        List<Message> outputMessages = new ArrayList<>();\\n+\\n+        Message outputMessage = inputMessage.copy();\\n+        outputMessage.setBody(inputMessage.getBody());\\n+        outputMessages.add(outputMessage);\\n+\\n+        return outputMessages;\\n+    }\\n+\\n+}\\n\\n\\nSource code:\\ncomponents/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java (Before)\\npublic class BeanExpression implements Expression, Predicate {\\n/**\\n * Invokes the bean and returns the result. If an exception was thrown while invoking the bean, then the exception\\n * is set on the exchange.\\n */\\nprivate static Object invokeBean(BeanHolder beanHolder, String beanName, String methodName, Exchange exchange) {\\n    Object result;\\n    try (BeanExpressionProcessor processor = new BeanExpressionProcessor(beanHolder)) {\\n        if (methodName != null) {\\n            processor.setMethod(methodName);\\n            // enable OGNL like invocation\\n            processor.setShorthandMethod(true);\\n        }\\n        // copy the original exchange to avoid side effects on it\\n        Exchange resultExchange = ExchangeHelper.createCopy(exchange, true);\\n        // remove any existing exception in case we do OGNL on the exception\\n        resultExchange.setException(null);\\n        // force to use InOut to retrieve the result on the OUT message\\n        resultExchange.setPattern(ExchangePattern.InOut);\\n        processor.process(resultExchange);\\n        // the response is always stored in OUT\\n        result = resultExchange.hasOut() ? resultExchange.getOut().getBody() : null;\\n        // propagate properties and headers from result\\n        if (resultExchange.hasProperties()) {\\n            exchange.getProperties().putAll(resultExchange.getProperties());\\n        }\\n        if (resultExchange.hasOut() && resultExchange.getOut().hasHeaders()) {\\n            exchange.getIn().getHeaders().putAll(resultExchange.getOut().getHeaders());\\n        }\\n        // propagate exceptions\\n        if (resultExchange.getException() != null) {\\n            exchange.setException(resultExchange.getException());\\n        }\\n    } catch (Exception e) {\\n        throw new RuntimeBeanExpressionException(exchange, beanName, methodName, e);\\n    }\\n    return result;\\n}\\n}\\n\\ncomponents/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java (After)\\npublic class BeanExpression implements Expression, Predicate {\\n/**\\n * Invokes the bean and returns the result. If an exception was thrown while invoking the bean, then the exception\\n * is set on the exchange.\\n */\\nprivate static Object invokeBean(BeanHolder beanHolder, String beanName, String methodName, Exchange exchange) {\\n    Object result;\\n    try {\\n        // do not close BeanExpressionProcessor as beanHolder should not be closed\\n        BeanExpressionProcessor processor = new BeanExpressionProcessor(beanHolder);\\n        if (methodName != null) {\\n            processor.setMethod(methodName);\\n            // enable OGNL like invocation\\n            processor.setShorthandMethod(true);\\n        }\\n        // copy the original exchange to avoid side effects on it\\n        Exchange resultExchange = ExchangeHelper.createCopy(exchange, true);\\n        // remove any existing exception in case we do OGNL on the exception\\n        resultExchange.setException(null);\\n        // force to use InOut to retrieve the result on the OUT message\\n        resultExchange.setPattern(ExchangePattern.InOut);\\n        processor.process(resultExchange);\\n        // the response is always stored in OUT\\n        result = resultExchange.hasOut() ? resultExchange.getOut().getBody() : null;\\n        // propagate properties and headers from result\\n        if (resultExchange.hasProperties()) {\\n            exchange.getProperties().putAll(resultExchange.getProperties());\\n        }\\n        if (resultExchange.hasOut() && resultExchange.getOut().hasHeaders()) {\\n            exchange.getIn().getHeaders().putAll(resultExchange.getOut().getHeaders());\\n        }\\n        // propagate exceptions\\n        if (resultExchange.getException() != null) {\\n            exchange.setException(resultExchange.getException());\\n        }\\n    } catch (Exception e) {\\n        throw new RuntimeBeanExpressionException(exchange, beanName, methodName, e);\\n    }\\n    return result;\\n}\\n}\\n\\ncore/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java (Before)\\npublic class DefaultRoute extends ServiceSupport implements Route {\\nprivate void gatherRootServices(List<Service> services) throws Exception {\\n    Endpoint endpoint = getEndpoint();\\n    consumer = endpoint.createConsumer(processor);\\n    if (consumer != null) {\\n        services.add(consumer);\\n        if (consumer instanceof RouteAware routeAware) {\\n            routeAware.setRoute(this);\\n        }\\n        if (consumer instanceof RouteIdAware routeIdAware) {\\n            routeIdAware.setRouteId(this.getId());\\n        }\\n        if (consumer instanceof ResumeAware resumeAware && resumeStrategy != null) {\\n            ResumeAdapter resumeAdapter = AdapterHelper.eval(getCamelContext(), resumeAware, resumeStrategy);\\n            resumeStrategy.setAdapter(resumeAdapter);\\n            resumeAware.setResumeStrategy(resumeStrategy);\\n        }\\n        if (consumer instanceof ConsumerListenerAware consumerListenerAware) {\\n            consumerListenerAware.setConsumerListener(consumerListener);\\n        }\\n    }\\n    if (processor instanceof Service service) {\\n        services.add(service);\\n    }\\n    for (Processor p : onCompletions.values()) {\\n        if (processor instanceof Service service) {\\n            services.add(service);\\n        }\\n    }\\n    for (Processor p : onExceptions.values()) {\\n        if (processor instanceof Service service) {\\n            services.add(service);\\n        }\\n    }\\n}\\n}\\n\\ncore/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java (After)\\npublic class DefaultRoute extends ServiceSupport implements Route {\\nprivate void gatherRootServices(List<Service> services) throws Exception {\\n    Endpoint endpoint = getEndpoint();\\n    consumer = endpoint.createConsumer(processor);\\n    if (consumer != null) {\\n        services.add(consumer);\\n        if (consumer instanceof RouteAware routeAware) {\\n            routeAware.setRoute(this);\\n        }\\n        if (consumer instanceof RouteIdAware routeIdAware) {\\n            routeIdAware.setRouteId(this.getId());\\n        }\\n        if (consumer instanceof ResumeAware resumeAware && resumeStrategy != null) {\\n            ResumeAdapter resumeAdapter = AdapterHelper.eval(getCamelContext(), resumeAware, resumeStrategy);\\n            resumeStrategy.setAdapter(resumeAdapter);\\n            resumeAware.setResumeStrategy(resumeStrategy);\\n        }\\n        if (consumer instanceof ConsumerListenerAware consumerListenerAware) {\\n            consumerListenerAware.setConsumerListener(consumerListener);\\n        }\\n    }\\n    if (processor instanceof Service service) {\\n        services.add(service);\\n    }\\n    for (Processor p : onCompletions.values()) {\\n        if (p instanceof Service service) {\\n            services.add(service);\\n        }\\n    }\\n    for (Processor p : onExceptions.values()) {\\n        if (p instanceof Service service) {\\n            services.add(service);\\n        }\\n    }\\n}\\n}\\n\\ncore/camel-core/src/test/java/org/apache/camel/processor/SupervisingRouteControllerSplitOnExceptionTest.java (After)\\npackage org.apache.camel.processor;\\n\\nimport java.util.ArrayList;\\nimport java.util.List;\\nimport org.apache.camel.Body;\\nimport org.apache.camel.CamelContext;\\nimport org.apache.camel.ContextTestSupport;\\nimport org.apache.camel.Message;\\nimport org.apache.camel.RoutesBuilder;\\nimport org.apache.camel.builder.RouteBuilder;\\nimport org.apache.camel.spi.SupervisingRouteController;\\nimport org.junit.jupiter.api.Test;\\npublic class SupervisingRouteControllerSplitOnExceptionTest extends ContextTestSupport {\\n@Override\\nprotected CamelContext createCamelContext() throws Exception {\\n    CamelContext context = super.createCamelContext();\\n    SupervisingRouteController src = context.getRouteController().supervising();\\n    src.setBackOffDelay(25);\\n    src.setBackOffMaxAttempts(3);\\n    src.setInitialDelay(100);\\n    src.setThreadPoolSize(1);\\n    return context;\\n}\\n@Test\\npublic void testSupervising() throws Exception {\\n    getMockEndpoint(\"mock:error\").expectedMessageCount(1);\\n    getMockEndpoint(\"mock:uk\").expectedMessageCount(0);\\n    getMockEndpoint(\"mock:other\").expectedMessageCount(0);\\n    template.sendBody(\"direct:start\", \"<hello>World\");\\n    assertMockEndpointsSatisfied();\\n}\\n@Override\\nprotected RoutesBuilder createRouteBuilder() throws Exception {\\n    return new RouteBuilder() {\\n\\n        @Override\\n        public void configure() throws Exception {\\n            onException().handled(true).split().method(SupervisingRouteControllerSplitOnExceptionTest.class, \"mySplit\").streaming().log(\"Exception occurred\").to(\"mock:error\");\\n            from(\"direct:start\").choice().when(xpath(\"/person/city = \\'London\\'\")).log(\"UK message\").to(\"mock:uk\").otherwise().log(\"Other message\").to(\"mock:other\");\\n        }\\n    };\\n}\\npublic static List<Message> mySplit(@Body Message inputMessage) {\\n    List<Message> outputMessages = new ArrayList<>();\\n    Message outputMessage = inputMessage.copy();\\n    outputMessage.setBody(inputMessage.getBody());\\n    outputMessages.add(outputMessage);\\n    return outputMessages;\\n}\\n}\\n\\n\\nSummary:', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Given a Git diff and the relevant source code, write a concise summary of the code changes in a way that a non-technical person can understand. Summarize in maximum three concise sentences. \\n\\nAvoid adding any additional comments or annotations to the summary.\\n\\nGit diff:\\ndiff --git a/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java b/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java\\nindex 2ed439d375..f76a4d15ea 100644\\n--- a/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java\\n+++ b/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java\\n@@ -42,7 +42,7 @@\\n  * <li>unlimited number of version components,</li>\\n  * <li>version components in the text can be digits or strings,</li>\\n  * <li>strings are checked for well-known qualifiers and the qualifier ordering is used for version ordering.\\n- *     Well-known qualifiers (case insensitive) are:<ul>\\n+ *     Well-known qualifiers (case-insensitive) are:<ul>\\n  *     <li><code>alpha</code> or <code>a</code></li>\\n  *     <li><code>beta</code> or <code>b</code></li>\\n  *     <li><code>milestone</code> or <code>m</code></li>\\n@@ -51,9 +51,9 @@\\n  *     <li><code>(the empty string)</code> or <code>ga</code> or <code>final</code></li>\\n  *     <li><code>sp</code></li>\\n  *     </ul>\\n- *     Unknown qualifiers are considered after known qualifiers, with lexical order (always case insensitive),\\n+ *     Unknown qualifiers are considered after known qualifiers, with lexical order (always case-insensitive),\\n  *   </li>\\n- * <li>a hyphen usually precedes a qualifier, and is always less important than digits/number, for example\\n+ * <li>A hyphen usually precedes a qualifier, and is always less important than digits/number. For example\\n  *   {@code 1.0.RC2 < 1.0-RC3 < 1.0.1}; but prefer {@code 1.0.0-RC1} over {@code 1.0.0.RC1}, and more\\n  *   generally: {@code 1.0.X2 < 1.0-X3 < 1.0.1} for any string {@code X}; but prefer {@code 1.0.0-X1}\\n  *   over {@code 1.0.0.X1}.</li>\\n@@ -656,7 +656,20 @@ public final void parseVersion(String version) {\\n         int startIndex = 0;\\n \\n         for (int i = 0; i < version.length(); i++) {\\n-            char c = version.charAt(i);\\n+            char character = version.charAt(i);\\n+            int c = character;\\n+            if (Character.isHighSurrogate(character)) {\\n+                // read the next character as a low surrogate and combine into a single int\\n+                try {\\n+                    char low = version.charAt(i + 1);\\n+                    char[] both = {character, low};\\n+                    c = Character.codePointAt(both, 0);\\n+                    i++;\\n+                } catch (IndexOutOfBoundsException ex) {\\n+                    // high surrogate without low surrogate. Not a lot we can do here except treat it as a regular\\n+                    // character\\n+                }\\n+            }\\n \\n             if (c == \\'.\\') {\\n                 if (i == startIndex) {\\n@@ -687,7 +700,7 @@ public final void parseVersion(String version) {\\n                     stack.push(list);\\n                 }\\n                 isCombination = false;\\n-            } else if (Character.isDigit(c)) {\\n+            } else if (c >= \\'0\\' && c <= \\'9\\') { // Check for ASCII digits only\\n                 if (!isDigit && i > startIndex) {\\n                     // X1\\n                     isCombination = true;\\ndiff --git a/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java b/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java\\nindex d7616405bd..219d760bab 100644\\n--- a/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java\\n+++ b/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java\\n@@ -27,7 +27,6 @@\\n \\n /**\\n  * Test ComparableVersion.\\n- *\\n  */\\n @SuppressWarnings(\"unchecked\")\\n class ComparableVersionTest {\\n@@ -222,6 +221,23 @@ void testLeadingZeroes() {\\n         checkVersionsOrder(\"0.2\", \"1.0.7\");\\n     }\\n \\n+    @Test\\n+    void testDigitGreaterThanNonAscii() {\\n+        ComparableVersion c1 = new ComparableVersion(\"1\");\\n+        ComparableVersion c2 = new ComparableVersion(\"é\");\\n+        assertTrue(c1.compareTo(c2) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\uD835\\\\uDFE4\");\\n+        assertTrue(c2.compareTo(c1) < 0, \"expected \" + \"\\\\uD835\\\\uDFE4\" + \" < \" + \"1\");\\n+    }\\n+\\n+    @Test\\n+    void testDigitGreaterThanNonBmpCharacters() {\\n+        ComparableVersion c1 = new ComparableVersion(\"1\");\\n+        // MATHEMATICAL SANS-SERIF DIGIT TWO\\n+        ComparableVersion c2 = new ComparableVersion(\"\\\\uD835\\\\uDFE4\");\\n+        assertTrue(c1.compareTo(c2) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\uD835\\\\uDFE4\");\\n+        assertTrue(c2.compareTo(c1) < 0, \"expected \" + \"\\\\uD835\\\\uDFE4\" + \" < \" + \"1\");\\n+    }\\n+\\n     @Test\\n     void testGetCanonical() {\\n         // MNG-7700\\n@@ -238,13 +254,25 @@ void testGetCanonical() {\\n \\n     @Test\\n     void testCompareDigitToLetter() {\\n-        ComparableVersion c1 = new ComparableVersion(\"7\");\\n-        ComparableVersion c2 = new ComparableVersion(\"J\");\\n-        ComparableVersion c3 = new ComparableVersion(\"c\");\\n-        assertTrue(c1.compareTo(c2) > 0, \"expected 7 > J\");\\n-        assertTrue(c2.compareTo(c1) < 0, \"expected J < 1\");\\n-        assertTrue(c1.compareTo(c3) > 0, \"expected 7 > c\");\\n-        assertTrue(c3.compareTo(c1) < 0, \"expected c < 7\");\\n+        ComparableVersion seven = new ComparableVersion(\"7\");\\n+        ComparableVersion capitalJ = new ComparableVersion(\"J\");\\n+        ComparableVersion lowerCaseC = new ComparableVersion(\"c\");\\n+        // Digits are greater than letters\\n+        assertTrue(seven.compareTo(capitalJ) > 0, \"expected 7 > J\");\\n+        assertTrue(capitalJ.compareTo(seven) < 0, \"expected J < 1\");\\n+        assertTrue(seven.compareTo(lowerCaseC) > 0, \"expected 7 > c\");\\n+        assertTrue(lowerCaseC.compareTo(seven) < 0, \"expected c < 7\");\\n+    }\\n+\\n+    @Test\\n+    void testNonAsciiDigits() { // These should not be treated as digits.\\n+        ComparableVersion asciiOne = new ComparableVersion(\"1\");\\n+        ComparableVersion arabicEight = new ComparableVersion(\"\\\\u0668\");\\n+        ComparableVersion asciiNine = new ComparableVersion(\"9\");\\n+        assertTrue(asciiOne.compareTo(arabicEight) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\u0668\");\\n+        assertTrue(arabicEight.compareTo(asciiOne) < 0, \"expected \" + \"\\\\u0668\" + \" < \" + \"1\");\\n+        assertTrue(asciiNine.compareTo(arabicEight) > 0, \"expected \" + \"9\" + \" > \" + \"\\\\u0668\");\\n+        assertTrue(arabicEight.compareTo(asciiNine) < 0, \"expected \" + \"\\\\u0668\" + \" < \" + \"9\");\\n     }\\n \\n     @Test\\n\\n\\nSource code:\\ncompat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java (Before)\\npublic class ComparableVersion implements Comparable<ComparableVersion> {\\n@SuppressWarnings(\"checkstyle:innerassignment\")\\npublic final void parseVersion(String version) {\\n    this.value = version;\\n    items = new ListItem();\\n    version = version.toLowerCase(Locale.ENGLISH);\\n    ListItem list = items;\\n    Deque<Item> stack = new ArrayDeque<>();\\n    stack.push(list);\\n    boolean isDigit = false;\\n    boolean isCombination = false;\\n    int startIndex = 0;\\n    for (int i = 0; i < version.length(); i++) {\\n        char c = version.charAt(i);\\n        if (c == \\'.\\') {\\n            if (i == startIndex) {\\n                list.add(IntItem.ZERO);\\n            } else {\\n                list.add(parseItem(isCombination, isDigit, version.substring(startIndex, i)));\\n            }\\n            isCombination = false;\\n            startIndex = i + 1;\\n        } else if (c == \\'-\\') {\\n            if (i == startIndex) {\\n                list.add(IntItem.ZERO);\\n            } else {\\n                if (!isDigit && i != version.length() - 1) {\\n                    char c1 = version.charAt(i + 1);\\n                    if (Character.isDigit(c1)) {\\n                        isCombination = true;\\n                        continue;\\n                    }\\n                }\\n                list.add(parseItem(isCombination, isDigit, version.substring(startIndex, i)));\\n            }\\n            startIndex = i + 1;\\n            if (!list.isEmpty()) {\\n                list.add(list = new ListItem());\\n                stack.push(list);\\n            }\\n            isCombination = false;\\n        } else if (Character.isDigit(c)) {\\n            if (!isDigit && i > startIndex) {\\n                isCombination = true;\\n                if (!list.isEmpty()) {\\n                    list.add(list = new ListItem());\\n                    stack.push(list);\\n                }\\n            }\\n            isDigit = true;\\n        } else {\\n            if (isDigit && i > startIndex) {\\n                list.add(parseItem(isCombination, true, version.substring(startIndex, i)));\\n                startIndex = i;\\n                list.add(list = new ListItem());\\n                stack.push(list);\\n                isCombination = false;\\n            }\\n            isDigit = false;\\n        }\\n    }\\n    if (version.length() > startIndex) {\\n        if (!isDigit && !list.isEmpty()) {\\n            list.add(list = new ListItem());\\n            stack.push(list);\\n        }\\n        list.add(parseItem(isCombination, isDigit, version.substring(startIndex)));\\n    }\\n    while (!stack.isEmpty()) {\\n        list = (ListItem) stack.pop();\\n        list.normalize();\\n    }\\n}\\n}\\n\\ncompat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java (After)\\npublic class ComparableVersion implements Comparable<ComparableVersion> {\\n@SuppressWarnings(\"checkstyle:innerassignment\")\\npublic final void parseVersion(String version) {\\n    this.value = version;\\n    items = new ListItem();\\n    version = version.toLowerCase(Locale.ENGLISH);\\n    ListItem list = items;\\n    Deque<Item> stack = new ArrayDeque<>();\\n    stack.push(list);\\n    boolean isDigit = false;\\n    boolean isCombination = false;\\n    int startIndex = 0;\\n    for (int i = 0; i < version.length(); i++) {\\n        char character = version.charAt(i);\\n        int c = character;\\n        if (Character.isHighSurrogate(character)) {\\n            try {\\n                char low = version.charAt(i + 1);\\n                char[] both = { character, low };\\n                c = Character.codePointAt(both, 0);\\n                i++;\\n            } catch (IndexOutOfBoundsException ex) {\\n            }\\n        }\\n        if (c == \\'.\\') {\\n            if (i == startIndex) {\\n                list.add(IntItem.ZERO);\\n            } else {\\n                list.add(parseItem(isCombination, isDigit, version.substring(startIndex, i)));\\n            }\\n            isCombination = false;\\n            startIndex = i + 1;\\n        } else if (c == \\'-\\') {\\n            if (i == startIndex) {\\n                list.add(IntItem.ZERO);\\n            } else {\\n                if (!isDigit && i != version.length() - 1) {\\n                    char c1 = version.charAt(i + 1);\\n                    if (Character.isDigit(c1)) {\\n                        isCombination = true;\\n                        continue;\\n                    }\\n                }\\n                list.add(parseItem(isCombination, isDigit, version.substring(startIndex, i)));\\n            }\\n            startIndex = i + 1;\\n            if (!list.isEmpty()) {\\n                list.add(list = new ListItem());\\n                stack.push(list);\\n            }\\n            isCombination = false;\\n        } else if (c >= \\'0\\' && c <= \\'9\\') {\\n            if (!isDigit && i > startIndex) {\\n                isCombination = true;\\n                if (!list.isEmpty()) {\\n                    list.add(list = new ListItem());\\n                    stack.push(list);\\n                }\\n            }\\n            isDigit = true;\\n        } else {\\n            if (isDigit && i > startIndex) {\\n                list.add(parseItem(isCombination, true, version.substring(startIndex, i)));\\n                startIndex = i;\\n                list.add(list = new ListItem());\\n                stack.push(list);\\n                isCombination = false;\\n            }\\n            isDigit = false;\\n        }\\n    }\\n    if (version.length() > startIndex) {\\n        if (!isDigit && !list.isEmpty()) {\\n            list.add(list = new ListItem());\\n            stack.push(list);\\n        }\\n        list.add(parseItem(isCombination, isDigit, version.substring(startIndex)));\\n    }\\n    while (!stack.isEmpty()) {\\n        list = (ListItem) stack.pop();\\n        list.normalize();\\n    }\\n}\\n}\\n\\ncompat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java (Before)\\nclass ComparableVersionTest {\\n@Test\\nvoid testLeadingZeroes() {\\n    checkVersionsOrder(\"0.7\", \"2\");\\n    checkVersionsOrder(\"0.2\", \"1.0.7\");\\n}\\n@Test\\nvoid testGetCanonical() {\\n    // MNG-7700\\n    newComparable(\"0.x\");\\n    newComparable(\"0-x\");\\n    newComparable(\"0.rc\");\\n    newComparable(\"0-1\");\\n    ComparableVersion version = new ComparableVersion(\"0.x\");\\n    assertEquals(\"x\", version.getCanonical());\\n    ComparableVersion version2 = new ComparableVersion(\"0.2\");\\n    assertEquals(\"0.2\", version2.getCanonical());\\n}\\n@Test\\nvoid testCompareDigitToLetter() {\\n    ComparableVersion c1 = new ComparableVersion(\"7\");\\n    ComparableVersion c2 = new ComparableVersion(\"J\");\\n    ComparableVersion c3 = new ComparableVersion(\"c\");\\n    assertTrue(c1.compareTo(c2) > 0, \"expected 7 > J\");\\n    assertTrue(c2.compareTo(c1) < 0, \"expected J < 1\");\\n    assertTrue(c1.compareTo(c3) > 0, \"expected 7 > c\");\\n    assertTrue(c3.compareTo(c1) < 0, \"expected c < 7\");\\n}\\n@Test\\nvoid testLexicographicOrder() {\\n    ComparableVersion aardvark = new ComparableVersion(\"aardvark\");\\n    ComparableVersion zebra = new ComparableVersion(\"zebra\");\\n    assertTrue(zebra.compareTo(aardvark) > 0);\\n    assertTrue(aardvark.compareTo(zebra) < 0);\\n    // Greek zebra\\n    ComparableVersion ????? = new ComparableVersion(\"?????\");\\n    assertTrue(?????.compareTo(zebra) > 0);\\n    assertTrue(zebra.compareTo(?????) < 0);\\n}\\n}\\n\\ncompat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java (After)\\nclass ComparableVersionTest {\\n@Test\\nvoid testLeadingZeroes() {\\n    checkVersionsOrder(\"0.7\", \"2\");\\n    checkVersionsOrder(\"0.2\", \"1.0.7\");\\n}\\n@Test\\nvoid testDigitGreaterThanNonAscii() {\\n    ComparableVersion c1 = new ComparableVersion(\"1\");\\n    ComparableVersion c2 = new ComparableVersion(\"é\");\\n    assertTrue(c1.compareTo(c2) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\uD835\\\\uDFE4\");\\n    assertTrue(c2.compareTo(c1) < 0, \"expected \" + \"\\\\uD835\\\\uDFE4\" + \" < \" + \"1\");\\n}\\n@Test\\nvoid testDigitGreaterThanNonBmpCharacters() {\\n    ComparableVersion c1 = new ComparableVersion(\"1\");\\n    // MATHEMATICAL SANS-SERIF DIGIT TWO\\n    ComparableVersion c2 = new ComparableVersion(\"\\\\uD835\\\\uDFE4\");\\n    assertTrue(c1.compareTo(c2) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\uD835\\\\uDFE4\");\\n    assertTrue(c2.compareTo(c1) < 0, \"expected \" + \"\\\\uD835\\\\uDFE4\" + \" < \" + \"1\");\\n}\\n@Test\\nvoid testGetCanonical() {\\n    // MNG-7700\\n    newComparable(\"0.x\");\\n    newComparable(\"0-x\");\\n    newComparable(\"0.rc\");\\n    newComparable(\"0-1\");\\n    ComparableVersion version = new ComparableVersion(\"0.x\");\\n    assertEquals(\"x\", version.getCanonical());\\n    ComparableVersion version2 = new ComparableVersion(\"0.2\");\\n    assertEquals(\"0.2\", version2.getCanonical());\\n}\\n@Test\\nvoid testCompareDigitToLetter() {\\n    ComparableVersion seven = new ComparableVersion(\"7\");\\n    ComparableVersion capitalJ = new ComparableVersion(\"J\");\\n    ComparableVersion lowerCaseC = new ComparableVersion(\"c\");\\n    // Digits are greater than letters\\n    assertTrue(seven.compareTo(capitalJ) > 0, \"expected 7 > J\");\\n    assertTrue(capitalJ.compareTo(seven) < 0, \"expected J < 1\");\\n    assertTrue(seven.compareTo(lowerCaseC) > 0, \"expected 7 > c\");\\n    assertTrue(lowerCaseC.compareTo(seven) < 0, \"expected c < 7\");\\n}\\n@Test\\nvoid testNonAsciiDigits() {\\n    // These should not be treated as digits.\\n    ComparableVersion asciiOne = new ComparableVersion(\"1\");\\n    ComparableVersion arabicEight = new ComparableVersion(\"\\\\u0668\");\\n    ComparableVersion asciiNine = new ComparableVersion(\"9\");\\n    assertTrue(asciiOne.compareTo(arabicEight) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\u0668\");\\n    assertTrue(arabicEight.compareTo(asciiOne) < 0, \"expected \" + \"\\\\u0668\" + \" < \" + \"1\");\\n    assertTrue(asciiNine.compareTo(arabicEight) > 0, \"expected \" + \"9\" + \" > \" + \"\\\\u0668\");\\n    assertTrue(arabicEight.compareTo(asciiNine) < 0, \"expected \" + \"\\\\u0668\" + \" < \" + \"9\");\\n}\\n@Test\\nvoid testLexicographicOrder() {\\n    ComparableVersion aardvark = new ComparableVersion(\"aardvark\");\\n    ComparableVersion zebra = new ComparableVersion(\"zebra\");\\n    assertTrue(zebra.compareTo(aardvark) > 0);\\n    assertTrue(aardvark.compareTo(zebra) < 0);\\n    // Greek zebra\\n    ComparableVersion ????? = new ComparableVersion(\"?????\");\\n    assertTrue(?????.compareTo(zebra) > 0);\\n    assertTrue(zebra.compareTo(?????) < 0);\\n}\\n}\\n\\n\\nSummary:', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Given a Git diff and the relevant source code, write a concise summary of the code changes in a way that a non-technical person can understand. Summarize in maximum three concise sentences. \\n\\nAvoid adding any additional comments or annotations to the summary.\\n\\nGit diff:\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n\\nSource code:\\ncontrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java (Before)\\npublic class HiveStoragePlugin extends AbstractStoragePlugin {\\n@Override\\npublic Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n    switch(phase) {\\n        case LOGICAL:\\n            final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n            ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnScan(optimizerContext, defaultPartitionValue));\\n            return ruleBuilder.build();\\n        case PHYSICAL:\\n            {\\n                ruleBuilder = ImmutableSet.builder();\\n                OptionManager options = optimizerContext.getPlannerSettings().getOptions();\\n                // TODO: Remove implicit using of convert_fromTIMESTAMP_IMPALA function\\n                // once \"store.parquet.reader.int96_as_timestamp\" will be true by default\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_SCAN_WITH_NATIVE_READERS) || options.getBoolean(ExecConstants.HIVE_OPTIMIZE_PARQUET_SCAN_WITH_NATIVE_READER)) {\\n                    ruleBuilder.add(ConvertHiveParquetScanToDrillParquetScan.INSTANCE);\\n                }\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER)) {\\n                    try {\\n                        Class<?> hiveToDrillMapRDBJsonRuleClass = Class.forName(\"org.apache.drill.exec.planner.sql.logical.ConvertHiveMapRDBJsonScanToDrillMapRDBJsonScan\");\\n                        ruleBuilder.add((StoragePluginOptimizerRule) hiveToDrillMapRDBJsonRuleClass.getField(\"INSTANCE\").get(null));\\n                    } catch (ReflectiveOperationException e) {\\n                        logger.warn(\"Current Drill build is not designed for working with Hive MapR-DB tables. \" + \"Please disable {} option\", ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER);\\n                    }\\n                }\\n                return ruleBuilder.build();\\n            }\\n        default:\\n            return ImmutableSet.of();\\n    }\\n}\\n}\\n\\ncontrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java (After)\\npublic class HiveStoragePlugin extends AbstractStoragePlugin {\\n@Override\\npublic Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n    switch(phase) {\\n        case PARTITION_PRUNING:\\n            final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n            ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\n            ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnScan(optimizerContext, defaultPartitionValue));\\n            return ruleBuilder.build();\\n        case PHYSICAL:\\n            {\\n                ruleBuilder = ImmutableSet.builder();\\n                OptionManager options = optimizerContext.getPlannerSettings().getOptions();\\n                // TODO: Remove implicit using of convert_fromTIMESTAMP_IMPALA function\\n                // once \"store.parquet.reader.int96_as_timestamp\" will be true by default\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_SCAN_WITH_NATIVE_READERS) || options.getBoolean(ExecConstants.HIVE_OPTIMIZE_PARQUET_SCAN_WITH_NATIVE_READER)) {\\n                    ruleBuilder.add(ConvertHiveParquetScanToDrillParquetScan.INSTANCE);\\n                }\\n                if (options.getBoolean(ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER)) {\\n                    try {\\n                        Class<?> hiveToDrillMapRDBJsonRuleClass = Class.forName(\"org.apache.drill.exec.planner.sql.logical.ConvertHiveMapRDBJsonScanToDrillMapRDBJsonScan\");\\n                        ruleBuilder.add((StoragePluginOptimizerRule) hiveToDrillMapRDBJsonRuleClass.getField(\"INSTANCE\").get(null));\\n                    } catch (ReflectiveOperationException e) {\\n                        logger.warn(\"Current Drill build is not designed for working with Hive MapR-DB tables. \" + \"Please disable {} option\", ExecConstants.HIVE_OPTIMIZE_MAPRDB_JSON_SCAN_WITH_NATIVE_READER);\\n                    }\\n                }\\n                return ruleBuilder.build();\\n            }\\n        default:\\n            return ImmutableSet.of();\\n    }\\n}\\n}\\n\\ncontrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java (Before)\\nimport org.apache.drill.exec.rpc.user.QueryDataBatch;\\nimport org.junit.AfterClass;\\nimport org.junit.BeforeClass;\\nimport org.junit.Ignore;\\nimport org.junit.Test;\\nimport org.junit.experimental.categories.Category;\\npublic class TestHivePartitionPruning extends HiveTestBase {\\n// DRILL-5032\\n@Test\\npublic void testPartitionColumnsCaching() throws Exception {\\n    final String query = \"EXPLAIN PLAN FOR SELECT * FROM hive.partition_with_few_schemas\";\\n    List<QueryDataBatch> queryDataBatches = testSqlWithResults(query);\\n    String resultString = getResultString(queryDataBatches, \"|\");\\n    // different for both partitions column strings from physical plan\\n    String columnString = \"\\\\\"name\\\\\" : \\\\\"a\\\\\"\";\\n    String secondColumnString = \"\\\\\"name\\\\\" : \\\\\"a1\\\\\"\";\\n    int columnIndex = resultString.indexOf(columnString);\\n    assertTrue(columnIndex >= 0);\\n    columnIndex = resultString.indexOf(columnString, columnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, columnIndex);\\n    int secondColumnIndex = resultString.indexOf(secondColumnString);\\n    assertTrue(secondColumnIndex >= 0);\\n    secondColumnIndex = resultString.indexOf(secondColumnString, secondColumnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, secondColumnIndex);\\n}\\n// DRILL-6173\\n@Test\\n@Ignore(\"DRILL-8400\")\\npublic void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" + \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" + \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n    int actualRowCount = testSql(query);\\n    int expectedRowCount = 450;\\n    assertEquals(\"Expected and actual row count should match\", expectedRowCount, actualRowCount);\\n    final String[] expectedPlan = { \"partition_with_few_schemas.*numPartitions=6\", \"partition_pruning_test.*numPartitions=6\" };\\n    testPlanMatchingPatterns(query, expectedPlan);\\n}\\n}\\n\\ncontrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java (After)\\nimport org.apache.drill.exec.rpc.user.QueryDataBatch;\\nimport org.junit.AfterClass;\\nimport org.junit.BeforeClass;\\nimport org.junit.Test;\\nimport org.junit.experimental.categories.Category;\\npublic class TestHivePartitionPruning extends HiveTestBase {\\n// DRILL-5032\\n@Test\\npublic void testPartitionColumnsCaching() throws Exception {\\n    final String query = \"EXPLAIN PLAN FOR SELECT * FROM hive.partition_with_few_schemas\";\\n    List<QueryDataBatch> queryDataBatches = testSqlWithResults(query);\\n    String resultString = getResultString(queryDataBatches, \"|\");\\n    // different for both partitions column strings from physical plan\\n    String columnString = \"\\\\\"name\\\\\" : \\\\\"a\\\\\"\";\\n    String secondColumnString = \"\\\\\"name\\\\\" : \\\\\"a1\\\\\"\";\\n    int columnIndex = resultString.indexOf(columnString);\\n    assertTrue(columnIndex >= 0);\\n    columnIndex = resultString.indexOf(columnString, columnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, columnIndex);\\n    int secondColumnIndex = resultString.indexOf(secondColumnString);\\n    assertTrue(secondColumnIndex >= 0);\\n    secondColumnIndex = resultString.indexOf(secondColumnString, secondColumnIndex + 1);\\n    // checks that column added to physical plan only one time\\n    assertEquals(-1, secondColumnIndex);\\n}\\n// DRILL-6173\\n@Test\\npublic void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" + \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" + \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n    int actualRowCount = testSql(query);\\n    int expectedRowCount = 450;\\n    assertEquals(\"Expected and actual row count should match\", expectedRowCount, actualRowCount);\\n    final String[] expectedPlan = { \"partition_with_few_schemas.*numPartitions=6\", \"partition_pruning_test.*numPartitions=6\" };\\n    testPlanMatchingPatterns(query, expectedPlan);\\n}\\n}\\n\\n\\nSummary:', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Given a Git diff and the relevant source code, write a concise summary of the code changes in a way that a non-technical person can understand. Summarize in maximum three concise sentences. \\n\\nAvoid adding any additional comments or annotations to the summary.\\n\\nGit diff:\\ndiff --git a/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java b/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java\\nindex 3ca787fc3..f8b275a01 100644\\n--- a/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java\\n+++ b/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java\\n@@ -17,10 +17,9 @@\\n  */\\n package org.apache.zookeeper.common;\\n \\n-\\n-import java.io.ByteArrayInputStream;\\n import java.io.Closeable;\\n import java.io.IOException;\\n+import java.lang.reflect.InvocationTargetException;\\n import java.net.Socket;\\n import java.nio.file.Path;\\n import java.nio.file.Paths;\\n@@ -33,15 +32,14 @@\\n import java.security.Security;\\n import java.security.cert.PKIXBuilderParameters;\\n import java.security.cert.X509CertSelector;\\n-import java.util.Arrays;\\n import java.util.Objects;\\n import java.util.concurrent.atomic.AtomicReference;\\n+import java.util.function.Supplier;\\n \\n import javax.net.ssl.CertPathTrustManagerParameters;\\n import javax.net.ssl.KeyManager;\\n import javax.net.ssl.KeyManagerFactory;\\n import javax.net.ssl.SSLContext;\\n-import javax.net.ssl.SSLParameters;\\n import javax.net.ssl.SSLServerSocket;\\n import javax.net.ssl.SSLSocket;\\n import javax.net.ssl.TrustManager;\\n@@ -137,6 +135,7 @@ public static ClientAuth fromPropertyValue(String prop) {\\n     private String sslTruststoreLocationProperty = getConfigPrefix() + \"trustStore.location\";\\n     private String sslTruststorePasswdProperty = getConfigPrefix() + \"trustStore.password\";\\n     private String sslTruststoreTypeProperty = getConfigPrefix() + \"trustStore.type\";\\n+    private String sslContextSupplierClassProperty = getConfigPrefix() + \"context.supplier.class\";\\n     private String sslHostnameVerificationEnabledProperty = getConfigPrefix() + \"hostnameVerification\";\\n     private String sslCrlEnabledProperty = getConfigPrefix() + \"crl\";\\n     private String sslOcspEnabledProperty = getConfigPrefix() + \"ocsp\";\\n@@ -202,6 +201,10 @@ public String getSslTruststoreTypeProperty() {\\n         return sslTruststoreTypeProperty;\\n     }\\n \\n+    public String getSslContextSupplierClassProperty() {\\n+        return sslContextSupplierClassProperty;\\n+    }\\n+\\n     public String getSslHostnameVerificationEnabledProperty() {\\n         return sslHostnameVerificationEnabledProperty;\\n     }\\n@@ -282,7 +285,28 @@ public int getSslHandshakeTimeoutMillis() {\\n         }\\n     }\\n \\n+    @SuppressWarnings(\"unchecked\")\\n     public SSLContextAndOptions createSSLContextAndOptions(ZKConfig config) throws SSLContextException {\\n+        final String supplierContextClassName = config.getProperty(sslContextSupplierClassProperty);\\n+        if (supplierContextClassName != null) {\\n+            if (LOG.isDebugEnabled()) {\\n+                LOG.debug(\"Loading SSLContext supplier from property \\'{}\\'\", sslContextSupplierClassProperty);\\n+            }\\n+            try {\\n+                Class<?> sslContextClass = Class.forName(supplierContextClassName);\\n+                Supplier<SSLContext> sslContextSupplier = (Supplier<SSLContext>) sslContextClass.getConstructor().newInstance();\\n+                return new SSLContextAndOptions(this, config, sslContextSupplier.get());\\n+            } catch (ClassNotFoundException | ClassCastException | NoSuchMethodException | InvocationTargetException |\\n+                    InstantiationException | IllegalAccessException e) {\\n+                throw new SSLContextException(\"Could not retrieve the SSLContext from supplier source \\'\" + supplierContextClassName +\\n+                        \"\\' provided in the property \\'\" + sslContextSupplierClassProperty + \"\\'\", e);\\n+            }\\n+        } else {\\n+            return createSSLContextAndOptionsFromConfig(config);\\n+        }\\n+    }\\n+\\n+    public SSLContextAndOptions createSSLContextAndOptionsFromConfig(ZKConfig config) throws SSLContextException {\\n         KeyManager[] keyManagers = null;\\n         TrustManager[] trustManagers = null;\\n \\ndiff --git a/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java b/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java\\nindex 43bc2d8e9..76bdd2e20 100644\\n--- a/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java\\n+++ b/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java\\n@@ -133,6 +133,8 @@ private void putSSLProperties(X509Util x509Util) {\\n                 System.getProperty(x509Util.getSslTruststorePasswdProperty()));\\n         properties.put(x509Util.getSslTruststoreTypeProperty(),\\n                 System.getProperty(x509Util.getSslTruststoreTypeProperty()));\\n+        properties.put(x509Util.getSslContextSupplierClassProperty(),\\n+                System.getProperty(x509Util.getSslContextSupplierClassProperty()));\\n         properties.put(x509Util.getSslHostnameVerificationEnabledProperty(),\\n                 System.getProperty(x509Util.getSslHostnameVerificationEnabledProperty()));\\n         properties.put(x509Util.getSslCrlEnabledProperty(),\\ndiff --git a/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java b/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java\\nindex 2a6bb3246..1fecd808d 100644\\n--- a/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java\\n+++ b/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java\\n@@ -22,6 +22,7 @@\\n import java.net.InetSocketAddress;\\n import java.net.ServerSocket;\\n import java.net.Socket;\\n+import java.security.NoSuchAlgorithmException;\\n import java.security.Security;\\n import java.util.Collection;\\n import java.util.concurrent.Callable;\\n@@ -30,6 +31,7 @@\\n import java.util.concurrent.Executors;\\n import java.util.concurrent.Future;\\n import java.util.concurrent.atomic.AtomicInteger;\\n+import java.util.function.Supplier;\\n \\n import javax.net.ssl.HandshakeCompletedEvent;\\n import javax.net.ssl.HandshakeCompletedListener;\\n@@ -403,6 +405,23 @@ public void testGetSslHandshakeDetectionTimeoutMillisProperty() {\\n         }\\n     }\\n \\n+    @Test(expected = X509Exception.SSLContextException.class)\\n+    public void testCreateSSLContext_invalidCustomSSLContextClass() throws Exception {\\n+        ZKConfig zkConfig = new ZKConfig();\\n+        ClientX509Util clientX509Util = new ClientX509Util();\\n+        zkConfig.setProperty(clientX509Util.getSslContextSupplierClassProperty(), String.class.getCanonicalName());\\n+        clientX509Util.createSSLContext(zkConfig);\\n+    }\\n+\\n+    @Test\\n+    public void testCreateSSLContext_validCustomSSLContextClass() throws Exception {\\n+        ZKConfig zkConfig = new ZKConfig();\\n+        ClientX509Util clientX509Util = new ClientX509Util();\\n+        zkConfig.setProperty(clientX509Util.getSslContextSupplierClassProperty(), SslContextSupplier.class.getName());\\n+        final SSLContext sslContext = clientX509Util.createSSLContext(zkConfig);\\n+        Assert.assertEquals(SSLContext.getDefault(), sslContext);\\n+    }\\n+\\n     private static void forceClose(Socket s) {\\n         if (s == null || s.isClosed()) {\\n             return;\\n@@ -528,4 +547,18 @@ private void setCustomCipherSuites() {\\n         x509Util.close(); // remember to close old instance before replacing it\\n         x509Util = new ClientX509Util();\\n     }\\n+\\n+    public static class SslContextSupplier implements Supplier<SSLContext> {\\n+\\n+        @Override\\n+        public SSLContext get() {\\n+            try {\\n+                return SSLContext.getDefault();\\n+            } catch (NoSuchAlgorithmException e) {\\n+                throw new RuntimeException(e);\\n+            }\\n+        }\\n+\\n+    }\\n+\\n }\\n\\n\\nSource code:\\nzookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java (Before)\\npackage org.apache.zookeeper.common;\\n\\nimport java.io.ByteArrayInputStream;\\nimport java.io.Closeable;\\nimport java.io.IOException;\\nimport java.net.Socket;\\nimport java.nio.file.Path;\\nimport java.nio.file.Paths;\\nimport java.security.Security;\\nimport java.security.cert.PKIXBuilderParameters;\\nimport java.security.cert.X509CertSelector;\\nimport java.util.Arrays;\\nimport java.util.Objects;\\nimport java.util.concurrent.atomic.AtomicReference;\\nimport javax.net.ssl.CertPathTrustManagerParameters;\\nimport javax.net.ssl.KeyManager;\\nimport javax.net.ssl.KeyManagerFactory;\\nimport javax.net.ssl.SSLContext;\\nimport javax.net.ssl.SSLParameters;\\nimport javax.net.ssl.SSLServerSocket;\\nimport javax.net.ssl.SSLSocket;\\nimport javax.net.ssl.TrustManager;\\npublic  abstract class X509Util implements Closeable, AutoCloseable {\\nprivate String sslTruststoreLocationProperty = getConfigPrefix() + \"trustStore.location\";\\nprivate String sslTruststorePasswdProperty = getConfigPrefix() + \"trustStore.password\";\\nprivate String sslTruststoreTypeProperty = getConfigPrefix() + \"trustStore.type\";\\nprivate String sslHostnameVerificationEnabledProperty = getConfigPrefix() + \"hostnameVerification\";\\nprivate String sslCrlEnabledProperty = getConfigPrefix() + \"crl\";\\nprivate String sslOcspEnabledProperty = getConfigPrefix() + \"ocsp\";\\npublic String getSslTruststoreTypeProperty() {\\n    return sslTruststoreTypeProperty;\\n}\\npublic String getSslHostnameVerificationEnabledProperty() {\\n    return sslHostnameVerificationEnabledProperty;\\n}\\n/**\\n * Returns the max amount of time, in milliseconds, that the first UnifiedServerSocket read() operation should\\n * block for when trying to detect the client mode (TLS or PLAINTEXT).\\n * Defaults to {@link X509Util#DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS}.\\n *\\n * @return the handshake detection timeout, in milliseconds.\\n */\\npublic int getSslHandshakeTimeoutMillis() {\\n    try {\\n        SSLContextAndOptions ctx = getDefaultSSLContextAndOptions();\\n        return ctx.getHandshakeDetectionTimeoutMillis();\\n    } catch (SSLContextException e) {\\n        LOG.error(\"Error creating SSL context and options\", e);\\n        return DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS;\\n    } catch (Exception e) {\\n        LOG.error(\"Error parsing config property \" + getSslHandshakeDetectionTimeoutMillisProperty(), e);\\n        return DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS;\\n    }\\n}\\npublic SSLContextAndOptions createSSLContextAndOptions(ZKConfig config) throws SSLContextException {\\n    KeyManager[] keyManagers = null;\\n    TrustManager[] trustManagers = null;\\n    String keyStoreLocationProp = config.getProperty(sslKeystoreLocationProperty, \"\");\\n    String keyStorePasswordProp = config.getProperty(sslKeystorePasswdProperty, \"\");\\n    String keyStoreTypeProp = config.getProperty(sslKeystoreTypeProperty);\\n    // There are legal states in some use cases for null KeyManager or TrustManager.\\n    // But if a user wanna specify one, location is required. Password defaults to empty string if it is not\\n    // specified by the user.\\n    if (keyStoreLocationProp.isEmpty()) {\\n        LOG.warn(getSslKeystoreLocationProperty() + \" not specified\");\\n    } else {\\n        try {\\n            keyManagers = new KeyManager[] { createKeyManager(keyStoreLocationProp, keyStorePasswordProp, keyStoreTypeProp) };\\n        } catch (KeyManagerException keyManagerException) {\\n            throw new SSLContextException(\"Failed to create KeyManager\", keyManagerException);\\n        } catch (IllegalArgumentException e) {\\n            throw new SSLContextException(\"Bad value for \" + sslKeystoreTypeProperty + \": \" + keyStoreTypeProp, e);\\n        }\\n    }\\n    String trustStoreLocationProp = config.getProperty(sslTruststoreLocationProperty, \"\");\\n    String trustStorePasswordProp = config.getProperty(sslTruststorePasswdProperty, \"\");\\n    String trustStoreTypeProp = config.getProperty(sslTruststoreTypeProperty);\\n    boolean sslCrlEnabled = config.getBoolean(this.sslCrlEnabledProperty);\\n    boolean sslOcspEnabled = config.getBoolean(this.sslOcspEnabledProperty);\\n    boolean sslServerHostnameVerificationEnabled = config.getBoolean(this.getSslHostnameVerificationEnabledProperty(), true);\\n    boolean sslClientHostnameVerificationEnabled = sslServerHostnameVerificationEnabled && shouldVerifyClientHostname();\\n    if (trustStoreLocationProp.isEmpty()) {\\n        LOG.warn(getSslTruststoreLocationProperty() + \" not specified\");\\n    } else {\\n        try {\\n            trustManagers = new TrustManager[] { createTrustManager(trustStoreLocationProp, trustStorePasswordProp, trustStoreTypeProp, sslCrlEnabled, sslOcspEnabled, sslServerHostnameVerificationEnabled, sslClientHostnameVerificationEnabled) };\\n        } catch (TrustManagerException trustManagerException) {\\n            throw new SSLContextException(\"Failed to create TrustManager\", trustManagerException);\\n        } catch (IllegalArgumentException e) {\\n            throw new SSLContextException(\"Bad value for \" + sslTruststoreTypeProperty + \": \" + trustStoreTypeProp, e);\\n        }\\n    }\\n    String protocol = config.getProperty(sslProtocolProperty, DEFAULT_PROTOCOL);\\n    try {\\n        SSLContext sslContext = SSLContext.getInstance(protocol);\\n        sslContext.init(keyManagers, trustManagers, null);\\n        return new SSLContextAndOptions(this, config, sslContext);\\n    } catch (NoSuchAlgorithmException | KeyManagementException sslContextInitException) {\\n        throw new SSLContextException(sslContextInitException);\\n    }\\n}\\n}\\n\\nzookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java (After)\\npackage org.apache.zookeeper.common;\\n\\nimport java.io.Closeable;\\nimport java.io.IOException;\\nimport java.lang.reflect.InvocationTargetException;\\nimport java.net.Socket;\\nimport java.nio.file.Path;\\nimport java.nio.file.Paths;\\nimport java.security.Security;\\nimport java.security.cert.PKIXBuilderParameters;\\nimport java.security.cert.X509CertSelector;\\nimport java.util.Objects;\\nimport java.util.concurrent.atomic.AtomicReference;\\nimport java.util.function.Supplier;\\nimport javax.net.ssl.CertPathTrustManagerParameters;\\nimport javax.net.ssl.KeyManager;\\nimport javax.net.ssl.KeyManagerFactory;\\nimport javax.net.ssl.SSLContext;\\nimport javax.net.ssl.SSLServerSocket;\\nimport javax.net.ssl.SSLSocket;\\nimport javax.net.ssl.TrustManager;\\npublic  abstract class X509Util implements Closeable, AutoCloseable {\\nprivate String sslTruststoreLocationProperty = getConfigPrefix() + \"trustStore.location\";\\nprivate String sslTruststorePasswdProperty = getConfigPrefix() + \"trustStore.password\";\\nprivate String sslTruststoreTypeProperty = getConfigPrefix() + \"trustStore.type\";\\nprivate String sslContextSupplierClassProperty = getConfigPrefix() + \"context.supplier.class\";\\nprivate String sslHostnameVerificationEnabledProperty = getConfigPrefix() + \"hostnameVerification\";\\nprivate String sslCrlEnabledProperty = getConfigPrefix() + \"crl\";\\nprivate String sslOcspEnabledProperty = getConfigPrefix() + \"ocsp\";\\npublic String getSslTruststoreTypeProperty() {\\n    return sslTruststoreTypeProperty;\\n}\\npublic String getSslContextSupplierClassProperty() {\\n    return sslContextSupplierClassProperty;\\n}\\npublic String getSslHostnameVerificationEnabledProperty() {\\n    return sslHostnameVerificationEnabledProperty;\\n}\\n/**\\n * Returns the max amount of time, in milliseconds, that the first UnifiedServerSocket read() operation should\\n * block for when trying to detect the client mode (TLS or PLAINTEXT).\\n * Defaults to {@link X509Util#DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS}.\\n *\\n * @return the handshake detection timeout, in milliseconds.\\n */\\npublic int getSslHandshakeTimeoutMillis() {\\n    try {\\n        SSLContextAndOptions ctx = getDefaultSSLContextAndOptions();\\n        return ctx.getHandshakeDetectionTimeoutMillis();\\n    } catch (SSLContextException e) {\\n        LOG.error(\"Error creating SSL context and options\", e);\\n        return DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS;\\n    } catch (Exception e) {\\n        LOG.error(\"Error parsing config property \" + getSslHandshakeDetectionTimeoutMillisProperty(), e);\\n        return DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS;\\n    }\\n}\\n@SuppressWarnings(\"unchecked\")\\npublic SSLContextAndOptions createSSLContextAndOptions(ZKConfig config) throws SSLContextException {\\n    final String supplierContextClassName = config.getProperty(sslContextSupplierClassProperty);\\n    if (supplierContextClassName != null) {\\n        if (LOG.isDebugEnabled()) {\\n            LOG.debug(\"Loading SSLContext supplier from property \\'{}\\'\", sslContextSupplierClassProperty);\\n        }\\n        try {\\n            Class<?> sslContextClass = Class.forName(supplierContextClassName);\\n            Supplier<SSLContext> sslContextSupplier = (Supplier<SSLContext>) sslContextClass.getConstructor().newInstance();\\n            return new SSLContextAndOptions(this, config, sslContextSupplier.get());\\n        } catch (ClassNotFoundException | ClassCastException | NoSuchMethodException | InvocationTargetException | InstantiationException | IllegalAccessException e) {\\n            throw new SSLContextException(\"Could not retrieve the SSLContext from supplier source \\'\" + supplierContextClassName + \"\\' provided in the property \\'\" + sslContextSupplierClassProperty + \"\\'\", e);\\n        }\\n    } else {\\n        return createSSLContextAndOptionsFromConfig(config);\\n    }\\n}\\npublic SSLContextAndOptions createSSLContextAndOptionsFromConfig(ZKConfig config) throws SSLContextException {\\n    KeyManager[] keyManagers = null;\\n    TrustManager[] trustManagers = null;\\n    String keyStoreLocationProp = config.getProperty(sslKeystoreLocationProperty, \"\");\\n    String keyStorePasswordProp = config.getProperty(sslKeystorePasswdProperty, \"\");\\n    String keyStoreTypeProp = config.getProperty(sslKeystoreTypeProperty);\\n    // There are legal states in some use cases for null KeyManager or TrustManager.\\n    // But if a user wanna specify one, location is required. Password defaults to empty string if it is not\\n    // specified by the user.\\n    if (keyStoreLocationProp.isEmpty()) {\\n        LOG.warn(getSslKeystoreLocationProperty() + \" not specified\");\\n    } else {\\n        try {\\n            keyManagers = new KeyManager[] { createKeyManager(keyStoreLocationProp, keyStorePasswordProp, keyStoreTypeProp) };\\n        } catch (KeyManagerException keyManagerException) {\\n            throw new SSLContextException(\"Failed to create KeyManager\", keyManagerException);\\n        } catch (IllegalArgumentException e) {\\n            throw new SSLContextException(\"Bad value for \" + sslKeystoreTypeProperty + \": \" + keyStoreTypeProp, e);\\n        }\\n    }\\n    String trustStoreLocationProp = config.getProperty(sslTruststoreLocationProperty, \"\");\\n    String trustStorePasswordProp = config.getProperty(sslTruststorePasswdProperty, \"\");\\n    String trustStoreTypeProp = config.getProperty(sslTruststoreTypeProperty);\\n    boolean sslCrlEnabled = config.getBoolean(this.sslCrlEnabledProperty);\\n    boolean sslOcspEnabled = config.getBoolean(this.sslOcspEnabledProperty);\\n    boolean sslServerHostnameVerificationEnabled = config.getBoolean(this.getSslHostnameVerificationEnabledProperty(), true);\\n    boolean sslClientHostnameVerificationEnabled = sslServerHostnameVerificationEnabled && shouldVerifyClientHostname();\\n    if (trustStoreLocationProp.isEmpty()) {\\n        LOG.warn(getSslTruststoreLocationProperty() + \" not specified\");\\n    } else {\\n        try {\\n            trustManagers = new TrustManager[] { createTrustManager(trustStoreLocationProp, trustStorePasswordProp, trustStoreTypeProp, sslCrlEnabled, sslOcspEnabled, sslServerHostnameVerificationEnabled, sslClientHostnameVerificationEnabled) };\\n        } catch (TrustManagerException trustManagerException) {\\n            throw new SSLContextException(\"Failed to create TrustManager\", trustManagerException);\\n        } catch (IllegalArgumentException e) {\\n            throw new SSLContextException(\"Bad value for \" + sslTruststoreTypeProperty + \": \" + trustStoreTypeProp, e);\\n        }\\n    }\\n    String protocol = config.getProperty(sslProtocolProperty, DEFAULT_PROTOCOL);\\n    try {\\n        SSLContext sslContext = SSLContext.getInstance(protocol);\\n        sslContext.init(keyManagers, trustManagers, null);\\n        return new SSLContextAndOptions(this, config, sslContext);\\n    } catch (NoSuchAlgorithmException | KeyManagementException sslContextInitException) {\\n        throw new SSLContextException(sslContextInitException);\\n    }\\n}\\n}\\n\\nzookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java (Before)\\npublic class ZKConfig {\\nprivate void putSSLProperties(X509Util x509Util) {\\n    properties.put(x509Util.getSslProtocolProperty(), System.getProperty(x509Util.getSslProtocolProperty()));\\n    properties.put(x509Util.getSslEnabledProtocolsProperty(), System.getProperty(x509Util.getSslEnabledProtocolsProperty()));\\n    properties.put(x509Util.getSslCipherSuitesProperty(), System.getProperty(x509Util.getSslCipherSuitesProperty()));\\n    properties.put(x509Util.getSslKeystoreLocationProperty(), System.getProperty(x509Util.getSslKeystoreLocationProperty()));\\n    properties.put(x509Util.getSslKeystorePasswdProperty(), System.getProperty(x509Util.getSslKeystorePasswdProperty()));\\n    properties.put(x509Util.getSslKeystoreTypeProperty(), System.getProperty(x509Util.getSslKeystoreTypeProperty()));\\n    properties.put(x509Util.getSslTruststoreLocationProperty(), System.getProperty(x509Util.getSslTruststoreLocationProperty()));\\n    properties.put(x509Util.getSslTruststorePasswdProperty(), System.getProperty(x509Util.getSslTruststorePasswdProperty()));\\n    properties.put(x509Util.getSslTruststoreTypeProperty(), System.getProperty(x509Util.getSslTruststoreTypeProperty()));\\n    properties.put(x509Util.getSslHostnameVerificationEnabledProperty(), System.getProperty(x509Util.getSslHostnameVerificationEnabledProperty()));\\n    properties.put(x509Util.getSslCrlEnabledProperty(), System.getProperty(x509Util.getSslCrlEnabledProperty()));\\n    properties.put(x509Util.getSslOcspEnabledProperty(), System.getProperty(x509Util.getSslOcspEnabledProperty()));\\n    properties.put(x509Util.getSslClientAuthProperty(), System.getProperty(x509Util.getSslClientAuthProperty()));\\n    properties.put(x509Util.getSslHandshakeDetectionTimeoutMillisProperty(), System.getProperty(x509Util.getSslHandshakeDetectionTimeoutMillisProperty()));\\n}\\n}\\n\\nzookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java (After)\\npublic class ZKConfig {\\nprivate void putSSLProperties(X509Util x509Util) {\\n    properties.put(x509Util.getSslProtocolProperty(), System.getProperty(x509Util.getSslProtocolProperty()));\\n    properties.put(x509Util.getSslEnabledProtocolsProperty(), System.getProperty(x509Util.getSslEnabledProtocolsProperty()));\\n    properties.put(x509Util.getSslCipherSuitesProperty(), System.getProperty(x509Util.getSslCipherSuitesProperty()));\\n    properties.put(x509Util.getSslKeystoreLocationProperty(), System.getProperty(x509Util.getSslKeystoreLocationProperty()));\\n    properties.put(x509Util.getSslKeystorePasswdProperty(), System.getProperty(x509Util.getSslKeystorePasswdProperty()));\\n    properties.put(x509Util.getSslKeystoreTypeProperty(), System.getProperty(x509Util.getSslKeystoreTypeProperty()));\\n    properties.put(x509Util.getSslTruststoreLocationProperty(), System.getProperty(x509Util.getSslTruststoreLocationProperty()));\\n    properties.put(x509Util.getSslTruststorePasswdProperty(), System.getProperty(x509Util.getSslTruststorePasswdProperty()));\\n    properties.put(x509Util.getSslTruststoreTypeProperty(), System.getProperty(x509Util.getSslTruststoreTypeProperty()));\\n    properties.put(x509Util.getSslContextSupplierClassProperty(), System.getProperty(x509Util.getSslContextSupplierClassProperty()));\\n    properties.put(x509Util.getSslHostnameVerificationEnabledProperty(), System.getProperty(x509Util.getSslHostnameVerificationEnabledProperty()));\\n    properties.put(x509Util.getSslCrlEnabledProperty(), System.getProperty(x509Util.getSslCrlEnabledProperty()));\\n    properties.put(x509Util.getSslOcspEnabledProperty(), System.getProperty(x509Util.getSslOcspEnabledProperty()));\\n    properties.put(x509Util.getSslClientAuthProperty(), System.getProperty(x509Util.getSslClientAuthProperty()));\\n    properties.put(x509Util.getSslHandshakeDetectionTimeoutMillisProperty(), System.getProperty(x509Util.getSslHandshakeDetectionTimeoutMillisProperty()));\\n}\\n}\\n\\nzookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java (Before)\\nimport java.net.InetSocketAddress;\\nimport java.net.ServerSocket;\\nimport java.net.Socket;\\nimport java.security.Security;\\nimport java.util.Collection;\\nimport java.util.concurrent.Callable;\\nimport java.util.concurrent.Executors;\\nimport java.util.concurrent.Future;\\nimport java.util.concurrent.atomic.AtomicInteger;\\nimport javax.net.ssl.HandshakeCompletedEvent;\\nimport javax.net.ssl.HandshakeCompletedListener;\\npublic class X509UtilTest extends BaseX509ParameterizedTestCase {\\n@Test\\npublic void testGetSslHandshakeDetectionTimeoutMillisProperty() {\\n    Assert.assertEquals(X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS, x509Util.getSslHandshakeTimeoutMillis());\\n    // Note: need to create a new ClientX509Util each time to pick up modified property value\\n    String newPropertyString = Integer.toString(X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS + 1);\\n    System.setProperty(x509Util.getSslHandshakeDetectionTimeoutMillisProperty(), newPropertyString);\\n    try (X509Util tempX509Util = new ClientX509Util()) {\\n        Assert.assertEquals(X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS + 1, tempX509Util.getSslHandshakeTimeoutMillis());\\n    }\\n    // 0 value not allowed, will return the default\\n    System.setProperty(x509Util.getSslHandshakeDetectionTimeoutMillisProperty(), \"0\");\\n    try (X509Util tempX509Util = new ClientX509Util()) {\\n        Assert.assertEquals(X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS, tempX509Util.getSslHandshakeTimeoutMillis());\\n    }\\n    // Negative value not allowed, will return the default\\n    System.setProperty(x509Util.getSslHandshakeDetectionTimeoutMillisProperty(), \"-1\");\\n    try (X509Util tempX509Util = new ClientX509Util()) {\\n        Assert.assertEquals(X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS, tempX509Util.getSslHandshakeTimeoutMillis());\\n    }\\n}\\nprivate static void forceClose(Socket s) {\\n    if (s == null || s.isClosed()) {\\n        return;\\n    }\\n    try {\\n        s.close();\\n    } catch (IOException e) {\\n    }\\n}\\n// Warning: this will reset the x509Util\\nprivate void setCustomCipherSuites() {\\n    System.setProperty(x509Util.getCipherSuitesProperty(), customCipherSuites[0] + \",\" + customCipherSuites[1]);\\n    // remember to close old instance before replacing it\\n    x509Util.close();\\n    x509Util = new ClientX509Util();\\n}\\n}\\n\\nzookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java (After)\\nimport java.net.InetSocketAddress;\\nimport java.net.ServerSocket;\\nimport java.net.Socket;\\nimport java.security.NoSuchAlgorithmException;\\nimport java.security.Security;\\nimport java.util.Collection;\\nimport java.util.concurrent.Callable;\\nimport java.util.concurrent.Executors;\\nimport java.util.concurrent.Future;\\nimport java.util.concurrent.atomic.AtomicInteger;\\nimport java.util.function.Supplier;\\nimport javax.net.ssl.HandshakeCompletedEvent;\\nimport javax.net.ssl.HandshakeCompletedListener;\\npublic class X509UtilTest extends BaseX509ParameterizedTestCase {\\n@Test\\npublic void testGetSslHandshakeDetectionTimeoutMillisProperty() {\\n    Assert.assertEquals(X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS, x509Util.getSslHandshakeTimeoutMillis());\\n    // Note: need to create a new ClientX509Util each time to pick up modified property value\\n    String newPropertyString = Integer.toString(X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS + 1);\\n    System.setProperty(x509Util.getSslHandshakeDetectionTimeoutMillisProperty(), newPropertyString);\\n    try (X509Util tempX509Util = new ClientX509Util()) {\\n        Assert.assertEquals(X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS + 1, tempX509Util.getSslHandshakeTimeoutMillis());\\n    }\\n    // 0 value not allowed, will return the default\\n    System.setProperty(x509Util.getSslHandshakeDetectionTimeoutMillisProperty(), \"0\");\\n    try (X509Util tempX509Util = new ClientX509Util()) {\\n        Assert.assertEquals(X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS, tempX509Util.getSslHandshakeTimeoutMillis());\\n    }\\n    // Negative value not allowed, will return the default\\n    System.setProperty(x509Util.getSslHandshakeDetectionTimeoutMillisProperty(), \"-1\");\\n    try (X509Util tempX509Util = new ClientX509Util()) {\\n        Assert.assertEquals(X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS, tempX509Util.getSslHandshakeTimeoutMillis());\\n    }\\n}\\n@Test(expected = X509Exception.SSLContextException.class)\\npublic void testCreateSSLContext_invalidCustomSSLContextClass() throws Exception {\\n    ZKConfig zkConfig = new ZKConfig();\\n    ClientX509Util clientX509Util = new ClientX509Util();\\n    zkConfig.setProperty(clientX509Util.getSslContextSupplierClassProperty(), String.class.getCanonicalName());\\n    clientX509Util.createSSLContext(zkConfig);\\n}\\n@Test\\npublic void testCreateSSLContext_validCustomSSLContextClass() throws Exception {\\n    ZKConfig zkConfig = new ZKConfig();\\n    ClientX509Util clientX509Util = new ClientX509Util();\\n    zkConfig.setProperty(clientX509Util.getSslContextSupplierClassProperty(), SslContextSupplier.class.getName());\\n    final SSLContext sslContext = clientX509Util.createSSLContext(zkConfig);\\n    Assert.assertEquals(SSLContext.getDefault(), sslContext);\\n}\\nprivate static void forceClose(Socket s) {\\n    if (s == null || s.isClosed()) {\\n        return;\\n    }\\n    try {\\n        s.close();\\n    } catch (IOException e) {\\n    }\\n}\\n// Warning: this will reset the x509Util\\nprivate void setCustomCipherSuites() {\\n    System.setProperty(x509Util.getCipherSuitesProperty(), customCipherSuites[0] + \",\" + customCipherSuites[1]);\\n    // remember to close old instance before replacing it\\n    x509Util.close();\\n    x509Util = new ClientX509Util();\\n}\\npublic static class SslContextSupplier implements Supplier<SSLContext> {\\n\\n    @Override\\n    public SSLContext get() {\\n        try {\\n            return SSLContext.getDefault();\\n        } catch (NoSuchAlgorithmException e) {\\n            throw new RuntimeException(e);\\n        }\\n    }\\n}\\n}\\n\\n\\nSummary:', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Given a Git diff and the relevant source code, write a concise summary of the code changes in a way that a non-technical person can understand. Summarize in maximum three concise sentences. \\n\\nAvoid adding any additional comments or annotations to the summary.\\n\\nGit diff:\\ndiff --git a/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java b/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java\\nindex 52419ddf..b567f1ea 100644\\n--- a/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java\\n+++ b/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java\\n@@ -23,9 +23,18 @@ import opennlp.tools.util.Sequence;\\n /**\\n  * A thread-safe version of the POSTaggerME. Using it is completely transparent. You can use it in\\n  * a single-threaded context as well, it only incurs a minimal overhead.\\n+ * <p>\\n+ * Note, however, that this implementation uses a {@link ThreadLocal}. Although the implementation is\\n+ * lightweight because the model is not duplicated, if you have many long-running threads,\\n+ * you may run into memory problems.\\n+ * </p>\\n+ * <p>\\n+ * Be careful when using this in a Jakarta EE application, for example.\\n+ * </p>\\n+ * The user is responsible for clearing the {@link ThreadLocal}.\\n  */\\n @ThreadSafe\\n-public class ThreadSafePOSTaggerME implements POSTagger {\\n+public class ThreadSafePOSTaggerME implements POSTagger, AutoCloseable {\\n \\n   private final POSModel model;\\n \\n@@ -64,4 +73,9 @@ public class ThreadSafePOSTaggerME implements POSTagger {\\n   public Sequence[] topKSequences(String[] sentence, Object[] additionaContext) {\\n     return getTagger().topKSequences(sentence, additionaContext);\\n   }\\n+\\n+  @Override\\n+  public void close() {\\n+    threadLocal.remove();\\n+  }\\n }\\ndiff --git a/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java b/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java\\nindex 99abc6fb..17ea14e8 100644\\n--- a/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java\\n+++ b/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java\\n@@ -24,16 +24,21 @@ import opennlp.tools.util.Span;\\n  * A thread-safe version of SentenceDetectorME. Using it is completely transparent. You can use it in\\n  * a single-threaded context as well, it only incurs a minimal overhead.\\n  * <p>\\n- * Note, however, that this implementation uses a ThreadLocal. Although the implementation is\\n- * lightweight as the model is not duplicated, if you have many long-running threads, you may run\\n- * into memory issues. Be careful when you use this in a JEE application, for example.\\n+ * Note, however, that this implementation uses a {@link ThreadLocal}. Although the implementation is\\n+ * lightweight because the model is not duplicated, if you have many long-running threads,\\n+ * you may run into memory problems.\\n+ * </p>\\n+ * <p>\\n+ * Be careful when using this in a Jakarta EE application, for example.\\n+ * </p>\\n+ * The user is responsible for clearing the {@link ThreadLocal}.\\n  */\\n @ThreadSafe\\n-public class ThreadSafeSentenceDetectorME implements SentenceDetector {\\n+public class ThreadSafeSentenceDetectorME implements SentenceDetector, AutoCloseable {\\n \\n   private final SentenceModel model;\\n \\n-  private final ThreadLocal<SentenceDetectorME> sentenceDetectorThreadLocal =\\n+  private final ThreadLocal<SentenceDetectorME> threadLocal =\\n       new ThreadLocal<>();\\n \\n   public ThreadSafeSentenceDetectorME(SentenceModel model) {\\n@@ -43,10 +48,10 @@ public class ThreadSafeSentenceDetectorME implements SentenceDetector {\\n \\n   // If a thread-local version exists, return it. Otherwise, create, then return.\\n   private SentenceDetectorME getSD() {\\n-    SentenceDetectorME sd = sentenceDetectorThreadLocal.get();\\n+    SentenceDetectorME sd = threadLocal.get();\\n     if (sd == null) {\\n       sd = new SentenceDetectorME(model);\\n-      sentenceDetectorThreadLocal.set(sd);\\n+      threadLocal.set(sd);\\n     }\\n     return sd;\\n   }\\n@@ -64,4 +69,9 @@ public class ThreadSafeSentenceDetectorME implements SentenceDetector {\\n   public Span[] sentPosDetect(CharSequence s) {\\n     return getSD().sentPosDetect(s);\\n   }\\n+\\n+  @Override\\n+  public void close() {\\n+    threadLocal.remove();\\n+  }\\n }\\ndiff --git a/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java b/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java\\nindex b92dd5e0..3ebbd1e3 100644\\n--- a/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java\\n+++ b/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java\\n@@ -23,13 +23,22 @@ import opennlp.tools.util.Span;\\n /**\\n  * A thread-safe version of TokenizerME. Using it is completely transparent. You can use it in\\n  * a single-threaded context as well, it only incurs a minimal overhead.\\n+ * <p>\\n+ * Note, however, that this implementation uses a {@link ThreadLocal}. Although the implementation is\\n+ * lightweight because the model is not duplicated, if you have many long-running threads,\\n+ * you may run into memory problems.\\n+ * </p>\\n+ * <p>\\n+ * Be careful when using this in a Jakarta EE application, for example.\\n+ * </p>\\n+ * The user is responsible for clearing the {@link ThreadLocal}.\\n  */\\n @ThreadSafe\\n-public class ThreadSafeTokenizerME implements Tokenizer {\\n+public class ThreadSafeTokenizerME implements Tokenizer, AutoCloseable {\\n \\n   private final TokenizerModel model;\\n \\n-  private final ThreadLocal<TokenizerME> tokenizerThreadLocal = new ThreadLocal<>();\\n+  private final ThreadLocal<TokenizerME> threadLocal = new ThreadLocal<>();\\n \\n   public ThreadSafeTokenizerME(TokenizerModel model) {\\n     super();\\n@@ -37,10 +46,10 @@ public class ThreadSafeTokenizerME implements Tokenizer {\\n   }\\n \\n   private TokenizerME getTokenizer() {\\n-    TokenizerME tokenizer = tokenizerThreadLocal.get();\\n+    TokenizerME tokenizer = threadLocal.get();\\n     if (tokenizer == null) {\\n       tokenizer = new TokenizerME(model);\\n-      tokenizerThreadLocal.set(tokenizer);\\n+      threadLocal.set(tokenizer);\\n     }\\n     return tokenizer;\\n   }\\n@@ -58,4 +67,9 @@ public class ThreadSafeTokenizerME implements Tokenizer {\\n   public double[] getProbabilities() {\\n     return getTokenizer().getTokenProbabilities();\\n   }\\n+\\n+  @Override\\n+  public void close() {\\n+    threadLocal.remove();\\n+  }\\n }\\n\\n\\nSource code:\\nopennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java (Before)\\npublic class ThreadSafePOSTaggerME implements POSTagger {\\nprivate final POSModel model;\\n@Override\\npublic Sequence[] topKSequences(String[] sentence, Object[] additionaContext) {\\n    return getTagger().topKSequences(sentence, additionaContext);\\n}\\n}\\n\\nopennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java (After)\\npublic class ThreadSafePOSTaggerME implements POSTagger, AutoCloseable {\\nprivate final POSModel model;\\n@Override\\npublic Sequence[] topKSequences(String[] sentence, Object[] additionaContext) {\\n    return getTagger().topKSequences(sentence, additionaContext);\\n}\\n@Override\\npublic void close() {\\n    threadLocal.remove();\\n}\\n}\\n\\nopennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java (Before)\\npublic class ThreadSafeSentenceDetectorME implements SentenceDetector {\\nprivate final SentenceModel model;\\nprivate final ThreadLocal<SentenceDetectorME> sentenceDetectorThreadLocal = new ThreadLocal<>();\\npublic ThreadSafeSentenceDetectorME(SentenceModel model) {\\n    super();\\n    this.model = model;\\n}\\n// If a thread-local version exists, return it. Otherwise, create, then return.\\nprivate SentenceDetectorME getSD() {\\n    SentenceDetectorME sd = sentenceDetectorThreadLocal.get();\\n    if (sd == null) {\\n        sd = new SentenceDetectorME(model);\\n        sentenceDetectorThreadLocal.set(sd);\\n    }\\n    return sd;\\n}\\n@Override\\npublic Span[] sentPosDetect(CharSequence s) {\\n    return getSD().sentPosDetect(s);\\n}\\n}\\n\\nopennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java (After)\\npublic class ThreadSafeSentenceDetectorME implements SentenceDetector, AutoCloseable {\\nprivate final SentenceModel model;\\nprivate final ThreadLocal<SentenceDetectorME> threadLocal = new ThreadLocal<>();\\npublic ThreadSafeSentenceDetectorME(SentenceModel model) {\\n    super();\\n    this.model = model;\\n}\\n// If a thread-local version exists, return it. Otherwise, create, then return.\\nprivate SentenceDetectorME getSD() {\\n    SentenceDetectorME sd = threadLocal.get();\\n    if (sd == null) {\\n        sd = new SentenceDetectorME(model);\\n        threadLocal.set(sd);\\n    }\\n    return sd;\\n}\\n@Override\\npublic Span[] sentPosDetect(CharSequence s) {\\n    return getSD().sentPosDetect(s);\\n}\\n@Override\\npublic void close() {\\n    threadLocal.remove();\\n}\\n}\\n\\nopennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java (Before)\\npublic class ThreadSafeTokenizerME implements Tokenizer {\\nprivate final TokenizerModel model;\\nprivate final ThreadLocal<TokenizerME> tokenizerThreadLocal = new ThreadLocal<>();\\npublic ThreadSafeTokenizerME(TokenizerModel model) {\\n    super();\\n    this.model = model;\\n}\\nprivate TokenizerME getTokenizer() {\\n    TokenizerME tokenizer = tokenizerThreadLocal.get();\\n    if (tokenizer == null) {\\n        tokenizer = new TokenizerME(model);\\n        tokenizerThreadLocal.set(tokenizer);\\n    }\\n    return tokenizer;\\n}\\npublic double[] getProbabilities() {\\n    return getTokenizer().getTokenProbabilities();\\n}\\n}\\n\\nopennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java (After)\\npublic class ThreadSafeTokenizerME implements Tokenizer, AutoCloseable {\\nprivate final TokenizerModel model;\\nprivate final ThreadLocal<TokenizerME> threadLocal = new ThreadLocal<>();\\npublic ThreadSafeTokenizerME(TokenizerModel model) {\\n    super();\\n    this.model = model;\\n}\\nprivate TokenizerME getTokenizer() {\\n    TokenizerME tokenizer = threadLocal.get();\\n    if (tokenizer == null) {\\n        tokenizer = new TokenizerME(model);\\n        threadLocal.set(tokenizer);\\n    }\\n    return tokenizer;\\n}\\npublic double[] getProbabilities() {\\n    return getTokenizer().getTokenProbabilities();\\n}\\n@Override\\npublic void close() {\\n    threadLocal.remove();\\n}\\n}\\n\\n\\nSummary:', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B098980>\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC022A0> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B021950>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC022A0> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B020E10>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC022A0> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B055350>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B055480>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B0527B0>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A7FE63680>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A7FE63790>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B076150>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B076350>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC022A0> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC022A0> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC022A0> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B00A300>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B00B980>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC022A0> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC022A0> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC022A0> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC022A0> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B089EF0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B089FD0>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A7FEC7520>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A7FE51550>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A7FE51250>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A7FE073E0>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A7FE07490>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B00FCF0>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"GET /info HTTP/11\" 200 672\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf7248fcf879-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf7248defd8d-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf724a165fc8-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf728f863dde-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf727f9237b5-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf720cd3899e-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf725f01400e-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf71fdcff900-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf727d22fdff-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf7208e6fd17-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf7248defd8d-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf728f863dde-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf724a165fc8-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf727d22fdff-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf720cd3899e-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf71fdcff900-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf725f01400e-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf727f9237b5-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf7248fcf879-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf7208e6fd17-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:faiss.loader:Environment variable FAISS_OPT_LEVEL is not set, so let's pick the instruction set according to the current CPU\n",
      "INFO:faiss.loader:Loading faiss with AVX512 support.\n",
      "INFO:faiss.loader:Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000024A0B00F4C0>, 'json_data': {'input': [[32, 502, 1493, 2082, 369, 45571, 24295, 927, 60179, 574, 3779, 11, 323, 279, 2082, 1457, 3872, 264, 1595, 1738, 39470, 15946, 1378, 63, 994, 264, 30174, 5784, 14865, 4245, 311, 459, 33316, 2085, 6300, 24645, 13, 362, 1296, 1162, 574, 1101, 3779, 311, 10356, 420, 7865, 13]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000024A68853D80>, 'json_data': {'input': [[791, 2082, 4442, 21736, 21686, 1268, 13643, 1052, 13006, 527, 18073, 323, 7999, 1862, 369, 14794, 3626, 311, 10299, 1988, 13006, 6089, 304, 3738, 26350, 13]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000024A68853E20>, 'json_data': {'input': [[791, 2082, 4442, 18885, 1984, 11850, 555, 23391, 41537, 6743, 527, 2744, 24411, 311, 4823, 3645, 323, 555, 18899, 279, 6727, 315, 279, 1984, 46588, 3196, 389, 279, 1984, 9284, 13]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000024A68853EC0>, 'json_data': {'input': [[791, 2082, 2349, 26420, 430, 279, 1515, 1212, 4445, 374, 1193, 6177, 422, 433, 374, 539, 2736, 743, 477, 422, 279, 502, 4445, 374, 7191, 1109, 279, 1510, 832, 11, 27252, 433, 505, 1694, 743, 311, 459, 6931, 907, 13, 362, 502, 1296, 574, 3779, 311, 10356, 279, 37166, 315, 50917, 477, 23329, 21282, 304, 8870, 5942, 13]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000024A0B00F600>, 'json_data': {'input': [[791, 2082, 4442, 19678, 264, 1749, 311, 10134, 279, 15356, 66206, 18, 5286, 734, 11, 10923, 279, 9659, 315, 264, 3284, 220, 845, 55052, 1401, 505, 264, 2728, 4037, 13, 23212, 11, 279, 1595, 80329, 2810, 63, 538, 1457, 11815, 34537, 1595, 80329, 63, 6302, 6089, 2085, 21939, 459, 4788, 13, 578, 1749, 5144, 323, 3977, 5144, 5552, 311, 279, 5286, 734, 1051, 1101, 37065, 369, 29237, 13]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000024A68853420>, 'json_data': {'input': [[791, 2082, 4442, 6177, 279, 1595, 39, 535, 5913, 11807, 63, 311, 3881, 17071, 86292, 5718, 2391, 264, 3230, 10474, 4619, 315, 279, 20406, 10474, 13, 23212, 11, 264, 8767, 12305, 1296, 369, 17071, 86292, 574, 312, 55292, 323, 1202, 3319, 925, 574, 44899, 13]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000024A68852B60>, 'json_data': {'input': [[791, 2082, 4442, 5471, 279, 15676, 315, 264, 3230, 18121, 311, 5766, 15676, 1202, 12102, 11, 5155, 264, 86205, 304, 264, 6471, 1405, 37686, 527, 3779, 311, 264, 1160, 315, 3600, 11, 323, 923, 264, 502, 1296, 1162, 369, 264, 15945, 3876, 6149, 6597, 11850, 20157, 13]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000024A68C8D440>, 'json_data': {'input': [[791, 2082, 1457, 6276, 38938, 264, 2587, 26384, 2317, 19353, 538, 1555, 264, 6683, 3424, 11, 28462, 810, 19303, 26384, 2317, 9886, 13, 578, 2349, 1101, 5764, 7177, 311, 10356, 279, 15293, 449, 2764, 323, 8482, 2587, 26384, 2317, 6989, 13]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000024A68C8D4E0>, 'json_data': {'input': [[791, 2082, 1457, 5764, 264, 1595, 5669, 63, 1749, 304, 2380, 6989, 29754, 6998, 26747, 3019, 10322, 7614, 7964, 1595, 6998, 26747, 85664, 32706, 7614, 7964, 323, 1595, 6998, 26747, 38534, 7614, 33981, 311, 2867, 279, 1595, 6998, 7469, 63, 3977, 11, 902, 8779, 10299, 5044, 10648, 994, 14892, 449, 1690, 14906, 13, 23212, 11, 1521, 6989, 1457, 4305, 279, 1595, 13556, 8084, 481, 63, 3834, 11, 10923, 1124, 311, 387, 1511, 304, 1456, 27281, 90911, 12518, 369, 17392, 5211, 6373, 13]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000024A68853F60>, 'json_data': {'input': [[791, 2082, 1457, 12722, 13777, 2373, 9246, 449, 1579, 68806, 49473, 5885, 11, 23391, 6300, 23115, 13, 1102, 1101, 5764, 502, 7177, 311, 10356, 430, 19016, 527, 2744, 6646, 7191, 1109, 2536, 12, 57550, 323, 2536, 7826, 5901, 5885, 13]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68C8CC30>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A10739520>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688AF950>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688AF650>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC02060> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC02060> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC02060> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC02060> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B84B0>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B8600>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC02060> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC02060> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B0CDC10>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B0CDD30>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B19A0>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B1A90>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC02060> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC02060> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC02060> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC02060> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68F468F0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B08C0>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B0140>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68F46260>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68F45FE0>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B07D0>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B0690>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B17C0>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B18B0>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B1A40>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'75'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-5fd58fb6cb-qcrlq'), (b'x-envoy-upstream-service-time', b'49'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999955'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'req_ae203e90fa9cb45b286bb1c057a69c62'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=fScxmoaCroBUwPCaHkwpOj5fEvMeIGDbUltiFb2lM9s-1740654836-1.0.1.1-nE1M0K4i0OB5wPxviLRJTEe3GZbNRISfm_X.Z24rE5UwqX6F6idTFazqsaAV2wyjdvuwmYFI283RsHcdTOYmJg; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=_gyfelb4dyQj4N2ewWEezJtcmronXmiFLjDMJI96.zY-1740654836118-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf93ca9b4c6b-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Thu, 27 Feb 2025 11:13:56 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '75'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-5fd58fb6cb-qcrlq'), ('x-envoy-upstream-service-time', '49'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999955'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '2ms'), ('x-request-id', 'req_ae203e90fa9cb45b286bb1c057a69c62'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=fScxmoaCroBUwPCaHkwpOj5fEvMeIGDbUltiFb2lM9s-1740654836-1.0.1.1-nE1M0K4i0OB5wPxviLRJTEe3GZbNRISfm_X.Z24rE5UwqX6F6idTFazqsaAV2wyjdvuwmYFI283RsHcdTOYmJg; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=_gyfelb4dyQj4N2ewWEezJtcmronXmiFLjDMJI96.zY-1740654836118-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9187bf93ca9b4c6b-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_ae203e90fa9cb45b286bb1c057a69c62\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8400\\nIssue Summary: Fix pruning partitions with pushed transitive predicates\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nSee {{TestHivePartitionPruning.prunePartitionsBasedOnTransitivePredicates()}} test for details.\\n\\n\\n\\nThe issue occurs for queries like these:\\n\\n{code:sql}\\n\\nSELECT * FROM hive.partition_pruning_test t1 \\n\\nJOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \\n\\nWHERE t2.`e` IS NOT NULL AND t1.`d` = 1\\n\\n{code}\\n\\n\\n\\nThe expected behavior is to create additional filters based on the existing filters and join conditions. We have a {{TRANSITIVE_CLOSURE}} planning phase, which is responsible for such query transformations, but Drill pushes down filters from the WHERE condition before that phase, so the optimization is not performed.\\n\\n\\n\\nIdeally, we should move rules from the {{TRANSITIVE_CLOSURE}} phase to the {{LOGICAL}} phase so that the planner will choose the most optimal plan, but it wouldn\\'t help until CALCITE-1048 is fixed (it is required to pull predicates when three has {{RelSubset}} nodes).\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'87'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-5fd58fb6cb-vzrjf'), (b'x-envoy-upstream-service-time', b'59'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2997'), (b'x-ratelimit-remaining-tokens', b'999905'), (b'x-ratelimit-reset-requests', b'41ms'), (b'x-ratelimit-reset-tokens', b'5ms'), (b'x-request-id', b'req_804045fcd8c97aee3517d9288c367e9f'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Q4AprQrSkxLViJuAAiKEbFa78RseqRdVGL6L7ZkVDzo-1740654836-1.0.1.1-lUA.qB52BnS2QlIXjUSFYtIpjmk5SoOZbTfrt_hcEuqzpVdssdCRzprSvVob4VpryFEX9QOXsiOUWAOcx0z5Ww; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=LieZ7902tSOYnJpRx8DOOpAJgneWCtjRzjcmLXljPVc-1740654836121-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf93bf559fb3-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8381\\nIssue Summary: Add support for filtered aggregate calls\\nIssue Type: New Feature\\nPriority: Major\\n\\nDescription:\\nCurrently, Drill ignores filters for filtered aggregate calls and returns incorrect results.\\n\\nHere is the example query for which Drill will return incorrect results:\\n\\n{code:sql}\\n\\nSELECT count(n_name) FILTER(WHERE n_regionkey = 1) AS nations_count_in_1_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 2) AS nations_count_in_2_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 3) AS nations_count_in_3_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 4) AS nations_count_in_4_region,\\n\\ncount(n_name) FILTER(WHERE n_regionkey = 0) AS nations_count_in_0_region\\n\\nFROM cp.`tpch/nation.parquet`\\n\\n{code}\\n\\n{noformat}\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| nations_count_in_1_region | nations_count_in_2_region | nations_count_in_3_region | nations_count_in_4_region | nations_count_in_0_region |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| 25                        | 25                        | 25                        | 25                        | 25                        |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n{noformat}\\n\\nBut the correct result is\\n\\n{noformat}\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| nations_count_in_1_region | nations_count_in_2_region | nations_count_in_3_region | nations_count_in_4_region | nations_count_in_0_region |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n| 5                         | 5                         | 5                         | 5                         | 5                         |\\n\\n+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\\n\\n{noformat}\\n\\nSide note:\\n\\nThe query above could be rewritten using PIVOT:\\n\\n{code:sql}\\n\\nSELECT `1` nations_count_in_1_region, `2` nations_count_in_2_region, `3` nations_count_in_3_region, `4` nations_count_in_4_region, `0` nations_count_in_0_region\\n\\nFROM (SELECT n_name, n_regionkey FROM cp.`tpch/nation.parquet`) \\n\\nPIVOT(count(n_name) FOR n_regionkey IN (0, 1, 2, 3, 4))\\n\\n{code}\\n\\nAnd will return correct results when this issue is fixed and Calcite is updated to 1.33.0\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\nindex c021ebca1..1ce138c0e 100644\\n--- a/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n+++ b/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveStoragePlugin.java\\n@@ -192,7 +192,7 @@ public class HiveStoragePlugin extends AbstractStoragePlugin {\\n   @Override\\n   public Set<StoragePluginOptimizerRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {\\n     switch (phase) {\\n-      case LOGICAL:\\n+      case PARTITION_PRUNING:\\n         final String defaultPartitionValue = hiveConf.get(ConfVars.DEFAULTPARTITIONNAME.varname);\\n         ImmutableSet.Builder<StoragePluginOptimizerRule> ruleBuilder = ImmutableSet.builder();\\n         ruleBuilder.add(HivePushPartitionFilterIntoScan.getFilterOnProject(optimizerContext, defaultPartitionValue));\\ndiff --git a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\nindex 62a2c136a..608aaf8d1 100644\\n--- a/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n+++ b/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/TestHivePartitionPruning.java\\n@@ -28,7 +28,6 @@ import org.apache.drill.exec.planner.physical.PlannerSettings;\\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\\n import org.junit.AfterClass;\\n import org.junit.BeforeClass;\\n-import org.junit.Ignore;\\n import org.junit.Test;\\n import org.junit.experimental.categories.Category;\\n \\n@@ -163,11 +162,10 @@ public class TestHivePartitionPruning extends HiveTestBase {\\n   }\\n \\n   @Test // DRILL-6173\\n-  @Ignore(\"DRILL-8400\")\\n   public void prunePartitionsBasedOnTransitivePredicates() throws Exception {\\n-    String query = String.format(\"SELECT * FROM hive.partition_pruning_test t1 \" +\\n+    String query = \"SELECT * FROM hive.partition_pruning_test t1 \" +\\n             \"JOIN hive.partition_with_few_schemas t2 ON t1.`d` = t2.`d` AND t1.`e` = t2.`e` \" +\\n-            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\");\\n+            \"WHERE t2.`e` IS NOT NULL AND t1.`d` = 1\";\\n \\n     int actualRowCount = testSql(query);\\n     int expectedRowCount = 450;\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: DRILL-8513\\nIssue Summary: Right Hash Join with empty Left table ruturns 0 result\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nDrill returns no results on the right Hash Join if the probe(left) table is empty.\\n\\n\\n\\nThe simplest way to reproduce the issue:\\n\\n\\n\\n1.To force Drill not to use merge join and use the hash join operator instead:\\n\\n{code:java}\\n\\nalter session set planner.enable_mergejoin = false;\\n\\nalter session set planner.enable_nestedloopjoin= false; {code}\\n\\n2. Disable join order optimization to prevent Drill from flipping join tables:\\n\\n{code:java}\\n\\nalter session set planner.enable_join_optimization = false;  {code}\\n\\n3. Execute a query with empty left table outcome:\\n\\n{code:java}\\n\\nSELECT *\\n\\nFROMÂ\\xa0\\n\\nÂ\\xa0 Â\\xa0 (SELECT * FROM (VALUES (1, \\'Max\\', 28),Â\\xa0\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0(2, \\'Jane\\', 32),\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0(3, \\'Saymon\\', 29)\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0) AS users(id, name, age)\\n\\nÂ\\xa0 Â\\xa0 WHERE false\\n\\nÂ\\xa0 Â\\xa0 ) AS users\\n\\nRIGHT JOINÂ\\xa0\\n\\nÂ\\xa0 Â\\xa0 (VALUES (1, \\'Engineer\\'),Â\\xa0\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 (2, \\'Doctor\\'),Â\\xa0\\n\\nÂ\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 Â\\xa0 (3, \\'Teacher\\')\\n\\nÂ\\xa0 Â\\xa0 ) AS job(id, title)\\n\\nON users.id = job.idÂ\\xa0{code}\\n\\nExpected result is:\\n\\n||id||name||age||id0||title||\\n\\n|null|null|null|1|Engineer|\\n\\n|null|null|null|2|Doctor|\\n\\n|null|null|null|3|Teacher|\\n\\n\\n\\nBut we get 0 rows.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Thu, 27 Feb 2025 11:13:56 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '87'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-5fd58fb6cb-vzrjf'), ('x-envoy-upstream-service-time', '59'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2997'), ('x-ratelimit-remaining-tokens', '999905'), ('x-ratelimit-reset-requests', '41ms'), ('x-ratelimit-reset-tokens', '5ms'), ('x-request-id', 'req_804045fcd8c97aee3517d9288c367e9f'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=Q4AprQrSkxLViJuAAiKEbFa78RseqRdVGL6L7ZkVDzo-1740654836-1.0.1.1-lUA.qB52BnS2QlIXjUSFYtIpjmk5SoOZbTfrt_hcEuqzpVdssdCRzprSvVob4VpryFEX9QOXsiOUWAOcx0z5Ww; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=LieZ7902tSOYnJpRx8DOOpAJgneWCtjRzjcmLXljPVc-1740654836121-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9187bf93bf559fb3-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_804045fcd8c97aee3517d9288c367e9f\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'74'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-7b44d88999-4d2w2'), (b'x-envoy-upstream-service-time', b'37'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999960'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'req_81985e30e25538754429c417cda18095'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=PMsBsRFlWTTwVJihkC8pCedwwEPYBCnPdIAhAiTqefc-1740654836-1.0.1.1-XNNZ1UEfni5HK4IAx9cMPZ5FbIAYN8CdYuIKtiQY1MFwy.PeRJMbvhsjBduBzOUJ84.FrAKWfqmf8EAlcZgnRA; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=RCq9luHgqO60.YTEYZc8Fbjo8RVq0gIyqgykCFylxdQ-1740654836159-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf93cf4e3da5-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'54'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-5765dbdcc8-58wj8'), (b'x-envoy-upstream-service-time', b'31'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2998'), (b'x-ratelimit-remaining-tokens', b'999973'), (b'x-ratelimit-reset-requests', b'33ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'req_165c2ab02803b24f42dd97e1ae712a00'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=A9FMbjdG_gLFks2Mbl.pGm89h0SIS22yERIyldYVs6M-1740654836-1.0.1.1-BEzWhVFr430cb2gFj2for4CzQhN2byer9LNfluulzJQbJOAGDbUUN78pC9UgDaX9z9_QtyZl1MPscXJ.DNFatg; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=QN0JgNyqnCnsAENp60W_oWcG1vyeN4qppukUJ3M8w1E-1740654836139-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf93b8c59fe5-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java\\nindex e1b6f9352..ca2906d88 100644\\n--- a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java\\n+++ b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java\\n@@ -26,6 +26,7 @@ import java.util.Map;\\n \\n import javax.jms.JMSException;\\n \\n+import com.thoughtworks.xstream.io.json.JsonHierarchicalStreamDriver;\\n import org.apache.activemq.advisory.AdvisorySupport;\\n import org.apache.activemq.broker.BrokerContext;\\n import org.apache.activemq.broker.BrokerContextAware;\\n@@ -102,6 +103,7 @@ public class JmsFrameTranslator extends LegacyFrameTranslator implements\\n     @Override\\n     public StompFrame convertMessage(ProtocolConverter converter,\\n             ActiveMQMessage message) throws IOException, JMSException {\\n+\\n         if (message.getDataStructureType() == ActiveMQObjectMessage.DATA_STRUCTURE_TYPE) {\\n             StompFrame command = new StompFrame();\\n             command.setAction(Stomp.Responses.MESSAGE);\\n@@ -153,6 +155,10 @@ public class JmsFrameTranslator extends LegacyFrameTranslator implements\\n             FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(\\n                     converter, message, command, this);\\n \\n+            if (!headers.containsKey(Stomp.Headers.TRANSFORMATION)) {\\n+                headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_ADVISORY_JSON.toString());\\n+            }\\n+\\n             if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_XML.toString())) {\\n                 headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_ADVISORY_XML.toString());\\n             } else if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_JSON.toString())) {\\n@@ -274,4 +280,16 @@ public class JmsFrameTranslator extends LegacyFrameTranslator implements\\n     public void setBrokerContext(BrokerContext brokerContext) {\\n         this.brokerContext = brokerContext;\\n     }\\n+\\n+    /**\\n+     * Return an Advisory message as a JSON formatted string\\n+     * @param ds\\n+     * @return\\n+     */\\n+    protected String marshallAdvisory(final DataStructure ds) {\\n+        XStream xstream = new XStream(new JsonHierarchicalStreamDriver());\\n+        xstream.setMode(XStream.NO_REFERENCES);\\n+        xstream.aliasPackage(\"\", \"org.apache.activemq.command\");\\n+        return xstream.toXML(ds);\\n+    }\\n }\\ndiff --git a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java\\nindex 1d826a44e..8cfa1219e 100644\\n--- a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java\\n+++ b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java\\n@@ -16,25 +16,19 @@\\n  */\\n package org.apache.activemq.transport.stomp;\\n \\n-import java.io.DataOutputStream;\\n-import java.io.IOException;\\n-import java.util.HashMap;\\n-import java.util.Map;\\n-\\n-import javax.jms.Destination;\\n-import javax.jms.JMSException;\\n-\\n-import org.apache.activemq.advisory.AdvisorySupport;\\n import org.apache.activemq.command.ActiveMQBytesMessage;\\n import org.apache.activemq.command.ActiveMQDestination;\\n import org.apache.activemq.command.ActiveMQMessage;\\n import org.apache.activemq.command.ActiveMQTextMessage;\\n-import org.apache.activemq.command.DataStructure;\\n import org.apache.activemq.util.ByteArrayOutputStream;\\n import org.apache.activemq.util.ByteSequence;\\n \\n-import com.thoughtworks.xstream.XStream;\\n-import com.thoughtworks.xstream.io.json.JsonHierarchicalStreamDriver;\\n+import javax.jms.Destination;\\n+import javax.jms.JMSException;\\n+import java.io.DataOutputStream;\\n+import java.io.IOException;\\n+import java.util.HashMap;\\n+import java.util.Map;\\n \\n /**\\n  * Implements ActiveMQ 4.0 translations\\n@@ -127,15 +121,8 @@ public class LegacyFrameTranslator implements FrameTranslator {\\n \\n             headers.put(Stomp.Headers.CONTENT_LENGTH, Integer.toString(data.length));\\n             command.setContent(data);\\n-        } else if (message.getDataStructureType() == ActiveMQMessage.DATA_STRUCTURE_TYPE &&\\n-                AdvisorySupport.ADIVSORY_MESSAGE_TYPE.equals(message.getType())) {\\n-\\n-            FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(\\n-                    converter, message, command, this);\\n-\\n-            String body = marshallAdvisory(message.getDataStructure());\\n-            command.setContent(body.getBytes(\"UTF-8\"));\\n         }\\n+\\n         return command;\\n     }\\n \\n@@ -212,15 +199,5 @@ public class LegacyFrameTranslator implements FrameTranslator {\\n         }\\n     }\\n \\n-    /**\\n-     * Return an Advisory message as a JSON formatted string\\n-     * @param ds\\n-     * @return\\n-     */\\n-    protected String marshallAdvisory(final DataStructure ds) {\\n-        XStream xstream = new XStream(new JsonHierarchicalStreamDriver());\\n-        xstream.setMode(XStream.NO_REFERENCES);\\n-        xstream.aliasPackage(\"\", \"org.apache.activemq.command\");\\n-        return xstream.toXML(ds);\\n-    }\\n+\\n }\\ndiff --git a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java\\nindex a6a22f17f..f31aad1b6 100644\\n--- a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java\\n+++ b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java\\n@@ -31,6 +31,7 @@ import java.util.concurrent.atomic.AtomicBoolean;\\n import javax.jms.JMSException;\\n \\n import org.apache.activemq.ActiveMQPrefetchPolicy;\\n+import org.apache.activemq.advisory.AdvisorySupport;\\n import org.apache.activemq.broker.BrokerContext;\\n import org.apache.activemq.broker.BrokerContextAware;\\n import org.apache.activemq.command.ActiveMQDestination;\\n@@ -200,19 +201,28 @@ public class ProtocolConverter {\\n     }\\n \\n     protected FrameTranslator findTranslator(String header) {\\n+        return findTranslator(header, null);\\n+    }\\n+\\n+    protected FrameTranslator findTranslator(String header, ActiveMQDestination destination) {\\n         FrameTranslator translator = frameTranslator;\\n         try {\\n             if (header != null) {\\n                 translator = (FrameTranslator) FRAME_TRANSLATOR_FINDER\\n                         .newInstance(header);\\n-                if (translator instanceof BrokerContextAware) {\\n-                    ((BrokerContextAware)translator).setBrokerContext(brokerContext);\\n+            } else {\\n+                if (destination != null && AdvisorySupport.isAdvisoryTopic(destination)) {\\n+                    translator = new JmsFrameTranslator();\\n                 }\\n             }\\n         } catch (Exception ignore) {\\n             // if anything goes wrong use the default translator\\n         }\\n \\n+        if (translator instanceof BrokerContextAware) {\\n+            ((BrokerContextAware)translator).setBrokerContext(brokerContext);\\n+        }\\n+\\n         return translator;\\n     }\\n \\n@@ -879,7 +889,7 @@ public class ProtocolConverter {\\n         if (ignoreTransformation == true) {\\n             return frameTranslator.convertMessage(this, message);\\n         } else {\\n-            return findTranslator(message.getStringProperty(Stomp.Headers.TRANSFORMATION)).convertMessage(this, message);\\n+            return findTranslator(message.getStringProperty(Stomp.Headers.TRANSFORMATION), message.getDestination()).convertMessage(this, message);\\n         }\\n     }\\n \\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: AMQ-7068\\nIssue Summary: Advisory messages are empty when received with a AMQP subscription\\nIssue Type: New Feature\\nPriority: Minor\\n\\nDescription:\\nWe are currently\\xa0moving from OpenWire to\\xa0AMQP\\xa0with .NET Library amqpnetlite\\xa0([https://github.com/Azure/amqpnetlite)]\\xa0to communicate. So far most things work fine, but we actively used and need the advisory messages in order to recognize if other clients disconnect for example.\\n\\n\\n\\nAccessing the advisory messages through the topic is not the problem, but the body is null for the\\xa0ActiveMQ.Advisory.Connection topic. There are some properties set, but no body set and I\\'m not able to find any important information, like the RemoveInfo. I attached a few screenshots from debugger.\\n\\n\\n\\nTo be honest, I don\\'t know if this is the desired behavior, but I think if there are messages on the advisory topic they should be useful.\\n\\n\\n\\nI know that a byte representation wouldn\\'t be that useful, but you could present the information in json or xml format, like in Stomp? (https://issues.apache.org/jira/browse/AMQ-2098)\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'79'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-5765dbdcc8-vmcds'), (b'x-envoy-upstream-service-time', b'58'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999951'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'req_19c72129b9c9cdc65b38109455f70201'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=pjODjZwrO4a7YdoVbXHFpv0wmlYWW5IznM8aQkiv.Ns-1740654836-1.0.1.1-c9sMZ1r0OIgNLm5D51slOIhYk8xfBO5GhfDSCLI8O5oyiaffnxtmvgChwhB.m7e5V5m4cnFc0bWLV_ExgoLyuA; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=l8wgu8CdjWkuFK5GYx9h8e93AV9qg4bG5QamSOS4G9g-1740654836162-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf93cd0e81b9-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java\\nindex e1b6f9352..ca2906d88 100644\\n--- a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java\\n+++ b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java\\n@@ -26,6 +26,7 @@ import java.util.Map;\\n \\n import javax.jms.JMSException;\\n \\n+import com.thoughtworks.xstream.io.json.JsonHierarchicalStreamDriver;\\n import org.apache.activemq.advisory.AdvisorySupport;\\n import org.apache.activemq.broker.BrokerContext;\\n import org.apache.activemq.broker.BrokerContextAware;\\n@@ -102,6 +103,7 @@ public class JmsFrameTranslator extends LegacyFrameTranslator implements\\n     @Override\\n     public StompFrame convertMessage(ProtocolConverter converter,\\n             ActiveMQMessage message) throws IOException, JMSException {\\n+\\n         if (message.getDataStructureType() == ActiveMQObjectMessage.DATA_STRUCTURE_TYPE) {\\n             StompFrame command = new StompFrame();\\n             command.setAction(Stomp.Responses.MESSAGE);\\n@@ -153,6 +155,10 @@ public class JmsFrameTranslator extends LegacyFrameTranslator implements\\n             FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(\\n                     converter, message, command, this);\\n \\n+            if (!headers.containsKey(Stomp.Headers.TRANSFORMATION)) {\\n+                headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_ADVISORY_JSON.toString());\\n+            }\\n+\\n             if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_XML.toString())) {\\n                 headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_ADVISORY_XML.toString());\\n             } else if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_JSON.toString())) {\\n@@ -274,4 +280,16 @@ public class JmsFrameTranslator extends LegacyFrameTranslator implements\\n     public void setBrokerContext(BrokerContext brokerContext) {\\n         this.brokerContext = brokerContext;\\n     }\\n+\\n+    /**\\n+     * Return an Advisory message as a JSON formatted string\\n+     * @param ds\\n+     * @return\\n+     */\\n+    protected String marshallAdvisory(final DataStructure ds) {\\n+        XStream xstream = new XStream(new JsonHierarchicalStreamDriver());\\n+        xstream.setMode(XStream.NO_REFERENCES);\\n+        xstream.aliasPackage(\"\", \"org.apache.activemq.command\");\\n+        return xstream.toXML(ds);\\n+    }\\n }\\ndiff --git a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java\\nindex 1d826a44e..8cfa1219e 100644\\n--- a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java\\n+++ b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java\\n@@ -16,25 +16,19 @@\\n  */\\n package org.apache.activemq.transport.stomp;\\n \\n-import java.io.DataOutputStream;\\n-import java.io.IOException;\\n-import java.util.HashMap;\\n-import java.util.Map;\\n-\\n-import javax.jms.Destination;\\n-import javax.jms.JMSException;\\n-\\n-import org.apache.activemq.advisory.AdvisorySupport;\\n import org.apache.activemq.command.ActiveMQBytesMessage;\\n import org.apache.activemq.command.ActiveMQDestination;\\n import org.apache.activemq.command.ActiveMQMessage;\\n import org.apache.activemq.command.ActiveMQTextMessage;\\n-import org.apache.activemq.command.DataStructure;\\n import org.apache.activemq.util.ByteArrayOutputStream;\\n import org.apache.activemq.util.ByteSequence;\\n \\n-import com.thoughtworks.xstream.XStream;\\n-import com.thoughtworks.xstream.io.json.JsonHierarchicalStreamDriver;\\n+import javax.jms.Destination;\\n+import javax.jms.JMSException;\\n+import java.io.DataOutputStream;\\n+import java.io.IOException;\\n+import java.util.HashMap;\\n+import java.util.Map;\\n \\n /**\\n  * Implements ActiveMQ 4.0 translations\\n@@ -127,15 +121,8 @@ public class LegacyFrameTranslator implements FrameTranslator {\\n \\n             headers.put(Stomp.Headers.CONTENT_LENGTH, Integer.toString(data.length));\\n             command.setContent(data);\\n-        } else if (message.getDataStructureType() == ActiveMQMessage.DATA_STRUCTURE_TYPE &&\\n-                AdvisorySupport.ADIVSORY_MESSAGE_TYPE.equals(message.getType())) {\\n-\\n-            FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(\\n-                    converter, message, command, this);\\n-\\n-            String body = marshallAdvisory(message.getDataStructure());\\n-            command.setContent(body.getBytes(\"UTF-8\"));\\n         }\\n+\\n         return command;\\n     }\\n \\n@@ -212,15 +199,5 @@ public class LegacyFrameTranslator implements FrameTranslator {\\n         }\\n     }\\n \\n-    /**\\n-     * Return an Advisory message as a JSON formatted string\\n-     * @param ds\\n-     * @return\\n-     */\\n-    protected String marshallAdvisory(final DataStructure ds) {\\n-        XStream xstream = new XStream(new JsonHierarchicalStreamDriver());\\n-        xstream.setMode(XStream.NO_REFERENCES);\\n-        xstream.aliasPackage(\"\", \"org.apache.activemq.command\");\\n-        return xstream.toXML(ds);\\n-    }\\n+\\n }\\ndiff --git a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java\\nindex a6a22f17f..f31aad1b6 100644\\n--- a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java\\n+++ b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java\\n@@ -31,6 +31,7 @@ import java.util.concurrent.atomic.AtomicBoolean;\\n import javax.jms.JMSException;\\n \\n import org.apache.activemq.ActiveMQPrefetchPolicy;\\n+import org.apache.activemq.advisory.AdvisorySupport;\\n import org.apache.activemq.broker.BrokerContext;\\n import org.apache.activemq.broker.BrokerContextAware;\\n import org.apache.activemq.command.ActiveMQDestination;\\n@@ -200,19 +201,28 @@ public class ProtocolConverter {\\n     }\\n \\n     protected FrameTranslator findTranslator(String header) {\\n+        return findTranslator(header, null);\\n+    }\\n+\\n+    protected FrameTranslator findTranslator(String header, ActiveMQDestination destination) {\\n         FrameTranslator translator = frameTranslator;\\n         try {\\n             if (header != null) {\\n                 translator = (FrameTranslator) FRAME_TRANSLATOR_FINDER\\n                         .newInstance(header);\\n-                if (translator instanceof BrokerContextAware) {\\n-                    ((BrokerContextAware)translator).setBrokerContext(brokerContext);\\n+            } else {\\n+                if (destination != null && AdvisorySupport.isAdvisoryTopic(destination)) {\\n+                    translator = new JmsFrameTranslator();\\n                 }\\n             }\\n         } catch (Exception ignore) {\\n             // if anything goes wrong use the default translator\\n         }\\n \\n+        if (translator instanceof BrokerContextAware) {\\n+            ((BrokerContextAware)translator).setBrokerContext(brokerContext);\\n+        }\\n+\\n         return translator;\\n     }\\n \\n@@ -879,7 +889,7 @@ public class ProtocolConverter {\\n         if (ignoreTransformation == true) {\\n             return frameTranslator.convertMessage(this, message);\\n         } else {\\n-            return findTranslator(message.getStringProperty(Stomp.Headers.TRANSFORMATION)).convertMessage(this, message);\\n+            return findTranslator(message.getStringProperty(Stomp.Headers.TRANSFORMATION), message.getDestination()).convertMessage(this, message);\\n         }\\n     }\\n \\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: AMQ-2098\\nIssue Summary: Advisory messages are empty when received with a Stomp subscription\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nWe need advisory messages for client connections, and we are stomp when connecting to ActiveMQ.  When we subscribe to ActiveMQ.Advisory.Connection, we get messages when clients connect of disconnect, but the messages are empty.  I suspect the information is lost when converting the ActiveMQMessage to a Stomp message.\\n\\nSome sort of simple serialization (like key: value pairs) of the ConnectionInfo object in ActiveMQMessage.getDataStructure would solve the problem.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java\\nindex e1b6f9352..ca2906d88 100644\\n--- a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java\\n+++ b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/JmsFrameTranslator.java\\n@@ -26,6 +26,7 @@ import java.util.Map;\\n \\n import javax.jms.JMSException;\\n \\n+import com.thoughtworks.xstream.io.json.JsonHierarchicalStreamDriver;\\n import org.apache.activemq.advisory.AdvisorySupport;\\n import org.apache.activemq.broker.BrokerContext;\\n import org.apache.activemq.broker.BrokerContextAware;\\n@@ -102,6 +103,7 @@ public class JmsFrameTranslator extends LegacyFrameTranslator implements\\n     @Override\\n     public StompFrame convertMessage(ProtocolConverter converter,\\n             ActiveMQMessage message) throws IOException, JMSException {\\n+\\n         if (message.getDataStructureType() == ActiveMQObjectMessage.DATA_STRUCTURE_TYPE) {\\n             StompFrame command = new StompFrame();\\n             command.setAction(Stomp.Responses.MESSAGE);\\n@@ -153,6 +155,10 @@ public class JmsFrameTranslator extends LegacyFrameTranslator implements\\n             FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(\\n                     converter, message, command, this);\\n \\n+            if (!headers.containsKey(Stomp.Headers.TRANSFORMATION)) {\\n+                headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_ADVISORY_JSON.toString());\\n+            }\\n+\\n             if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_XML.toString())) {\\n                 headers.put(Stomp.Headers.TRANSFORMATION, Stomp.Transformations.JMS_ADVISORY_XML.toString());\\n             } else if (headers.get(Stomp.Headers.TRANSFORMATION).equals(Stomp.Transformations.JMS_JSON.toString())) {\\n@@ -274,4 +280,16 @@ public class JmsFrameTranslator extends LegacyFrameTranslator implements\\n     public void setBrokerContext(BrokerContext brokerContext) {\\n         this.brokerContext = brokerContext;\\n     }\\n+\\n+    /**\\n+     * Return an Advisory message as a JSON formatted string\\n+     * @param ds\\n+     * @return\\n+     */\\n+    protected String marshallAdvisory(final DataStructure ds) {\\n+        XStream xstream = new XStream(new JsonHierarchicalStreamDriver());\\n+        xstream.setMode(XStream.NO_REFERENCES);\\n+        xstream.aliasPackage(\"\", \"org.apache.activemq.command\");\\n+        return xstream.toXML(ds);\\n+    }\\n }\\ndiff --git a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java\\nindex 1d826a44e..8cfa1219e 100644\\n--- a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java\\n+++ b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/LegacyFrameTranslator.java\\n@@ -16,25 +16,19 @@\\n  */\\n package org.apache.activemq.transport.stomp;\\n \\n-import java.io.DataOutputStream;\\n-import java.io.IOException;\\n-import java.util.HashMap;\\n-import java.util.Map;\\n-\\n-import javax.jms.Destination;\\n-import javax.jms.JMSException;\\n-\\n-import org.apache.activemq.advisory.AdvisorySupport;\\n import org.apache.activemq.command.ActiveMQBytesMessage;\\n import org.apache.activemq.command.ActiveMQDestination;\\n import org.apache.activemq.command.ActiveMQMessage;\\n import org.apache.activemq.command.ActiveMQTextMessage;\\n-import org.apache.activemq.command.DataStructure;\\n import org.apache.activemq.util.ByteArrayOutputStream;\\n import org.apache.activemq.util.ByteSequence;\\n \\n-import com.thoughtworks.xstream.XStream;\\n-import com.thoughtworks.xstream.io.json.JsonHierarchicalStreamDriver;\\n+import javax.jms.Destination;\\n+import javax.jms.JMSException;\\n+import java.io.DataOutputStream;\\n+import java.io.IOException;\\n+import java.util.HashMap;\\n+import java.util.Map;\\n \\n /**\\n  * Implements ActiveMQ 4.0 translations\\n@@ -127,15 +121,8 @@ public class LegacyFrameTranslator implements FrameTranslator {\\n \\n             headers.put(Stomp.Headers.CONTENT_LENGTH, Integer.toString(data.length));\\n             command.setContent(data);\\n-        } else if (message.getDataStructureType() == ActiveMQMessage.DATA_STRUCTURE_TYPE &&\\n-                AdvisorySupport.ADIVSORY_MESSAGE_TYPE.equals(message.getType())) {\\n-\\n-            FrameTranslator.Helper.copyStandardHeadersFromMessageToFrame(\\n-                    converter, message, command, this);\\n-\\n-            String body = marshallAdvisory(message.getDataStructure());\\n-            command.setContent(body.getBytes(\"UTF-8\"));\\n         }\\n+\\n         return command;\\n     }\\n \\n@@ -212,15 +199,5 @@ public class LegacyFrameTranslator implements FrameTranslator {\\n         }\\n     }\\n \\n-    /**\\n-     * Return an Advisory message as a JSON formatted string\\n-     * @param ds\\n-     * @return\\n-     */\\n-    protected String marshallAdvisory(final DataStructure ds) {\\n-        XStream xstream = new XStream(new JsonHierarchicalStreamDriver());\\n-        xstream.setMode(XStream.NO_REFERENCES);\\n-        xstream.aliasPackage(\"\", \"org.apache.activemq.command\");\\n-        return xstream.toXML(ds);\\n-    }\\n+\\n }\\ndiff --git a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java\\nindex a6a22f17f..f31aad1b6 100644\\n--- a/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java\\n+++ b/activemq-stomp/src/main/java/org/apache/activemq/transport/stomp/ProtocolConverter.java\\n@@ -31,6 +31,7 @@ import java.util.concurrent.atomic.AtomicBoolean;\\n import javax.jms.JMSException;\\n \\n import org.apache.activemq.ActiveMQPrefetchPolicy;\\n+import org.apache.activemq.advisory.AdvisorySupport;\\n import org.apache.activemq.broker.BrokerContext;\\n import org.apache.activemq.broker.BrokerContextAware;\\n import org.apache.activemq.command.ActiveMQDestination;\\n@@ -200,19 +201,28 @@ public class ProtocolConverter {\\n     }\\n \\n     protected FrameTranslator findTranslator(String header) {\\n+        return findTranslator(header, null);\\n+    }\\n+\\n+    protected FrameTranslator findTranslator(String header, ActiveMQDestination destination) {\\n         FrameTranslator translator = frameTranslator;\\n         try {\\n             if (header != null) {\\n                 translator = (FrameTranslator) FRAME_TRANSLATOR_FINDER\\n                         .newInstance(header);\\n-                if (translator instanceof BrokerContextAware) {\\n-                    ((BrokerContextAware)translator).setBrokerContext(brokerContext);\\n+            } else {\\n+                if (destination != null && AdvisorySupport.isAdvisoryTopic(destination)) {\\n+                    translator = new JmsFrameTranslator();\\n                 }\\n             }\\n         } catch (Exception ignore) {\\n             // if anything goes wrong use the default translator\\n         }\\n \\n+        if (translator instanceof BrokerContextAware) {\\n+            ((BrokerContextAware)translator).setBrokerContext(brokerContext);\\n+        }\\n+\\n         return translator;\\n     }\\n \\n@@ -879,7 +889,7 @@ public class ProtocolConverter {\\n         if (ignoreTransformation == true) {\\n             return frameTranslator.convertMessage(this, message);\\n         } else {\\n-            return findTranslator(message.getStringProperty(Stomp.Headers.TRANSFORMATION)).convertMessage(this, message);\\n+            return findTranslator(message.getStringProperty(Stomp.Headers.TRANSFORMATION), message.getDestination()).convertMessage(this, message);\\n         }\\n     }\\n \\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: AMQ-9473\\nIssue Summary: Client SSL Socket configuration fails while settings parameters\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nClient connection creation fails when setting socket parameters.\\n\\n\\n\\nException was thrown, when I tried to set enabledProtocols parameter using url:\\n\\n\\n\\nssl://127.0.0.1:12345?socket.enabledProtocols=TLSv1.3\\n\\n\\n\\nException is also thrown, when using tcpNoDelay parameter. It is thrown probably with most of the parameters related to sockets.\\n\\n\\n\\nHere is the exception thrown:\\n\\n\\n\\n{code:java}\\n\\njava.lang.reflect.InaccessibleObjectException: Unable to make public void sun.security.ssl.SSLSocketImpl.setEnabledProtocols(java.lang.String[]) accessible: module java.base does not \"exports sun.security.ssl\" to unnamed module @48f2bd5b\\n\\n13:22:43.976 [main] ERROR org.apache.activemq.util.IntrospectionSupport - Could not set property enabledProtocols on SSLSocket[hostname=127.0.0.1, port=12345, Session(...)]\\n\\n            at java.lang.reflect.AccessibleObject.throwInaccessibleObjectException(AccessibleObject.java:391) ~[?:?]\\n\\n            at java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:367) ~[?:?]\\n\\n            at java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:315) ~[?:?]\\n\\n            at java.lang.reflect.Method.checkCanSetAccessible(Method.java:203) ~[?:?]\\n\\n            at java.lang.reflect.Method.setAccessible(Method.java:197) ~[?:?]\\n\\n            at org.apache.activemq.util.IntrospectionSupport.setProperty(IntrospectionSupport.java:184) [test/:6.0.1]\\n\\n            at org.apache.activemq.util.IntrospectionSupport.setProperties(IntrospectionSupport.java:155) [test/:6.0.1]\\n\\n            at org.apache.activemq.util.IntrospectionSupport.setProperties(IntrospectionSupport.java:140) [test/:6.0.1]\\n\\n            at org.apache.activemq.transport.tcp.TcpTransport.initialiseSocket(TcpTransport.java:449) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at org.apache.activemq.transport.tcp.SslTransport.initialiseSocket(SslTransport.java:137) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at org.apache.activemq.transport.tcp.TcpTransport.connect(TcpTransport.java:542) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at org.apache.activemq.transport.tcp.TcpTransport.doStart(TcpTransport.java:488) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at org.apache.activemq.util.ServiceSupport.start(ServiceSupport.java:55) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at org.apache.activemq.transport.AbstractInactivityMonitor.start(AbstractInactivityMonitor.java:172) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at org.apache.activemq.transport.InactivityMonitor.start(InactivityMonitor.java:52) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at org.apache.activemq.transport.TransportFilter.start(TransportFilter.java:64) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at org.apache.activemq.transport.WireFormatNegotiator.start(WireFormatNegotiator.java:72) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at org.apache.activemq.transport.TransportFilter.start(TransportFilter.java:64) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at org.apache.activemq.transport.TransportFilter.start(TransportFilter.java:64) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at org.apache.activemq.ActiveMQConnectionFactory.createActiveMQConnection(ActiveMQConnectionFactory.java:399) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at org.apache.activemq.ActiveMQConnectionFactory.createActiveMQConnection(ActiveMQConnectionFactory.java:349) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at org.apache.activemq.ActiveMQConnectionFactory.createConnection(ActiveMQConnectionFactory.java:245) [activemq-client-6.0.1.jar:6.0.1]\\n\\n            at test.ActiveMQClientSSLSocketParameter.main(ActiveMQClientSSLSocketParameter.java:25) [test/:?]\\n\\n{code}\\n\\n\\n\\n\\n\\nHere is example to reproduce issue:\\n\\n{code:java}\\n\\npackage test;\\n\\n\\n\\nimport java.io.IOException;\\n\\nimport java.net.ServerSocket;\\n\\nimport org.apache.activemq.ActiveMQSslConnectionFactory;\\n\\n\\n\\npublic class ActiveMQClientSSLSocketParameter {\\n\\n\\n\\n\\xa0 \\xa0 public static void main(String[] args) throws Exception{\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 // Dummy server\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 ServerSocket server = new ServerSocket(12345);\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 new Thread(() -> {\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 try {\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 var client = server.accept();\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 client.close();\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 }catch(Exception e) {\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 e.printStackTrace();\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 }\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 }).start();\\n\\n\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 var factory = new ActiveMQSslConnectionFactory(\"ssl://127.0.0.1:12345?socket.enabledProtocols=TLSv1.3\");\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 // or socket.enabledCipherSuites=TLS_AES_256_GCM_SHA384\\n\\n\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 try(var connection = factory.createConnection()){\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 //NOP\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 } finally {\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 try {\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 server.close();\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 } catch (IOException e) {\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 e.printStackTrace();\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 }\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 }\\n\\n\\n\\n\\xa0 \\xa0 }\\n\\n}\\n\\n{code}\\n\\n\\n\\nFix seems to be trivial, because same kind of issue is already corrected with server side (SSLServerSocket). See line https://github.com/apache/activemq/blob/3636a497ede5b95cf8257c2f359a3bc8a02fb325/activemq-client/src/main/java/org/apache/activemq/util/IntrospectionSupport.java#L172\\n\\n\\n\\nSnippet from IntrospectionSupport:\\n\\n{code}\\n\\n public static boolean setProperty(Object target, String name, Object value) {\\n\\n        try {\\n\\n            Class<?> clazz = target.getClass();\\n\\n            if (target instanceof SSLServerSocket) {\\n\\n                // overcome illegal access issues with internal implementation class\\n\\n                clazz = SSLServerSocket.class;\\n\\n            }\\n\\n            // ...\\n\\n{code}\\n\\n\\n\\nFix for this issue would be:\\n\\n{code}\\n\\n public static boolean setProperty(Object target, String name, Object value) {\\n\\n        try {\\n\\n            Class<?> clazz = target.getClass();\\n\\n            if (target instanceof SSLServerSocket) {\\n\\n                // overcome illegal access issues with internal implementation class\\n\\n                clazz = SSLServerSocket.class;\\n\\n            } else if (target instanceof javax.net.ssl.SSLSocket) {\\n\\n                // overcome illegal access issues with internal implementation class\\n\\n                clazz = javax.net.ssl.SSLSocket.class;\\n\\n            }\\n\\n           // ...\\n\\n{code}\\n\\n\\xa0\\n\\nThere is also similar code (https://github.com/apache/activemq/blob/3636a497ede5b95cf8257c2f359a3bc8a02fb325/activemq-jms-pool/src/main/java/org/apache/activemq/jms/pool/IntrospectionSupport.java#L87),  which probably should be corrected the same manner.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Thu, 27 Feb 2025 11:13:56 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '74'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-7b44d88999-4d2w2'), ('x-envoy-upstream-service-time', '37'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999960'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '2ms'), ('x-request-id', 'req_81985e30e25538754429c417cda18095'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=PMsBsRFlWTTwVJihkC8pCedwwEPYBCnPdIAhAiTqefc-1740654836-1.0.1.1-XNNZ1UEfni5HK4IAx9cMPZ5FbIAYN8CdYuIKtiQY1MFwy.PeRJMbvhsjBduBzOUJ84.FrAKWfqmf8EAlcZgnRA; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=RCq9luHgqO60.YTEYZc8Fbjo8RVq0gIyqgykCFylxdQ-1740654836159-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9187bf93cf4e3da5-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Thu, 27 Feb 2025 11:13:56 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '54'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-5765dbdcc8-58wj8'), ('x-envoy-upstream-service-time', '31'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2998'), ('x-ratelimit-remaining-tokens', '999973'), ('x-ratelimit-reset-requests', '33ms'), ('x-ratelimit-reset-tokens', '1ms'), ('x-request-id', 'req_165c2ab02803b24f42dd97e1ae712a00'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=A9FMbjdG_gLFks2Mbl.pGm89h0SIS22yERIyldYVs6M-1740654836-1.0.1.1-BEzWhVFr430cb2gFj2for4CzQhN2byer9LNfluulzJQbJOAGDbUUN78pC9UgDaX9z9_QtyZl1MPscXJ.DNFatg; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=QN0JgNyqnCnsAENp60W_oWcG1vyeN4qppukUJ3M8w1E-1740654836139-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9187bf93b8c59fe5-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_81985e30e25538754429c417cda18095\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Thu, 27 Feb 2025 11:13:56 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '79'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-5765dbdcc8-vmcds'), ('x-envoy-upstream-service-time', '58'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999951'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '2ms'), ('x-request-id', 'req_19c72129b9c9cdc65b38109455f70201'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=pjODjZwrO4a7YdoVbXHFpv0wmlYWW5IznM8aQkiv.Ns-1740654836-1.0.1.1-c9sMZ1r0OIgNLm5D51slOIhYk8xfBO5GhfDSCLI8O5oyiaffnxtmvgChwhB.m7e5V5m4cnFc0bWLV_ExgoLyuA; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=l8wgu8CdjWkuFK5GYx9h8e93AV9qg4bG5QamSOS4G9g-1740654836162-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9187bf93cd0e81b9-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_165c2ab02803b24f42dd97e1ae712a00\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688EB930>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688EBF70>\n",
      "DEBUG:openai._base_client:request_id: req_19c72129b9c9cdc65b38109455f70201\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A0B06BCA0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java b/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java\\nindex 2ed439d375..f76a4d15ea 100644\\n--- a/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java\\n+++ b/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java\\n@@ -42,7 +42,7 @@\\n  * <li>unlimited number of version components,</li>\\n  * <li>version components in the text can be digits or strings,</li>\\n  * <li>strings are checked for well-known qualifiers and the qualifier ordering is used for version ordering.\\n- *     Well-known qualifiers (case insensitive) are:<ul>\\n+ *     Well-known qualifiers (case-insensitive) are:<ul>\\n  *     <li><code>alpha</code> or <code>a</code></li>\\n  *     <li><code>beta</code> or <code>b</code></li>\\n  *     <li><code>milestone</code> or <code>m</code></li>\\n@@ -51,9 +51,9 @@\\n  *     <li><code>(the empty string)</code> or <code>ga</code> or <code>final</code></li>\\n  *     <li><code>sp</code></li>\\n  *     </ul>\\n- *     Unknown qualifiers are considered after known qualifiers, with lexical order (always case insensitive),\\n+ *     Unknown qualifiers are considered after known qualifiers, with lexical order (always case-insensitive),\\n  *   </li>\\n- * <li>a hyphen usually precedes a qualifier, and is always less important than digits/number, for example\\n+ * <li>A hyphen usually precedes a qualifier, and is always less important than digits/number. For example\\n  *   {@code 1.0.RC2 < 1.0-RC3 < 1.0.1}; but prefer {@code 1.0.0-RC1} over {@code 1.0.0.RC1}, and more\\n  *   generally: {@code 1.0.X2 < 1.0-X3 < 1.0.1} for any string {@code X}; but prefer {@code 1.0.0-X1}\\n  *   over {@code 1.0.0.X1}.</li>\\n@@ -656,7 +656,20 @@ public final void parseVersion(String version) {\\n         int startIndex = 0;\\n \\n         for (int i = 0; i < version.length(); i++) {\\n-            char c = version.charAt(i);\\n+            char character = version.charAt(i);\\n+            int c = character;\\n+            if (Character.isHighSurrogate(character)) {\\n+                // read the next character as a low surrogate and combine into a single int\\n+                try {\\n+                    char low = version.charAt(i + 1);\\n+                    char[] both = {character, low};\\n+                    c = Character.codePointAt(both, 0);\\n+                    i++;\\n+                } catch (IndexOutOfBoundsException ex) {\\n+                    // high surrogate without low surrogate. Not a lot we can do here except treat it as a regular\\n+                    // character\\n+                }\\n+            }\\n \\n             if (c == \\'.\\') {\\n                 if (i == startIndex) {\\n@@ -687,7 +700,7 @@ public final void parseVersion(String version) {\\n                     stack.push(list);\\n                 }\\n                 isCombination = false;\\n-            } else if (Character.isDigit(c)) {\\n+            } else if (c >= \\'0\\' && c <= \\'9\\') { // Check for ASCII digits only\\n                 if (!isDigit && i > startIndex) {\\n                     // X1\\n                     isCombination = true;\\ndiff --git a/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java b/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java\\nindex d7616405bd..219d760bab 100644\\n--- a/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java\\n+++ b/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java\\n@@ -27,7 +27,6 @@\\n \\n /**\\n  * Test ComparableVersion.\\n- *\\n  */\\n @SuppressWarnings(\"unchecked\")\\n class ComparableVersionTest {\\n@@ -222,6 +221,23 @@ void testLeadingZeroes() {\\n         checkVersionsOrder(\"0.2\", \"1.0.7\");\\n     }\\n \\n+    @Test\\n+    void testDigitGreaterThanNonAscii() {\\n+        ComparableVersion c1 = new ComparableVersion(\"1\");\\n+        ComparableVersion c2 = new ComparableVersion(\"é\");\\n+        assertTrue(c1.compareTo(c2) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\uD835\\\\uDFE4\");\\n+        assertTrue(c2.compareTo(c1) < 0, \"expected \" + \"\\\\uD835\\\\uDFE4\" + \" < \" + \"1\");\\n+    }\\n+\\n+    @Test\\n+    void testDigitGreaterThanNonBmpCharacters() {\\n+        ComparableVersion c1 = new ComparableVersion(\"1\");\\n+        // MATHEMATICAL SANS-SERIF DIGIT TWO\\n+        ComparableVersion c2 = new ComparableVersion(\"\\\\uD835\\\\uDFE4\");\\n+        assertTrue(c1.compareTo(c2) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\uD835\\\\uDFE4\");\\n+        assertTrue(c2.compareTo(c1) < 0, \"expected \" + \"\\\\uD835\\\\uDFE4\" + \" < \" + \"1\");\\n+    }\\n+\\n     @Test\\n     void testGetCanonical() {\\n         // MNG-7700\\n@@ -238,13 +254,25 @@ void testGetCanonical() {\\n \\n     @Test\\n     void testCompareDigitToLetter() {\\n-        ComparableVersion c1 = new ComparableVersion(\"7\");\\n-        ComparableVersion c2 = new ComparableVersion(\"J\");\\n-        ComparableVersion c3 = new ComparableVersion(\"c\");\\n-        assertTrue(c1.compareTo(c2) > 0, \"expected 7 > J\");\\n-        assertTrue(c2.compareTo(c1) < 0, \"expected J < 1\");\\n-        assertTrue(c1.compareTo(c3) > 0, \"expected 7 > c\");\\n-        assertTrue(c3.compareTo(c1) < 0, \"expected c < 7\");\\n+        ComparableVersion seven = new ComparableVersion(\"7\");\\n+        ComparableVersion capitalJ = new ComparableVersion(\"J\");\\n+        ComparableVersion lowerCaseC = new ComparableVersion(\"c\");\\n+        // Digits are greater than letters\\n+        assertTrue(seven.compareTo(capitalJ) > 0, \"expected 7 > J\");\\n+        assertTrue(capitalJ.compareTo(seven) < 0, \"expected J < 1\");\\n+        assertTrue(seven.compareTo(lowerCaseC) > 0, \"expected 7 > c\");\\n+        assertTrue(lowerCaseC.compareTo(seven) < 0, \"expected c < 7\");\\n+    }\\n+\\n+    @Test\\n+    void testNonAsciiDigits() { // These should not be treated as digits.\\n+        ComparableVersion asciiOne = new ComparableVersion(\"1\");\\n+        ComparableVersion arabicEight = new ComparableVersion(\"\\\\u0668\");\\n+        ComparableVersion asciiNine = new ComparableVersion(\"9\");\\n+        assertTrue(asciiOne.compareTo(arabicEight) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\u0668\");\\n+        assertTrue(arabicEight.compareTo(asciiOne) < 0, \"expected \" + \"\\\\u0668\" + \" < \" + \"1\");\\n+        assertTrue(asciiNine.compareTo(arabicEight) > 0, \"expected \" + \"9\" + \" > \" + \"\\\\u0668\");\\n+        assertTrue(arabicEight.compareTo(asciiNine) < 0, \"expected \" + \"\\\\u0668\" + \" < \" + \"9\");\\n     }\\n \\n     @Test\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: MNG-8241\\nIssue Summary: ComparableVersion incorrectly handles Unicode non-BMP characters\\nIssue Type: Bug\\nPriority: Minor\\n\\nDescription:\\nJava strings are (usually) Unicode, but Java chars are a subset of Unicode. ComparableVersion makes heavy use of [String.charAt|https://docs.oracle.com/en/java/javase/22/docs/api/java.base/java/lang/String.html#charAt(int)], which will return surrogate values instead of Unicode code points whenever a character takes more than 16 bits.\\n\\n\\n\\nThis leads to the following behavior:\\n\\n\\n\\n\\xa0\\n\\n{noformat}\\n\\njava -jar ~/.m2/repository/org/apache/maven/maven-artifact/3.9.4/maven-artifact-3.9.4.jar 1 𝟤\\n\\nDisplay parameters as parsed by Maven (in canonical form and as a list of tokens) and comparison result:\\n\\n1. 1 -> 1; tokens: [1]\\n\\n   1 > 𝟤\\n\\n2. 𝟤 -> 𝟤; tokens: [𝟤]{noformat}\\n\\n1 (DIGIT ONE) > 𝟤 (MATHEMATICAL SANS-SERIF DIGIT TWO) because ComparableVersion sees 𝟤 as two invalid characters and treats it as text.\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\xa0\\n\\n{noformat}\\n\\njava -jar ~/.m2/repository/org/apache/maven/maven-artifact/3.9.4/maven-artifact-3.9.4.jar 0 𝟤\\n\\nDisplay parameters as parsed by Maven (in canonical form and as a list of tokens) and comparison result:\\n\\n1. 0 -> ; tokens: []\\n\\n   0 < 𝟤\\n\\n2. 𝟤 -> 𝟤; tokens: [𝟤]{noformat}\\n\\nHowever, 0 (DIGIT 0) is still < 𝟤 (MATHEMATICAL SANS-SERIF DIGIT TWO). 0 < 𝟤 < 1 the same way 0 < a < 1.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nIt\\'s unclear whether this should be considered to be a bug or whether it\\'s just an undocumented limitation. String.charAt and String.length should be avoided unless you can be sure the characters are all BMP (Basic Multilingual Plane).\\n\\n\\n\\nI was initially worried that 𝟣𝟣𝟣𝟣𝟣 (MATHEMATICAL SANS-SERIF DIGIT ONE) > 22222 (DIGIT TWO) because \"𝟣𝟣𝟣𝟣𝟣\".length is 10, greater than MAX_INTITEM_LENGTH, but that code doesn\\'t even get hit because String.charAt is producing effectively \"�����������\". If the code is changed to identify non-BMP Nd class digits like 𝟣 as digits then the code that determines the required size of the data type needs to be updated to measure the length in code points instead of chars.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\\nindex 42d91d536b..470d052d89 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\\n@@ -1039,7 +1039,8 @@ public static List<String> mergeUniqElems(List<String> src, List<String> dest) {\\n     return src;\\n   }\\n \\n-  private static final String tmpPrefix = \"_tmp.\";\\n+  private static final String hadoopTmpPrefix = \"_tmp.\";\\n+  private static final String tmpPrefix = \"-tmp.\";\\n   private static final String taskTmpPrefix = \"_task_tmp.\";\\n \\n   public static Path toTaskTempPath(Path orig) {\\n@@ -1070,7 +1071,7 @@ private static boolean isTempPath(FileStatus file) {\\n     String name = file.getPath().getName();\\n     // in addition to detecting hive temporary files, we also check hadoop\\n     // temporary folders that used to show up in older releases\\n-    return (name.startsWith(\"_task\") || name.startsWith(tmpPrefix));\\n+    return (name.startsWith(\"_task\") || name.startsWith(tmpPrefix) || name.startsWith(hadoopTmpPrefix));\\n   }\\n \\n   /**\\n@@ -1393,7 +1394,7 @@ private static String replaceTaskIdFromFilename(String filename, String oldTaskI\\n   }\\n \\n \\n-  private static boolean shouldAvoidRename(FileSinkDesc conf, Configuration hConf) {\\n+  public static boolean shouldAvoidRename(FileSinkDesc conf, Configuration hConf) {\\n     // we are avoiding rename/move only if following conditions are met\\n     //  * execution engine is tez\\n     //  * if it is select query\\n@@ -3524,6 +3525,9 @@ public static List<Path> getInputPaths(JobConf job, MapWork work, Path hiveScrat\\n     Set<Path> pathsProcessed = new HashSet<Path>();\\n     List<Path> pathsToAdd = new LinkedList<Path>();\\n     DriverState driverState = DriverState.getDriverState();\\n+    if (work.isUseInputPathsDirectly() && work.getInputPaths() != null) {\\n+      return work.getInputPaths();\\n+    }\\n     // AliasToWork contains all the aliases\\n     Collection<String> aliasToWork = work.getAliasToWork().keySet();\\n     if (!skipDummy) {\\n@@ -4555,7 +4559,7 @@ private static Path getManifestDir(Path specPath, long writeId, int stmtId, Stri\\n     if (isDelete) {\\n       deltaDir = AcidUtils.deleteDeltaSubdir(writeId, writeId, stmtId);\\n     }\\n-    Path manifestPath = new Path(manifestRoot, \"_tmp.\" + deltaDir);\\n+    Path manifestPath = new Path(manifestRoot, Utilities.toTempPath(deltaDir));\\n \\n     if (isInsertOverwrite) {\\n       // When doing a multi-statement insert overwrite query with dynamic partitioning, the\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java\\nindex 594289eda4..53a4f3a843 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java\\n@@ -46,7 +46,6 @@\\n public class MergeFileWork extends MapWork {\\n \\n   private static final Logger LOG = LoggerFactory.getLogger(MergeFileWork.class);\\n-  private List<Path> inputPaths;\\n   private Path outputDir;\\n   private boolean hasDynamicPartitions;\\n   private boolean isListBucketingAlterTableConcatenate;\\n@@ -84,14 +83,6 @@ public MergeFileWork(List<Path> inputPaths, Path outputDir,\\n     this.isListBucketingAlterTableConcatenate = false;\\n   }\\n \\n-  public List<Path> getInputPaths() {\\n-    return inputPaths;\\n-  }\\n-\\n-  public void setInputPaths(List<Path> inputPaths) {\\n-    this.inputPaths = inputPaths;\\n-  }\\n-\\n   public Path getOutputDir() {\\n     return outputDir;\\n   }\\n@@ -137,8 +128,6 @@ public void resolveDynamicPartitionStoredAsSubDirsMerge(HiveConf conf,\\n         aliases, partDesc);\\n     // set internal input format for all partition descriptors\\n     partDesc.setInputFileFormatClass(internalInputFormat);\\n-    // Add the DP path to the list of input paths\\n-    inputPaths.add(path);\\n   }\\n \\n   /**\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java\\nindex da20eceba3..0e6816ae40 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java\\n@@ -20,18 +20,27 @@\\n \\n import java.io.IOException;\\n import java.io.Serializable;\\n+import java.nio.charset.Charset;\\n import java.util.ArrayList;\\n+import java.util.HashMap;\\n import java.util.HashSet;\\n import java.util.LinkedHashMap;\\n import java.util.List;\\n+import java.util.LongSummaryStatistics;\\n import java.util.Map;\\n+import java.util.stream.Collectors;\\n \\n+import com.google.common.collect.Lists;\\n+import org.apache.commons.io.IOUtils;\\n+import org.apache.hadoop.fs.FSDataInputStream;\\n import org.apache.hadoop.fs.FileStatus;\\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n import org.apache.hadoop.hive.common.HiveStatsUtils;\\n import org.apache.hadoop.hive.conf.HiveConf;\\n import org.apache.hadoop.hive.metastore.Warehouse;\\n+import org.apache.hadoop.hive.metastore.utils.FileUtils;\\n+import org.apache.hadoop.hive.ql.exec.Operator;\\n import org.apache.hadoop.hive.ql.exec.Task;\\n import org.apache.hadoop.hive.ql.exec.Utilities;\\n import org.slf4j.Logger;\\n@@ -151,6 +160,11 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n         }\\n \\n         int lbLevel = (ctx.getLbCtx() == null) ? 0 : ctx.getLbCtx().calculateListBucketingLevel();\\n+        boolean manifestFilePresent = false;\\n+        FileSystem manifestFs = dirPath.getFileSystem(conf);\\n+        if (manifestFs.exists(new Path(dirPath, Utilities.BLOB_MANIFEST_FILE))) {\\n+          manifestFilePresent = true;\\n+        }\\n \\n         /**\\n          * In order to make code easier to read, we write the following in the way:\\n@@ -168,15 +182,25 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n           int dpLbLevel = numDPCols + lbLevel;\\n \\n           generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask,\\n-              mrAndMvTask, dirPath, inpFs, ctx, work, dpLbLevel);\\n+              mrAndMvTask, dirPath, inpFs, ctx, work, dpLbLevel, manifestFilePresent);\\n         } else { // no dynamic partitions\\n           if(lbLevel == 0) {\\n             // static partition without list bucketing\\n-            long totalSz = getMergeSize(inpFs, dirPath, avgConditionSize);\\n-            Utilities.FILE_OP_LOGGER.debug(\"merge resolve simple case - totalSz \" + totalSz + \" from \" + dirPath);\\n+            List<FileStatus> manifestFilePaths = new ArrayList<>();\\n+            long totalSize;\\n+            if (manifestFilePresent) {\\n+              manifestFilePaths = getManifestFilePaths(conf, dirPath);\\n+              totalSize = getMergeSize(manifestFilePaths, avgConditionSize);\\n+            } else {\\n+              totalSize = getMergeSize(inpFs, dirPath, avgConditionSize);\\n+              Utilities.FILE_OP_LOGGER.debug(\"merge resolve simple case - totalSize \" + totalSize + \" from \" + dirPath);\\n+            }\\n \\n-            if (totalSz >= 0) { // add the merge job\\n-              setupMapRedWork(conf, work, trgtSize, totalSz);\\n+            if (totalSize >= 0) { // add the merge job\\n+              if (manifestFilePresent) {\\n+                setupWorkWhenUsingManifestFile(work, manifestFilePaths, dirPath, true);\\n+              }\\n+              setupMapRedWork(conf, work, trgtSize, totalSize);\\n               resTsks.add(mrTask);\\n             } else { // don\\'t need to merge, add the move job\\n               resTsks.add(mvTask);\\n@@ -184,7 +208,7 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n           } else {\\n             // static partition and list bucketing\\n             generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask,\\n-                mrAndMvTask, dirPath, inpFs, ctx, work, lbLevel);\\n+                mrAndMvTask, dirPath, inpFs, ctx, work, lbLevel, manifestFilePresent);\\n           }\\n         }\\n       } else {\\n@@ -229,11 +253,22 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n   private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n       long trgtSize, long avgConditionSize, Task<?> mvTask,\\n       Task<?> mrTask, Task<?> mrAndMvTask, Path dirPath,\\n-      FileSystem inpFs, ConditionalResolverMergeFilesCtx ctx, MapWork work, int dpLbLevel)\\n+      FileSystem inpFs, ConditionalResolverMergeFilesCtx ctx, MapWork work, int dpLbLevel,\\n+      boolean manifestFilePresent)\\n       throws IOException {\\n     DynamicPartitionCtx dpCtx = ctx.getDPCtx();\\n-    // get list of dynamic partitions\\n-    List<FileStatus> statusList = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);\\n+    List<FileStatus> statusList;\\n+    Map<FileStatus, List<FileStatus>> manifestDirToFile = new HashMap<>();\\n+    if (manifestFilePresent) {\\n+      // Get the list of files from manifest file.\\n+      List<FileStatus> fileStatuses = getManifestFilePaths(conf, dirPath);\\n+      // Setup the work to include all the files present in the manifest.\\n+      setupWorkWhenUsingManifestFile(work, fileStatuses, dirPath, false);\\n+      manifestDirToFile = getManifestDirs(inpFs, fileStatuses);\\n+      statusList = new ArrayList<>(manifestDirToFile.keySet());\\n+    } else {\\n+      statusList = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);\\n+    }\\n     FileStatus[] status = statusList.toArray(new FileStatus[statusList.size()]);\\n \\n     // cleanup pathToPartitionInfo\\n@@ -253,15 +288,21 @@ private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n     work.removePathToAlias(path); // the root path is not useful anymore\\n \\n     // populate pathToPartitionInfo and pathToAliases w/ DP paths\\n-    long totalSz = 0;\\n+    long totalSize = 0;\\n     boolean doMerge = false;\\n     // list of paths that don\\'t need to merge but need to move to the dest location\\n-    List<Path> toMove = new ArrayList<Path>();\\n+    List<Path> toMove = new ArrayList<>();\\n+    List<Path> toMerge = new ArrayList<>();\\n     for (int i = 0; i < status.length; ++i) {\\n-      long len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);\\n+      long len;\\n+      if (manifestFilePresent) {\\n+        len = getMergeSize(manifestDirToFile.get(status[i]), avgConditionSize);\\n+      } else {\\n+        len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);\\n+      }\\n       if (len >= 0) {\\n         doMerge = true;\\n-        totalSz += len;\\n+        totalSize += len;\\n         PartitionDesc pDesc = (dpCtx != null) ? generateDPFullPartSpec(dpCtx, status, tblDesc, i)\\n             : partDesc;\\n         if (pDesc == null) {\\n@@ -271,6 +312,13 @@ private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n         Utilities.FILE_OP_LOGGER.debug(\"merge resolver will merge \" + status[i].getPath());\\n         work.resolveDynamicPartitionStoredAsSubDirsMerge(conf, status[i].getPath(), tblDesc,\\n             aliases, pDesc);\\n+        // Do not add input file since its already added when the manifest file is present.\\n+        if (manifestFilePresent) {\\n+          toMerge.addAll(manifestDirToFile.get(status[i])\\n+              .stream().map(FileStatus::getPath).collect(Collectors.toList()));\\n+        } else {\\n+          toMerge.add(status[i].getPath());\\n+        }\\n       } else {\\n         Utilities.FILE_OP_LOGGER.debug(\"merge resolver will move \" + status[i].getPath());\\n \\n@@ -278,8 +326,13 @@ private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n       }\\n     }\\n     if (doMerge) {\\n+      // Set paths appropriately.\\n+      if (work.getInputPaths() != null && !work.getInputPaths().isEmpty()) {\\n+        toMerge.addAll(work.getInputPaths());\\n+      }\\n+      work.setInputPaths(toMerge);\\n       // add the merge MR job\\n-      setupMapRedWork(conf, work, trgtSize, totalSz);\\n+      setupMapRedWork(conf, work, trgtSize, totalSize);\\n \\n       // add the move task for those partitions that do not need merging\\n       if (toMove.size() > 0) {\\n@@ -359,11 +412,11 @@ private void setupMapRedWork(HiveConf conf, MapWork mWork, long targetSize, long\\n     mWork.setIsMergeFromResolver(true);\\n   }\\n \\n-  private static class AverageSize {\\n+  private static class FileSummary {\\n     private final long totalSize;\\n-    private final int numFiles;\\n+    private final long numFiles;\\n \\n-    public AverageSize(long totalSize, int numFiles) {\\n+    public FileSummary(long totalSize, long numFiles) {\\n       this.totalSize = totalSize;\\n       this.numFiles  = numFiles;\\n     }\\n@@ -372,64 +425,106 @@ public long getTotalSize() {\\n       return totalSize;\\n     }\\n \\n-    public int getNumFiles() {\\n+    public long getNumFiles() {\\n       return numFiles;\\n     }\\n   }\\n \\n-  private AverageSize getAverageSize(FileSystem inpFs, Path dirPath) {\\n-    AverageSize error = new AverageSize(-1, -1);\\n-    try {\\n-      FileStatus[] fStats = inpFs.listStatus(dirPath);\\n-\\n-      long totalSz = 0;\\n-      int numFiles = 0;\\n-      for (FileStatus fStat : fStats) {\\n-        Utilities.FILE_OP_LOGGER.debug(\"Resolver looking at \" + fStat.getPath());\\n-        if (fStat.isDir()) {\\n-          AverageSize avgSzDir = getAverageSize(inpFs, fStat.getPath());\\n-          if (avgSzDir.getTotalSize() < 0) {\\n-            return error;\\n-          }\\n-          totalSz += avgSzDir.getTotalSize();\\n-          numFiles += avgSzDir.getNumFiles();\\n-        }\\n-        else {\\n-          totalSz += fStat.getLen();\\n-          numFiles++;\\n-        }\\n-      }\\n+  private FileSummary getFileSummary(List<FileStatus> fileStatusList) {\\n+    LongSummaryStatistics stats = fileStatusList.stream().filter(FileStatus::isFile)\\n+        .mapToLong(FileStatus::getLen).summaryStatistics();\\n+    return new FileSummary(stats.getSum(), stats.getCount());\\n+  }\\n \\n-      return new AverageSize(totalSz, numFiles);\\n-    } catch (IOException e) {\\n-      return error;\\n+  private List<FileStatus> getManifestFilePaths(HiveConf conf, Path dirPath) throws IOException {\\n+    FileSystem manifestFs = dirPath.getFileSystem(conf);\\n+    List<String> filesKept;\\n+    List<FileStatus> pathsKept = new ArrayList<>();\\n+    try (FSDataInputStream inStream = manifestFs.open(new Path(dirPath, Utilities.BLOB_MANIFEST_FILE))) {\\n+      String paths = IOUtils.toString(inStream, Charset.defaultCharset());\\n+      filesKept = Lists.newArrayList(paths.split(System.lineSeparator()));\\n     }\\n+    // The first string contains the directory information. Not useful.\\n+    filesKept.remove(0);\\n+\\n+    for (String file : filesKept) {\\n+      pathsKept.add(manifestFs.getFileStatus(new Path(file)));\\n+    }\\n+    return pathsKept;\\n+  }\\n+\\n+  private long getMergeSize(FileSystem inpFs, Path dirPath, long avgSize) {\\n+    List<FileStatus> result = FileUtils.getFileStatusRecurse(dirPath, inpFs);\\n+    return getMergeSize(result, avgSize);\\n   }\\n \\n   /**\\n    * Whether to merge files inside directory given the threshold of the average file size.\\n    *\\n-   * @param inpFs input file system.\\n-   * @param dirPath input file directory.\\n+   * @param fileStatuses a list of FileStatus instances.\\n    * @param avgSize threshold of average file size.\\n    * @return -1 if not need to merge (either because of there is only 1 file or the\\n    * average size is larger than avgSize). Otherwise the size of the total size of files.\\n    * If return value is 0 that means there are multiple files each of which is an empty file.\\n    * This could be true when the table is bucketized and all buckets are empty.\\n    */\\n-  private long getMergeSize(FileSystem inpFs, Path dirPath, long avgSize) {\\n-    AverageSize averageSize = getAverageSize(inpFs, dirPath);\\n-    if (averageSize.getTotalSize() < 0) {\\n+  private long getMergeSize(List<FileStatus> fileStatuses, long avgSize) {\\n+    FileSummary fileSummary = getFileSummary(fileStatuses);\\n+    if (fileSummary.getTotalSize() <= 0) {\\n       return -1;\\n     }\\n \\n-    if (averageSize.getNumFiles() <= 1) {\\n+    if (fileSummary.getNumFiles() <= 1) {\\n       return -1;\\n     }\\n \\n-    if (averageSize.getTotalSize()/averageSize.getNumFiles() < avgSize) {\\n-      return averageSize.getTotalSize();\\n+    if (fileSummary.getTotalSize() / fileSummary.getNumFiles() < avgSize) {\\n+      return fileSummary.getTotalSize();\\n     }\\n     return -1;\\n   }\\n+\\n+  private void setupWorkWhenUsingManifestFile(MapWork mapWork, List<FileStatus> fileStatuses, Path dirPath,\\n+                                              boolean isTblLevel) {\\n+    Map<String, Operator<? extends OperatorDesc>> aliasToWork = mapWork.getAliasToWork();\\n+    Map<Path, PartitionDesc> pathToPartitionInfo = mapWork.getPathToPartitionInfo();\\n+    Operator<? extends OperatorDesc> op = aliasToWork.get(dirPath.toString());\\n+    PartitionDesc partitionDesc = pathToPartitionInfo.get(dirPath);\\n+    Path tmpDirPath = Utilities.toTempPath(dirPath);\\n+    if (op != null) {\\n+      aliasToWork.remove(dirPath.toString());\\n+      aliasToWork.put(tmpDirPath.toString(), op);\\n+      mapWork.setAliasToWork(aliasToWork);\\n+    }\\n+    if (partitionDesc != null) {\\n+      pathToPartitionInfo.remove(dirPath);\\n+      pathToPartitionInfo.put(tmpDirPath, partitionDesc);\\n+      mapWork.setPathToPartitionInfo(pathToPartitionInfo);\\n+    }\\n+    mapWork.removePathToAlias(dirPath);\\n+    mapWork.addPathToAlias(tmpDirPath, tmpDirPath.toString());\\n+    if (isTblLevel) {\\n+      List<Path> inputPaths = fileStatuses.stream()\\n+          .filter(FileStatus::isFile)\\n+          .map(FileStatus::getPath).collect(Collectors.toList());\\n+      mapWork.setInputPaths(inputPaths);\\n+    }\\n+    mapWork.setUseInputPathsDirectly(true);\\n+  }\\n+\\n+  private Map<FileStatus, List<FileStatus>> getManifestDirs(FileSystem inpFs, List<FileStatus> fileStatuses)\\n+      throws IOException {\\n+    Map<FileStatus, List<FileStatus>> manifestDirsToPaths = new HashMap<>();\\n+    for (FileStatus fileStatus : fileStatuses) {\\n+      if (!fileStatus.isDirectory()) {\\n+        FileStatus parentDir = inpFs.getFileStatus(fileStatus.getPath().getParent());\\n+        List<FileStatus> fileStatusList = Lists.newArrayList(fileStatus);\\n+        manifestDirsToPaths.merge(parentDir, fileStatusList, (oldValue, newValue) -> {\\n+          oldValue.addAll(newValue);\\n+          return oldValue;\\n+        });\\n+      }\\n+    }\\n+    return manifestDirsToPaths;\\n+  }\\n }\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\\nindex 17e105310c..076ef0a99b 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\\n@@ -180,6 +180,10 @@ public enum LlapIODescriptor {\\n \\n   private ProbeDecodeContext probeDecodeContext = null;\\n \\n+  protected List<Path> inputPaths;\\n+\\n+  private boolean useInputPathsDirectly;\\n+\\n   public MapWork() {}\\n \\n   public MapWork(String name) {\\n@@ -934,4 +938,20 @@ public MapExplainVectorization getMapExplainVectorization() {\\n     }\\n     return new MapExplainVectorization(this);\\n   }\\n+\\n+  public List<Path> getInputPaths() {\\n+    return inputPaths;\\n+  }\\n+\\n+  public void setInputPaths(List<Path> inputPaths) {\\n+    this.inputPaths = inputPaths;\\n+  }\\n+\\n+  public void setUseInputPathsDirectly(boolean useInputPathsDirectly) {\\n+    this.useInputPathsDirectly = useInputPathsDirectly;\\n+  }\\n+\\n+  public boolean isUseInputPathsDirectly() {\\n+    return useInputPathsDirectly;\\n+  }\\n }\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: HIVE-28473\\nIssue Summary: INSERT OVERWRITE LOCAL DIRECTORY writes staging files to wrong hdfs directory\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nusing HIVE 3.1.3 ; mr engine; HADOOP 3.3.4\\n\\n\\n\\n\\xa0\\n\\n\\n\\n*Description*\\n\\n\\n\\nWhen I try to insert data into the local directory \"/path/to/local\", Hive usually first creates an intermediate HDFS directory like \"hdfs:/session/execution/.staging-hive-xx\", which is based on sessionId and executionId. After that, it moves the results to the local filesystem at \"/path/to/local\".\\n\\n\\n\\nHowever, it’s currently trying to create an intermediate HDFS directory at \"hdfs:/path/to/local/.staging-hive-xx\", which incorrectly uses the local filesystem path. This causes an error because it\\'s attempting to create a new path starting from {{{}/root{}}}, where we don\\'t have sufficient permissions.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nIt can be reproduced by:\\n\\n{code:java}\\n\\nINSERT OVERWRITE LOCAL DIRECTORY \"/path/to/local/dir\"\\n\\nselect a \\n\\nfrom table \\n\\ngroup by a; {code}\\n\\n\\xa0\\n\\n\\n\\nStackTrace:\\n\\n{code:java}\\n\\nRuntimeException: cannot create staging directory \"hdfs:/path/to/local/dir/.hive-staging-xx\":\\n\\nPermission denied: user=aaa, access=WRITE, inode=\"/\":hdfs:hdfs:drwxr-xr-x {code}\\n\\n\\xa0\\n\\n\\n\\n*ANALYSE*\\n\\n\\n\\n\\xa0\\n\\n\\n\\nIn function _org.apache.hadoop.hive.ql.parse.SemanticAnalyzer#createFileSinkDesc._ We do the same execution for both _QBMetaData.DEST_LOCAL_FILE_ and _QBMetaData.DEST_DFS_FILE,_\\xa0and then we set the value\\xa0_ctx.getTempDirForInterimJobPath(dest_path).toString() to_\\xa0{_}statsTmpLoc{_}. But for local filesystem dest_path is always totally different from the paths of HADOOP filesystem, and then we get the exception that we cannot create a HDFS directory because we don\\'t have sufficient permissions.\\n\\n\\n\\n\\xa0\\n\\n\\n\\n*SOLUTION*\\n\\n\\n\\n\\xa0\\n\\n\\n\\nwe should modify the function \\xa0_org.apache.hadoop.hive.ql.parse.SemanticAnalyzer#createFileSinkDesc_\\xa0to treat _QBMetaData.DEST_LOCAL_FILE_ and _QBMetaData.DEST_DFS_FILE_ differently by\\xa0giving the value _ctx.getMRTmpPath().toString()_ to _statsTmpLoc_ to avoid creating a wrong intermediate direcoty.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A6959DB80>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A6959DC70>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A6959DD60>\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java b/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java\\nindex 2ed439d375..f76a4d15ea 100644\\n--- a/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java\\n+++ b/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java\\n@@ -42,7 +42,7 @@\\n  * <li>unlimited number of version components,</li>\\n  * <li>version components in the text can be digits or strings,</li>\\n  * <li>strings are checked for well-known qualifiers and the qualifier ordering is used for version ordering.\\n- *     Well-known qualifiers (case insensitive) are:<ul>\\n+ *     Well-known qualifiers (case-insensitive) are:<ul>\\n  *     <li><code>alpha</code> or <code>a</code></li>\\n  *     <li><code>beta</code> or <code>b</code></li>\\n  *     <li><code>milestone</code> or <code>m</code></li>\\n@@ -51,9 +51,9 @@\\n  *     <li><code>(the empty string)</code> or <code>ga</code> or <code>final</code></li>\\n  *     <li><code>sp</code></li>\\n  *     </ul>\\n- *     Unknown qualifiers are considered after known qualifiers, with lexical order (always case insensitive),\\n+ *     Unknown qualifiers are considered after known qualifiers, with lexical order (always case-insensitive),\\n  *   </li>\\n- * <li>a hyphen usually precedes a qualifier, and is always less important than digits/number, for example\\n+ * <li>A hyphen usually precedes a qualifier, and is always less important than digits/number. For example\\n  *   {@code 1.0.RC2 < 1.0-RC3 < 1.0.1}; but prefer {@code 1.0.0-RC1} over {@code 1.0.0.RC1}, and more\\n  *   generally: {@code 1.0.X2 < 1.0-X3 < 1.0.1} for any string {@code X}; but prefer {@code 1.0.0-X1}\\n  *   over {@code 1.0.0.X1}.</li>\\n@@ -656,7 +656,20 @@ public final void parseVersion(String version) {\\n         int startIndex = 0;\\n \\n         for (int i = 0; i < version.length(); i++) {\\n-            char c = version.charAt(i);\\n+            char character = version.charAt(i);\\n+            int c = character;\\n+            if (Character.isHighSurrogate(character)) {\\n+                // read the next character as a low surrogate and combine into a single int\\n+                try {\\n+                    char low = version.charAt(i + 1);\\n+                    char[] both = {character, low};\\n+                    c = Character.codePointAt(both, 0);\\n+                    i++;\\n+                } catch (IndexOutOfBoundsException ex) {\\n+                    // high surrogate without low surrogate. Not a lot we can do here except treat it as a regular\\n+                    // character\\n+                }\\n+            }\\n \\n             if (c == \\'.\\') {\\n                 if (i == startIndex) {\\n@@ -687,7 +700,7 @@ public final void parseVersion(String version) {\\n                     stack.push(list);\\n                 }\\n                 isCombination = false;\\n-            } else if (Character.isDigit(c)) {\\n+            } else if (c >= \\'0\\' && c <= \\'9\\') { // Check for ASCII digits only\\n                 if (!isDigit && i > startIndex) {\\n                     // X1\\n                     isCombination = true;\\ndiff --git a/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java b/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java\\nindex d7616405bd..219d760bab 100644\\n--- a/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java\\n+++ b/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java\\n@@ -27,7 +27,6 @@\\n \\n /**\\n  * Test ComparableVersion.\\n- *\\n  */\\n @SuppressWarnings(\"unchecked\")\\n class ComparableVersionTest {\\n@@ -222,6 +221,23 @@ void testLeadingZeroes() {\\n         checkVersionsOrder(\"0.2\", \"1.0.7\");\\n     }\\n \\n+    @Test\\n+    void testDigitGreaterThanNonAscii() {\\n+        ComparableVersion c1 = new ComparableVersion(\"1\");\\n+        ComparableVersion c2 = new ComparableVersion(\"é\");\\n+        assertTrue(c1.compareTo(c2) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\uD835\\\\uDFE4\");\\n+        assertTrue(c2.compareTo(c1) < 0, \"expected \" + \"\\\\uD835\\\\uDFE4\" + \" < \" + \"1\");\\n+    }\\n+\\n+    @Test\\n+    void testDigitGreaterThanNonBmpCharacters() {\\n+        ComparableVersion c1 = new ComparableVersion(\"1\");\\n+        // MATHEMATICAL SANS-SERIF DIGIT TWO\\n+        ComparableVersion c2 = new ComparableVersion(\"\\\\uD835\\\\uDFE4\");\\n+        assertTrue(c1.compareTo(c2) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\uD835\\\\uDFE4\");\\n+        assertTrue(c2.compareTo(c1) < 0, \"expected \" + \"\\\\uD835\\\\uDFE4\" + \" < \" + \"1\");\\n+    }\\n+\\n     @Test\\n     void testGetCanonical() {\\n         // MNG-7700\\n@@ -238,13 +254,25 @@ void testGetCanonical() {\\n \\n     @Test\\n     void testCompareDigitToLetter() {\\n-        ComparableVersion c1 = new ComparableVersion(\"7\");\\n-        ComparableVersion c2 = new ComparableVersion(\"J\");\\n-        ComparableVersion c3 = new ComparableVersion(\"c\");\\n-        assertTrue(c1.compareTo(c2) > 0, \"expected 7 > J\");\\n-        assertTrue(c2.compareTo(c1) < 0, \"expected J < 1\");\\n-        assertTrue(c1.compareTo(c3) > 0, \"expected 7 > c\");\\n-        assertTrue(c3.compareTo(c1) < 0, \"expected c < 7\");\\n+        ComparableVersion seven = new ComparableVersion(\"7\");\\n+        ComparableVersion capitalJ = new ComparableVersion(\"J\");\\n+        ComparableVersion lowerCaseC = new ComparableVersion(\"c\");\\n+        // Digits are greater than letters\\n+        assertTrue(seven.compareTo(capitalJ) > 0, \"expected 7 > J\");\\n+        assertTrue(capitalJ.compareTo(seven) < 0, \"expected J < 1\");\\n+        assertTrue(seven.compareTo(lowerCaseC) > 0, \"expected 7 > c\");\\n+        assertTrue(lowerCaseC.compareTo(seven) < 0, \"expected c < 7\");\\n+    }\\n+\\n+    @Test\\n+    void testNonAsciiDigits() { // These should not be treated as digits.\\n+        ComparableVersion asciiOne = new ComparableVersion(\"1\");\\n+        ComparableVersion arabicEight = new ComparableVersion(\"\\\\u0668\");\\n+        ComparableVersion asciiNine = new ComparableVersion(\"9\");\\n+        assertTrue(asciiOne.compareTo(arabicEight) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\u0668\");\\n+        assertTrue(arabicEight.compareTo(asciiOne) < 0, \"expected \" + \"\\\\u0668\" + \" < \" + \"1\");\\n+        assertTrue(asciiNine.compareTo(arabicEight) > 0, \"expected \" + \"9\" + \" > \" + \"\\\\u0668\");\\n+        assertTrue(arabicEight.compareTo(asciiNine) < 0, \"expected \" + \"\\\\u0668\" + \" < \" + \"9\");\\n     }\\n \\n     @Test\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: MNG-8349\\nIssue Summary: NumberFormatException during model building\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\n{code}\\n\\n<project>\\n\\n  <modelVersion>4..0.0</modelVersion>\\n\\n  <groupId>org.test</groupId>\\n\\n  <artifactId>foo</artifactId>\\n\\n  <version>1.0-SNAPSHOT</version>\\n\\n  <packaging>jar</packaging>\\n\\n</project>\\n\\n{code}\\n\\n\\n\\nThe above pom.xml leads to the following error:\\n\\n\\n\\n{code}\\n\\n[INFO] Scanning for projects...\\n\\n[ERROR] Internal error: java.lang.NumberFormatException: For input string: \"\" -> [Help 1]\\n\\norg.apache.maven.InternalErrorException: Internal error: java.lang.NumberFormatException: For input string: \"\"\\n\\n    at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:157)\\n\\n    at org.apache.maven.cling.invoker.mvn.DefaultMavenInvoker.doExecute(DefaultMavenInvoker.java:496)\\n\\n    at org.apache.maven.cling.invoker.mvn.DefaultMavenInvoker.execute(DefaultMavenInvoker.java:113)\\n\\n    at org.apache.maven.cling.invoker.mvn.DefaultMavenInvoker.execute(DefaultMavenInvoker.java:80)\\n\\n    at org.apache.maven.cling.invoker.LookupInvoker.doInvoke(LookupInvoker.java:235)\\n\\n    at org.apache.maven.cling.invoker.LookupInvoker.invoke(LookupInvoker.java:210)\\n\\n    at org.apache.maven.cling.ClingSupport.run(ClingSupport.java:68)\\n\\n    at org.apache.maven.cling.MavenCling.main(MavenCling.java:51)\\n\\n    at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\\n\\n    at java.lang.reflect.Method.invoke(Method.java:580)\\n\\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:255)\\n\\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:201)\\n\\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:361)\\n\\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:314)\\n\\nCaused by: java.lang.NumberFormatException: For input string: \"\"\\n\\n    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\\n\\n    at java.lang.Long.parseLong(Long.java:719)\\n\\n    at java.lang.Long.valueOf(Long.java:1157)\\n\\n    at org.apache.maven.internal.impl.model.DefaultModelValidator.compareModelVersions(DefaultModelValidator.java:1967)\\n\\n    at org.apache.maven.internal.impl.model.DefaultModelValidator.validateModelVersion(DefaultModelValidator.java:1911)\\n\\n    at org.apache.maven.internal.impl.model.DefaultModelValidator.validateFileModel(DefaultModelValidator.java:367)\\n\\n    at org.apache.maven.internal.impl.model.DefaultModelBuilder$DefaultModelBuilderSession.doReadFileModel(DefaultModelBuilder.java:1434)\\n\\n    at org.apache.maven.internal.impl.model.DefaultModelCache$CachingSupplier.get(DefaultModelCache.java:178)\\n\\n    at org.apache.maven.internal.impl.model.DefaultModelCache.computeIfAbsent(DefaultModelCache.java:65)\\n\\n    at org.apache.maven.internal.impl.model.DefaultModelCache.computeIfAbsent(DefaultModelCache.java:56)\\n\\n    at org.apache.maven.internal.impl.model.DefaultModelBuilder$DefaultModelBuilderSession.cache(DefaultModelBuilder.java:1734)\\n\\n    at org.apache.maven.internal.impl.model.DefaultModelBuilder$DefaultModelBuilderSession.readFileModel(DefaultModelBuilder.java:1239)\\n\\n    at org.apache.maven.internal.impl.model.DefaultModelBuilder$DefaultModelBuilderSession.loadFilePom(DefaultModelBuilder.java:733)\\n\\n    at org.apache.maven.internal.impl.model.DefaultModelBuilder$DefaultModelBuilderSession.loadFromRoot(DefaultModelBuilder.java:711)\\n\\n    at org.apache.maven.internal.impl.model.DefaultModelBuilder$DefaultModelBuilderSession.buildBuildPom(DefaultModelBuilder.java:658)\\n\\n    at org.apache.maven.internal.impl.model.DefaultModelBuilder$1.build(DefaultModelBuilder.java:226)\\n\\n    at org.apache.maven.project.DefaultProjectBuilder$BuildSession.build(DefaultProjectBuilder.java:497)\\n\\n    at org.apache.maven.project.DefaultProjectBuilder$BuildSession.lambda$doBuild$5(DefaultProjectBuilder.java:474)\\n\\n    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)\\n\\n    at java.util.Collections$2.tryAdvance(Collections.java:5073)\\n\\n    at java.util.Collections$2.forEachRemaining(Collections.java:5081)\\n\\n    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)\\n\\n    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)\\n\\n    at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:921)\\n\\n    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\\n\\n    at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682)\\n\\n    at org.apache.maven.project.DefaultProjectBuilder$BuildSession.doBuild(DefaultProjectBuilder.java:476)\\n\\n    at org.apache.maven.project.DefaultProjectBuilder$BuildSession.build(DefaultProjectBuilder.java:450)\\n\\n    at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:184)\\n\\n    at org.apache.maven.project.collector.DefaultProjectsSelector.selectProjects(DefaultProjectsSelector.java:61)\\n\\n    at org.apache.maven.project.collector.RequestPomCollectionStrategy.collectProjects(RequestPomCollectionStrategy.java:49)\\n\\n    at org.apache.maven.graph.DefaultGraphBuilder.getProjectsForMavenReactor(DefaultGraphBuilder.java:364)\\n\\n    at org.apache.maven.graph.DefaultGraphBuilder.build(DefaultGraphBuilder.java:100)\\n\\n    at org.apache.maven.DefaultMaven.buildGraph(DefaultMaven.java:629)\\n\\n    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:250)\\n\\n    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:225)\\n\\n    at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:149)\\n\\n    at org.apache.maven.cling.invoker.mvn.DefaultMavenInvoker.doExecute(DefaultMavenInvoker.java:496)\\n\\n    at org.apache.maven.cling.invoker.mvn.DefaultMavenInvoker.execute(DefaultMavenInvoker.java:113)\\n\\n    at org.apache.maven.cling.invoker.mvn.DefaultMavenInvoker.execute(DefaultMavenInvoker.java:80)\\n\\n    at org.apache.maven.cling.invoker.LookupInvoker.doInvoke(LookupInvoker.java:235)\\n\\n    at org.apache.maven.cling.invoker.LookupInvoker.invoke(LookupInvoker.java:210)\\n\\n    at org.apache.maven.cling.ClingSupport.run(ClingSupport.java:68)\\n\\n    at org.apache.maven.cling.MavenCling.main(MavenCling.java:51)\\n\\n    at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\\n\\n    at java.lang.reflect.Method.invoke(Method.java:580)\\n\\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:255)\\n\\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:201)\\n\\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:361)\\n\\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:314)\\n\\n[ERROR] \\n\\n[ERROR] To see the full stack trace of the errors, re-run Maven with the \\'-e\\' switch\\n\\n[ERROR] Re-run Maven using the \\'-X\\' switch to enable verbose output\\n\\n[ERROR] \\n\\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\\n\\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/InternalErrorException\\n\\n{code}\\n\\n\\n\\nThe error is somehow expected given the typo in the {{modelVersion}}, but a more friendly exception would be welcomed.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\\nindex 42d91d536b..470d052d89 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\\n@@ -1039,7 +1039,8 @@ public static List<String> mergeUniqElems(List<String> src, List<String> dest) {\\n     return src;\\n   }\\n \\n-  private static final String tmpPrefix = \"_tmp.\";\\n+  private static final String hadoopTmpPrefix = \"_tmp.\";\\n+  private static final String tmpPrefix = \"-tmp.\";\\n   private static final String taskTmpPrefix = \"_task_tmp.\";\\n \\n   public static Path toTaskTempPath(Path orig) {\\n@@ -1070,7 +1071,7 @@ private static boolean isTempPath(FileStatus file) {\\n     String name = file.getPath().getName();\\n     // in addition to detecting hive temporary files, we also check hadoop\\n     // temporary folders that used to show up in older releases\\n-    return (name.startsWith(\"_task\") || name.startsWith(tmpPrefix));\\n+    return (name.startsWith(\"_task\") || name.startsWith(tmpPrefix) || name.startsWith(hadoopTmpPrefix));\\n   }\\n \\n   /**\\n@@ -1393,7 +1394,7 @@ private static String replaceTaskIdFromFilename(String filename, String oldTaskI\\n   }\\n \\n \\n-  private static boolean shouldAvoidRename(FileSinkDesc conf, Configuration hConf) {\\n+  public static boolean shouldAvoidRename(FileSinkDesc conf, Configuration hConf) {\\n     // we are avoiding rename/move only if following conditions are met\\n     //  * execution engine is tez\\n     //  * if it is select query\\n@@ -3524,6 +3525,9 @@ public static List<Path> getInputPaths(JobConf job, MapWork work, Path hiveScrat\\n     Set<Path> pathsProcessed = new HashSet<Path>();\\n     List<Path> pathsToAdd = new LinkedList<Path>();\\n     DriverState driverState = DriverState.getDriverState();\\n+    if (work.isUseInputPathsDirectly() && work.getInputPaths() != null) {\\n+      return work.getInputPaths();\\n+    }\\n     // AliasToWork contains all the aliases\\n     Collection<String> aliasToWork = work.getAliasToWork().keySet();\\n     if (!skipDummy) {\\n@@ -4555,7 +4559,7 @@ private static Path getManifestDir(Path specPath, long writeId, int stmtId, Stri\\n     if (isDelete) {\\n       deltaDir = AcidUtils.deleteDeltaSubdir(writeId, writeId, stmtId);\\n     }\\n-    Path manifestPath = new Path(manifestRoot, \"_tmp.\" + deltaDir);\\n+    Path manifestPath = new Path(manifestRoot, Utilities.toTempPath(deltaDir));\\n \\n     if (isInsertOverwrite) {\\n       // When doing a multi-statement insert overwrite query with dynamic partitioning, the\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java\\nindex 594289eda4..53a4f3a843 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java\\n@@ -46,7 +46,6 @@\\n public class MergeFileWork extends MapWork {\\n \\n   private static final Logger LOG = LoggerFactory.getLogger(MergeFileWork.class);\\n-  private List<Path> inputPaths;\\n   private Path outputDir;\\n   private boolean hasDynamicPartitions;\\n   private boolean isListBucketingAlterTableConcatenate;\\n@@ -84,14 +83,6 @@ public MergeFileWork(List<Path> inputPaths, Path outputDir,\\n     this.isListBucketingAlterTableConcatenate = false;\\n   }\\n \\n-  public List<Path> getInputPaths() {\\n-    return inputPaths;\\n-  }\\n-\\n-  public void setInputPaths(List<Path> inputPaths) {\\n-    this.inputPaths = inputPaths;\\n-  }\\n-\\n   public Path getOutputDir() {\\n     return outputDir;\\n   }\\n@@ -137,8 +128,6 @@ public void resolveDynamicPartitionStoredAsSubDirsMerge(HiveConf conf,\\n         aliases, partDesc);\\n     // set internal input format for all partition descriptors\\n     partDesc.setInputFileFormatClass(internalInputFormat);\\n-    // Add the DP path to the list of input paths\\n-    inputPaths.add(path);\\n   }\\n \\n   /**\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java\\nindex da20eceba3..0e6816ae40 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java\\n@@ -20,18 +20,27 @@\\n \\n import java.io.IOException;\\n import java.io.Serializable;\\n+import java.nio.charset.Charset;\\n import java.util.ArrayList;\\n+import java.util.HashMap;\\n import java.util.HashSet;\\n import java.util.LinkedHashMap;\\n import java.util.List;\\n+import java.util.LongSummaryStatistics;\\n import java.util.Map;\\n+import java.util.stream.Collectors;\\n \\n+import com.google.common.collect.Lists;\\n+import org.apache.commons.io.IOUtils;\\n+import org.apache.hadoop.fs.FSDataInputStream;\\n import org.apache.hadoop.fs.FileStatus;\\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n import org.apache.hadoop.hive.common.HiveStatsUtils;\\n import org.apache.hadoop.hive.conf.HiveConf;\\n import org.apache.hadoop.hive.metastore.Warehouse;\\n+import org.apache.hadoop.hive.metastore.utils.FileUtils;\\n+import org.apache.hadoop.hive.ql.exec.Operator;\\n import org.apache.hadoop.hive.ql.exec.Task;\\n import org.apache.hadoop.hive.ql.exec.Utilities;\\n import org.slf4j.Logger;\\n@@ -151,6 +160,11 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n         }\\n \\n         int lbLevel = (ctx.getLbCtx() == null) ? 0 : ctx.getLbCtx().calculateListBucketingLevel();\\n+        boolean manifestFilePresent = false;\\n+        FileSystem manifestFs = dirPath.getFileSystem(conf);\\n+        if (manifestFs.exists(new Path(dirPath, Utilities.BLOB_MANIFEST_FILE))) {\\n+          manifestFilePresent = true;\\n+        }\\n \\n         /**\\n          * In order to make code easier to read, we write the following in the way:\\n@@ -168,15 +182,25 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n           int dpLbLevel = numDPCols + lbLevel;\\n \\n           generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask,\\n-              mrAndMvTask, dirPath, inpFs, ctx, work, dpLbLevel);\\n+              mrAndMvTask, dirPath, inpFs, ctx, work, dpLbLevel, manifestFilePresent);\\n         } else { // no dynamic partitions\\n           if(lbLevel == 0) {\\n             // static partition without list bucketing\\n-            long totalSz = getMergeSize(inpFs, dirPath, avgConditionSize);\\n-            Utilities.FILE_OP_LOGGER.debug(\"merge resolve simple case - totalSz \" + totalSz + \" from \" + dirPath);\\n+            List<FileStatus> manifestFilePaths = new ArrayList<>();\\n+            long totalSize;\\n+            if (manifestFilePresent) {\\n+              manifestFilePaths = getManifestFilePaths(conf, dirPath);\\n+              totalSize = getMergeSize(manifestFilePaths, avgConditionSize);\\n+            } else {\\n+              totalSize = getMergeSize(inpFs, dirPath, avgConditionSize);\\n+              Utilities.FILE_OP_LOGGER.debug(\"merge resolve simple case - totalSize \" + totalSize + \" from \" + dirPath);\\n+            }\\n \\n-            if (totalSz >= 0) { // add the merge job\\n-              setupMapRedWork(conf, work, trgtSize, totalSz);\\n+            if (totalSize >= 0) { // add the merge job\\n+              if (manifestFilePresent) {\\n+                setupWorkWhenUsingManifestFile(work, manifestFilePaths, dirPath, true);\\n+              }\\n+              setupMapRedWork(conf, work, trgtSize, totalSize);\\n               resTsks.add(mrTask);\\n             } else { // don\\'t need to merge, add the move job\\n               resTsks.add(mvTask);\\n@@ -184,7 +208,7 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n           } else {\\n             // static partition and list bucketing\\n             generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask,\\n-                mrAndMvTask, dirPath, inpFs, ctx, work, lbLevel);\\n+                mrAndMvTask, dirPath, inpFs, ctx, work, lbLevel, manifestFilePresent);\\n           }\\n         }\\n       } else {\\n@@ -229,11 +253,22 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n   private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n       long trgtSize, long avgConditionSize, Task<?> mvTask,\\n       Task<?> mrTask, Task<?> mrAndMvTask, Path dirPath,\\n-      FileSystem inpFs, ConditionalResolverMergeFilesCtx ctx, MapWork work, int dpLbLevel)\\n+      FileSystem inpFs, ConditionalResolverMergeFilesCtx ctx, MapWork work, int dpLbLevel,\\n+      boolean manifestFilePresent)\\n       throws IOException {\\n     DynamicPartitionCtx dpCtx = ctx.getDPCtx();\\n-    // get list of dynamic partitions\\n-    List<FileStatus> statusList = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);\\n+    List<FileStatus> statusList;\\n+    Map<FileStatus, List<FileStatus>> manifestDirToFile = new HashMap<>();\\n+    if (manifestFilePresent) {\\n+      // Get the list of files from manifest file.\\n+      List<FileStatus> fileStatuses = getManifestFilePaths(conf, dirPath);\\n+      // Setup the work to include all the files present in the manifest.\\n+      setupWorkWhenUsingManifestFile(work, fileStatuses, dirPath, false);\\n+      manifestDirToFile = getManifestDirs(inpFs, fileStatuses);\\n+      statusList = new ArrayList<>(manifestDirToFile.keySet());\\n+    } else {\\n+      statusList = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);\\n+    }\\n     FileStatus[] status = statusList.toArray(new FileStatus[statusList.size()]);\\n \\n     // cleanup pathToPartitionInfo\\n@@ -253,15 +288,21 @@ private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n     work.removePathToAlias(path); // the root path is not useful anymore\\n \\n     // populate pathToPartitionInfo and pathToAliases w/ DP paths\\n-    long totalSz = 0;\\n+    long totalSize = 0;\\n     boolean doMerge = false;\\n     // list of paths that don\\'t need to merge but need to move to the dest location\\n-    List<Path> toMove = new ArrayList<Path>();\\n+    List<Path> toMove = new ArrayList<>();\\n+    List<Path> toMerge = new ArrayList<>();\\n     for (int i = 0; i < status.length; ++i) {\\n-      long len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);\\n+      long len;\\n+      if (manifestFilePresent) {\\n+        len = getMergeSize(manifestDirToFile.get(status[i]), avgConditionSize);\\n+      } else {\\n+        len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);\\n+      }\\n       if (len >= 0) {\\n         doMerge = true;\\n-        totalSz += len;\\n+        totalSize += len;\\n         PartitionDesc pDesc = (dpCtx != null) ? generateDPFullPartSpec(dpCtx, status, tblDesc, i)\\n             : partDesc;\\n         if (pDesc == null) {\\n@@ -271,6 +312,13 @@ private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n         Utilities.FILE_OP_LOGGER.debug(\"merge resolver will merge \" + status[i].getPath());\\n         work.resolveDynamicPartitionStoredAsSubDirsMerge(conf, status[i].getPath(), tblDesc,\\n             aliases, pDesc);\\n+        // Do not add input file since its already added when the manifest file is present.\\n+        if (manifestFilePresent) {\\n+          toMerge.addAll(manifestDirToFile.get(status[i])\\n+              .stream().map(FileStatus::getPath).collect(Collectors.toList()));\\n+        } else {\\n+          toMerge.add(status[i].getPath());\\n+        }\\n       } else {\\n         Utilities.FILE_OP_LOGGER.debug(\"merge resolver will move \" + status[i].getPath());\\n \\n@@ -278,8 +326,13 @@ private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n       }\\n     }\\n     if (doMerge) {\\n+      // Set paths appropriately.\\n+      if (work.getInputPaths() != null && !work.getInputPaths().isEmpty()) {\\n+        toMerge.addAll(work.getInputPaths());\\n+      }\\n+      work.setInputPaths(toMerge);\\n       // add the merge MR job\\n-      setupMapRedWork(conf, work, trgtSize, totalSz);\\n+      setupMapRedWork(conf, work, trgtSize, totalSize);\\n \\n       // add the move task for those partitions that do not need merging\\n       if (toMove.size() > 0) {\\n@@ -359,11 +412,11 @@ private void setupMapRedWork(HiveConf conf, MapWork mWork, long targetSize, long\\n     mWork.setIsMergeFromResolver(true);\\n   }\\n \\n-  private static class AverageSize {\\n+  private static class FileSummary {\\n     private final long totalSize;\\n-    private final int numFiles;\\n+    private final long numFiles;\\n \\n-    public AverageSize(long totalSize, int numFiles) {\\n+    public FileSummary(long totalSize, long numFiles) {\\n       this.totalSize = totalSize;\\n       this.numFiles  = numFiles;\\n     }\\n@@ -372,64 +425,106 @@ public long getTotalSize() {\\n       return totalSize;\\n     }\\n \\n-    public int getNumFiles() {\\n+    public long getNumFiles() {\\n       return numFiles;\\n     }\\n   }\\n \\n-  private AverageSize getAverageSize(FileSystem inpFs, Path dirPath) {\\n-    AverageSize error = new AverageSize(-1, -1);\\n-    try {\\n-      FileStatus[] fStats = inpFs.listStatus(dirPath);\\n-\\n-      long totalSz = 0;\\n-      int numFiles = 0;\\n-      for (FileStatus fStat : fStats) {\\n-        Utilities.FILE_OP_LOGGER.debug(\"Resolver looking at \" + fStat.getPath());\\n-        if (fStat.isDir()) {\\n-          AverageSize avgSzDir = getAverageSize(inpFs, fStat.getPath());\\n-          if (avgSzDir.getTotalSize() < 0) {\\n-            return error;\\n-          }\\n-          totalSz += avgSzDir.getTotalSize();\\n-          numFiles += avgSzDir.getNumFiles();\\n-        }\\n-        else {\\n-          totalSz += fStat.getLen();\\n-          numFiles++;\\n-        }\\n-      }\\n+  private FileSummary getFileSummary(List<FileStatus> fileStatusList) {\\n+    LongSummaryStatistics stats = fileStatusList.stream().filter(FileStatus::isFile)\\n+        .mapToLong(FileStatus::getLen).summaryStatistics();\\n+    return new FileSummary(stats.getSum(), stats.getCount());\\n+  }\\n \\n-      return new AverageSize(totalSz, numFiles);\\n-    } catch (IOException e) {\\n-      return error;\\n+  private List<FileStatus> getManifestFilePaths(HiveConf conf, Path dirPath) throws IOException {\\n+    FileSystem manifestFs = dirPath.getFileSystem(conf);\\n+    List<String> filesKept;\\n+    List<FileStatus> pathsKept = new ArrayList<>();\\n+    try (FSDataInputStream inStream = manifestFs.open(new Path(dirPath, Utilities.BLOB_MANIFEST_FILE))) {\\n+      String paths = IOUtils.toString(inStream, Charset.defaultCharset());\\n+      filesKept = Lists.newArrayList(paths.split(System.lineSeparator()));\\n     }\\n+    // The first string contains the directory information. Not useful.\\n+    filesKept.remove(0);\\n+\\n+    for (String file : filesKept) {\\n+      pathsKept.add(manifestFs.getFileStatus(new Path(file)));\\n+    }\\n+    return pathsKept;\\n+  }\\n+\\n+  private long getMergeSize(FileSystem inpFs, Path dirPath, long avgSize) {\\n+    List<FileStatus> result = FileUtils.getFileStatusRecurse(dirPath, inpFs);\\n+    return getMergeSize(result, avgSize);\\n   }\\n \\n   /**\\n    * Whether to merge files inside directory given the threshold of the average file size.\\n    *\\n-   * @param inpFs input file system.\\n-   * @param dirPath input file directory.\\n+   * @param fileStatuses a list of FileStatus instances.\\n    * @param avgSize threshold of average file size.\\n    * @return -1 if not need to merge (either because of there is only 1 file or the\\n    * average size is larger than avgSize). Otherwise the size of the total size of files.\\n    * If return value is 0 that means there are multiple files each of which is an empty file.\\n    * This could be true when the table is bucketized and all buckets are empty.\\n    */\\n-  private long getMergeSize(FileSystem inpFs, Path dirPath, long avgSize) {\\n-    AverageSize averageSize = getAverageSize(inpFs, dirPath);\\n-    if (averageSize.getTotalSize() < 0) {\\n+  private long getMergeSize(List<FileStatus> fileStatuses, long avgSize) {\\n+    FileSummary fileSummary = getFileSummary(fileStatuses);\\n+    if (fileSummary.getTotalSize() <= 0) {\\n       return -1;\\n     }\\n \\n-    if (averageSize.getNumFiles() <= 1) {\\n+    if (fileSummary.getNumFiles() <= 1) {\\n       return -1;\\n     }\\n \\n-    if (averageSize.getTotalSize()/averageSize.getNumFiles() < avgSize) {\\n-      return averageSize.getTotalSize();\\n+    if (fileSummary.getTotalSize() / fileSummary.getNumFiles() < avgSize) {\\n+      return fileSummary.getTotalSize();\\n     }\\n     return -1;\\n   }\\n+\\n+  private void setupWorkWhenUsingManifestFile(MapWork mapWork, List<FileStatus> fileStatuses, Path dirPath,\\n+                                              boolean isTblLevel) {\\n+    Map<String, Operator<? extends OperatorDesc>> aliasToWork = mapWork.getAliasToWork();\\n+    Map<Path, PartitionDesc> pathToPartitionInfo = mapWork.getPathToPartitionInfo();\\n+    Operator<? extends OperatorDesc> op = aliasToWork.get(dirPath.toString());\\n+    PartitionDesc partitionDesc = pathToPartitionInfo.get(dirPath);\\n+    Path tmpDirPath = Utilities.toTempPath(dirPath);\\n+    if (op != null) {\\n+      aliasToWork.remove(dirPath.toString());\\n+      aliasToWork.put(tmpDirPath.toString(), op);\\n+      mapWork.setAliasToWork(aliasToWork);\\n+    }\\n+    if (partitionDesc != null) {\\n+      pathToPartitionInfo.remove(dirPath);\\n+      pathToPartitionInfo.put(tmpDirPath, partitionDesc);\\n+      mapWork.setPathToPartitionInfo(pathToPartitionInfo);\\n+    }\\n+    mapWork.removePathToAlias(dirPath);\\n+    mapWork.addPathToAlias(tmpDirPath, tmpDirPath.toString());\\n+    if (isTblLevel) {\\n+      List<Path> inputPaths = fileStatuses.stream()\\n+          .filter(FileStatus::isFile)\\n+          .map(FileStatus::getPath).collect(Collectors.toList());\\n+      mapWork.setInputPaths(inputPaths);\\n+    }\\n+    mapWork.setUseInputPathsDirectly(true);\\n+  }\\n+\\n+  private Map<FileStatus, List<FileStatus>> getManifestDirs(FileSystem inpFs, List<FileStatus> fileStatuses)\\n+      throws IOException {\\n+    Map<FileStatus, List<FileStatus>> manifestDirsToPaths = new HashMap<>();\\n+    for (FileStatus fileStatus : fileStatuses) {\\n+      if (!fileStatus.isDirectory()) {\\n+        FileStatus parentDir = inpFs.getFileStatus(fileStatus.getPath().getParent());\\n+        List<FileStatus> fileStatusList = Lists.newArrayList(fileStatus);\\n+        manifestDirsToPaths.merge(parentDir, fileStatusList, (oldValue, newValue) -> {\\n+          oldValue.addAll(newValue);\\n+          return oldValue;\\n+        });\\n+      }\\n+    }\\n+    return manifestDirsToPaths;\\n+  }\\n }\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\\nindex 17e105310c..076ef0a99b 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\\n@@ -180,6 +180,10 @@ public enum LlapIODescriptor {\\n \\n   private ProbeDecodeContext probeDecodeContext = null;\\n \\n+  protected List<Path> inputPaths;\\n+\\n+  private boolean useInputPathsDirectly;\\n+\\n   public MapWork() {}\\n \\n   public MapWork(String name) {\\n@@ -934,4 +938,20 @@ public MapExplainVectorization getMapExplainVectorization() {\\n     }\\n     return new MapExplainVectorization(this);\\n   }\\n+\\n+  public List<Path> getInputPaths() {\\n+    return inputPaths;\\n+  }\\n+\\n+  public void setInputPaths(List<Path> inputPaths) {\\n+    this.inputPaths = inputPaths;\\n+  }\\n+\\n+  public void setUseInputPathsDirectly(boolean useInputPathsDirectly) {\\n+    this.useInputPathsDirectly = useInputPathsDirectly;\\n+  }\\n+\\n+  public boolean isUseInputPathsDirectly() {\\n+    return useInputPathsDirectly;\\n+  }\\n }\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: HIVE-27536\\nIssue Summary: Merge task must be invoked after optimisation for external CTAS queries\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nMerge task is not invoked on S3 file system / object stores when CTAS query is performed.\\xa0\\n\\nRepro test - Test.q\\n\\n{code:java}\\n\\n--! qt:dataset:src\\n\\nset hive.mapred.mode=nonstrict;\\n\\nset hive.explain.user=false;\\n\\nset hive.merge.mapredfiles=true;\\n\\nset hive.merge.mapfiles=true;\\n\\nset hive.merge.tezfiles=true;\\n\\nset hive.blobstore.supported.schemes=hdfs,file;\\n\\nset hive.merge.smallfiles.avgsize=7500;\\n\\n\\n\\n-- SORT_QUERY_RESULTS\\n\\n\\n\\ncreate table part_source(key string, value string) partitioned by (ds string);\\n\\ncreate table source(key string);\\n\\n\\n\\n-- The partitioned table must have 2 files per partition (necessary for merge task)\\n\\ninsert overwrite table part_source partition(ds=\\'102\\') select * from src;\\n\\ninsert into table part_source partition(ds=\\'102\\') select * from src;\\n\\ninsert overwrite table part_source partition(ds=\\'103\\') select * from src;\\n\\ninsert into table part_source partition(ds=\\'102\\') select * from src;\\n\\n\\n\\n-- The unpartitioned table must have 2 files.\\n\\ninsert overwrite table source select key from src;\\n\\ninsert into table source select key from src;\\n\\n\\n\\n-- Create CTAS tables both for unpartitioned and partitioned cases for ORC formats.\\n\\nexplain analyze create external table ctas_table stored as orc as select * from source;\\n\\ncreate external table ctas_table stored as orc as select * from source;\\n\\nexplain analyze create external table ctas_part_table partitioned by (ds) stored as orc as select * from part_source;\\n\\ncreate external table ctas_part_table partitioned by (ds) stored as orc as select * from part_source;\\n\\n\\n\\n-- This must be 1 indicating there is 1 file after merge.\\n\\nselect count(distinct(INPUT__FILE__NAME)) from ctas_table;\\n\\n-- This must be 2 indicating there is 1 file per partition after merge.\\n\\nselect count(distinct(INPUT__FILE__NAME)) from ctas_part_table;\\n\\n\\n\\n-- Create CTAS tables both for unpartitioned and partitioned cases for non-ORC formats.\\n\\nexplain analyze create external table ctas_table_non_orc as select * from source;\\n\\ncreate external table ctas_table_non_orc as select * from source;\\n\\nexplain analyze create external table ctas_part_table_non_orc partitioned by (ds) as select * from part_source;\\n\\ncreate external table ctas_part_table_non_orc partitioned by (ds) as select * from part_source;\\n\\n\\n\\n-- This must be 1 indicating there is 1 file after merge.\\n\\nselect count(distinct(INPUT__FILE__NAME)) from ctas_table_non_orc;\\n\\n-- This must be 2 indicating there is 1 file per partition after merge.\\n\\nselect count(distinct(INPUT__FILE__NAME)) from ctas_part_table_non_orc;\\n\\n{code}\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java b/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java\\nindex 2ed439d375..f76a4d15ea 100644\\n--- a/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java\\n+++ b/compat/maven-artifact/src/main/java/org/apache/maven/artifact/versioning/ComparableVersion.java\\n@@ -42,7 +42,7 @@\\n  * <li>unlimited number of version components,</li>\\n  * <li>version components in the text can be digits or strings,</li>\\n  * <li>strings are checked for well-known qualifiers and the qualifier ordering is used for version ordering.\\n- *     Well-known qualifiers (case insensitive) are:<ul>\\n+ *     Well-known qualifiers (case-insensitive) are:<ul>\\n  *     <li><code>alpha</code> or <code>a</code></li>\\n  *     <li><code>beta</code> or <code>b</code></li>\\n  *     <li><code>milestone</code> or <code>m</code></li>\\n@@ -51,9 +51,9 @@\\n  *     <li><code>(the empty string)</code> or <code>ga</code> or <code>final</code></li>\\n  *     <li><code>sp</code></li>\\n  *     </ul>\\n- *     Unknown qualifiers are considered after known qualifiers, with lexical order (always case insensitive),\\n+ *     Unknown qualifiers are considered after known qualifiers, with lexical order (always case-insensitive),\\n  *   </li>\\n- * <li>a hyphen usually precedes a qualifier, and is always less important than digits/number, for example\\n+ * <li>A hyphen usually precedes a qualifier, and is always less important than digits/number. For example\\n  *   {@code 1.0.RC2 < 1.0-RC3 < 1.0.1}; but prefer {@code 1.0.0-RC1} over {@code 1.0.0.RC1}, and more\\n  *   generally: {@code 1.0.X2 < 1.0-X3 < 1.0.1} for any string {@code X}; but prefer {@code 1.0.0-X1}\\n  *   over {@code 1.0.0.X1}.</li>\\n@@ -656,7 +656,20 @@ public final void parseVersion(String version) {\\n         int startIndex = 0;\\n \\n         for (int i = 0; i < version.length(); i++) {\\n-            char c = version.charAt(i);\\n+            char character = version.charAt(i);\\n+            int c = character;\\n+            if (Character.isHighSurrogate(character)) {\\n+                // read the next character as a low surrogate and combine into a single int\\n+                try {\\n+                    char low = version.charAt(i + 1);\\n+                    char[] both = {character, low};\\n+                    c = Character.codePointAt(both, 0);\\n+                    i++;\\n+                } catch (IndexOutOfBoundsException ex) {\\n+                    // high surrogate without low surrogate. Not a lot we can do here except treat it as a regular\\n+                    // character\\n+                }\\n+            }\\n \\n             if (c == \\'.\\') {\\n                 if (i == startIndex) {\\n@@ -687,7 +700,7 @@ public final void parseVersion(String version) {\\n                     stack.push(list);\\n                 }\\n                 isCombination = false;\\n-            } else if (Character.isDigit(c)) {\\n+            } else if (c >= \\'0\\' && c <= \\'9\\') { // Check for ASCII digits only\\n                 if (!isDigit && i > startIndex) {\\n                     // X1\\n                     isCombination = true;\\ndiff --git a/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java b/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java\\nindex d7616405bd..219d760bab 100644\\n--- a/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java\\n+++ b/compat/maven-artifact/src/test/java/org/apache/maven/artifact/versioning/ComparableVersionTest.java\\n@@ -27,7 +27,6 @@\\n \\n /**\\n  * Test ComparableVersion.\\n- *\\n  */\\n @SuppressWarnings(\"unchecked\")\\n class ComparableVersionTest {\\n@@ -222,6 +221,23 @@ void testLeadingZeroes() {\\n         checkVersionsOrder(\"0.2\", \"1.0.7\");\\n     }\\n \\n+    @Test\\n+    void testDigitGreaterThanNonAscii() {\\n+        ComparableVersion c1 = new ComparableVersion(\"1\");\\n+        ComparableVersion c2 = new ComparableVersion(\"é\");\\n+        assertTrue(c1.compareTo(c2) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\uD835\\\\uDFE4\");\\n+        assertTrue(c2.compareTo(c1) < 0, \"expected \" + \"\\\\uD835\\\\uDFE4\" + \" < \" + \"1\");\\n+    }\\n+\\n+    @Test\\n+    void testDigitGreaterThanNonBmpCharacters() {\\n+        ComparableVersion c1 = new ComparableVersion(\"1\");\\n+        // MATHEMATICAL SANS-SERIF DIGIT TWO\\n+        ComparableVersion c2 = new ComparableVersion(\"\\\\uD835\\\\uDFE4\");\\n+        assertTrue(c1.compareTo(c2) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\uD835\\\\uDFE4\");\\n+        assertTrue(c2.compareTo(c1) < 0, \"expected \" + \"\\\\uD835\\\\uDFE4\" + \" < \" + \"1\");\\n+    }\\n+\\n     @Test\\n     void testGetCanonical() {\\n         // MNG-7700\\n@@ -238,13 +254,25 @@ void testGetCanonical() {\\n \\n     @Test\\n     void testCompareDigitToLetter() {\\n-        ComparableVersion c1 = new ComparableVersion(\"7\");\\n-        ComparableVersion c2 = new ComparableVersion(\"J\");\\n-        ComparableVersion c3 = new ComparableVersion(\"c\");\\n-        assertTrue(c1.compareTo(c2) > 0, \"expected 7 > J\");\\n-        assertTrue(c2.compareTo(c1) < 0, \"expected J < 1\");\\n-        assertTrue(c1.compareTo(c3) > 0, \"expected 7 > c\");\\n-        assertTrue(c3.compareTo(c1) < 0, \"expected c < 7\");\\n+        ComparableVersion seven = new ComparableVersion(\"7\");\\n+        ComparableVersion capitalJ = new ComparableVersion(\"J\");\\n+        ComparableVersion lowerCaseC = new ComparableVersion(\"c\");\\n+        // Digits are greater than letters\\n+        assertTrue(seven.compareTo(capitalJ) > 0, \"expected 7 > J\");\\n+        assertTrue(capitalJ.compareTo(seven) < 0, \"expected J < 1\");\\n+        assertTrue(seven.compareTo(lowerCaseC) > 0, \"expected 7 > c\");\\n+        assertTrue(lowerCaseC.compareTo(seven) < 0, \"expected c < 7\");\\n+    }\\n+\\n+    @Test\\n+    void testNonAsciiDigits() { // These should not be treated as digits.\\n+        ComparableVersion asciiOne = new ComparableVersion(\"1\");\\n+        ComparableVersion arabicEight = new ComparableVersion(\"\\\\u0668\");\\n+        ComparableVersion asciiNine = new ComparableVersion(\"9\");\\n+        assertTrue(asciiOne.compareTo(arabicEight) > 0, \"expected \" + \"1\" + \" > \" + \"\\\\u0668\");\\n+        assertTrue(arabicEight.compareTo(asciiOne) < 0, \"expected \" + \"\\\\u0668\" + \" < \" + \"1\");\\n+        assertTrue(asciiNine.compareTo(arabicEight) > 0, \"expected \" + \"9\" + \" > \" + \"\\\\u0668\");\\n+        assertTrue(arabicEight.compareTo(asciiNine) < 0, \"expected \" + \"\\\\u0668\" + \" < \" + \"9\");\\n     }\\n \\n     @Test\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: MNG-7486\\nIssue Summary: Create a multiline message helper for boxed log messages\\nIssue Type: Task\\nPriority: Major\\n\\nDescription:\\nSimplify the way how boxed messages, e.g., for non-threadsafe plugins is created.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\\nindex 42d91d536b..470d052d89 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\\n@@ -1039,7 +1039,8 @@ public static List<String> mergeUniqElems(List<String> src, List<String> dest) {\\n     return src;\\n   }\\n \\n-  private static final String tmpPrefix = \"_tmp.\";\\n+  private static final String hadoopTmpPrefix = \"_tmp.\";\\n+  private static final String tmpPrefix = \"-tmp.\";\\n   private static final String taskTmpPrefix = \"_task_tmp.\";\\n \\n   public static Path toTaskTempPath(Path orig) {\\n@@ -1070,7 +1071,7 @@ private static boolean isTempPath(FileStatus file) {\\n     String name = file.getPath().getName();\\n     // in addition to detecting hive temporary files, we also check hadoop\\n     // temporary folders that used to show up in older releases\\n-    return (name.startsWith(\"_task\") || name.startsWith(tmpPrefix));\\n+    return (name.startsWith(\"_task\") || name.startsWith(tmpPrefix) || name.startsWith(hadoopTmpPrefix));\\n   }\\n \\n   /**\\n@@ -1393,7 +1394,7 @@ private static String replaceTaskIdFromFilename(String filename, String oldTaskI\\n   }\\n \\n \\n-  private static boolean shouldAvoidRename(FileSinkDesc conf, Configuration hConf) {\\n+  public static boolean shouldAvoidRename(FileSinkDesc conf, Configuration hConf) {\\n     // we are avoiding rename/move only if following conditions are met\\n     //  * execution engine is tez\\n     //  * if it is select query\\n@@ -3524,6 +3525,9 @@ public static List<Path> getInputPaths(JobConf job, MapWork work, Path hiveScrat\\n     Set<Path> pathsProcessed = new HashSet<Path>();\\n     List<Path> pathsToAdd = new LinkedList<Path>();\\n     DriverState driverState = DriverState.getDriverState();\\n+    if (work.isUseInputPathsDirectly() && work.getInputPaths() != null) {\\n+      return work.getInputPaths();\\n+    }\\n     // AliasToWork contains all the aliases\\n     Collection<String> aliasToWork = work.getAliasToWork().keySet();\\n     if (!skipDummy) {\\n@@ -4555,7 +4559,7 @@ private static Path getManifestDir(Path specPath, long writeId, int stmtId, Stri\\n     if (isDelete) {\\n       deltaDir = AcidUtils.deleteDeltaSubdir(writeId, writeId, stmtId);\\n     }\\n-    Path manifestPath = new Path(manifestRoot, \"_tmp.\" + deltaDir);\\n+    Path manifestPath = new Path(manifestRoot, Utilities.toTempPath(deltaDir));\\n \\n     if (isInsertOverwrite) {\\n       // When doing a multi-statement insert overwrite query with dynamic partitioning, the\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java\\nindex 594289eda4..53a4f3a843 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java\\n@@ -46,7 +46,6 @@\\n public class MergeFileWork extends MapWork {\\n \\n   private static final Logger LOG = LoggerFactory.getLogger(MergeFileWork.class);\\n-  private List<Path> inputPaths;\\n   private Path outputDir;\\n   private boolean hasDynamicPartitions;\\n   private boolean isListBucketingAlterTableConcatenate;\\n@@ -84,14 +83,6 @@ public MergeFileWork(List<Path> inputPaths, Path outputDir,\\n     this.isListBucketingAlterTableConcatenate = false;\\n   }\\n \\n-  public List<Path> getInputPaths() {\\n-    return inputPaths;\\n-  }\\n-\\n-  public void setInputPaths(List<Path> inputPaths) {\\n-    this.inputPaths = inputPaths;\\n-  }\\n-\\n   public Path getOutputDir() {\\n     return outputDir;\\n   }\\n@@ -137,8 +128,6 @@ public void resolveDynamicPartitionStoredAsSubDirsMerge(HiveConf conf,\\n         aliases, partDesc);\\n     // set internal input format for all partition descriptors\\n     partDesc.setInputFileFormatClass(internalInputFormat);\\n-    // Add the DP path to the list of input paths\\n-    inputPaths.add(path);\\n   }\\n \\n   /**\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java\\nindex da20eceba3..0e6816ae40 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java\\n@@ -20,18 +20,27 @@\\n \\n import java.io.IOException;\\n import java.io.Serializable;\\n+import java.nio.charset.Charset;\\n import java.util.ArrayList;\\n+import java.util.HashMap;\\n import java.util.HashSet;\\n import java.util.LinkedHashMap;\\n import java.util.List;\\n+import java.util.LongSummaryStatistics;\\n import java.util.Map;\\n+import java.util.stream.Collectors;\\n \\n+import com.google.common.collect.Lists;\\n+import org.apache.commons.io.IOUtils;\\n+import org.apache.hadoop.fs.FSDataInputStream;\\n import org.apache.hadoop.fs.FileStatus;\\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n import org.apache.hadoop.hive.common.HiveStatsUtils;\\n import org.apache.hadoop.hive.conf.HiveConf;\\n import org.apache.hadoop.hive.metastore.Warehouse;\\n+import org.apache.hadoop.hive.metastore.utils.FileUtils;\\n+import org.apache.hadoop.hive.ql.exec.Operator;\\n import org.apache.hadoop.hive.ql.exec.Task;\\n import org.apache.hadoop.hive.ql.exec.Utilities;\\n import org.slf4j.Logger;\\n@@ -151,6 +160,11 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n         }\\n \\n         int lbLevel = (ctx.getLbCtx() == null) ? 0 : ctx.getLbCtx().calculateListBucketingLevel();\\n+        boolean manifestFilePresent = false;\\n+        FileSystem manifestFs = dirPath.getFileSystem(conf);\\n+        if (manifestFs.exists(new Path(dirPath, Utilities.BLOB_MANIFEST_FILE))) {\\n+          manifestFilePresent = true;\\n+        }\\n \\n         /**\\n          * In order to make code easier to read, we write the following in the way:\\n@@ -168,15 +182,25 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n           int dpLbLevel = numDPCols + lbLevel;\\n \\n           generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask,\\n-              mrAndMvTask, dirPath, inpFs, ctx, work, dpLbLevel);\\n+              mrAndMvTask, dirPath, inpFs, ctx, work, dpLbLevel, manifestFilePresent);\\n         } else { // no dynamic partitions\\n           if(lbLevel == 0) {\\n             // static partition without list bucketing\\n-            long totalSz = getMergeSize(inpFs, dirPath, avgConditionSize);\\n-            Utilities.FILE_OP_LOGGER.debug(\"merge resolve simple case - totalSz \" + totalSz + \" from \" + dirPath);\\n+            List<FileStatus> manifestFilePaths = new ArrayList<>();\\n+            long totalSize;\\n+            if (manifestFilePresent) {\\n+              manifestFilePaths = getManifestFilePaths(conf, dirPath);\\n+              totalSize = getMergeSize(manifestFilePaths, avgConditionSize);\\n+            } else {\\n+              totalSize = getMergeSize(inpFs, dirPath, avgConditionSize);\\n+              Utilities.FILE_OP_LOGGER.debug(\"merge resolve simple case - totalSize \" + totalSize + \" from \" + dirPath);\\n+            }\\n \\n-            if (totalSz >= 0) { // add the merge job\\n-              setupMapRedWork(conf, work, trgtSize, totalSz);\\n+            if (totalSize >= 0) { // add the merge job\\n+              if (manifestFilePresent) {\\n+                setupWorkWhenUsingManifestFile(work, manifestFilePaths, dirPath, true);\\n+              }\\n+              setupMapRedWork(conf, work, trgtSize, totalSize);\\n               resTsks.add(mrTask);\\n             } else { // don\\'t need to merge, add the move job\\n               resTsks.add(mvTask);\\n@@ -184,7 +208,7 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n           } else {\\n             // static partition and list bucketing\\n             generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask,\\n-                mrAndMvTask, dirPath, inpFs, ctx, work, lbLevel);\\n+                mrAndMvTask, dirPath, inpFs, ctx, work, lbLevel, manifestFilePresent);\\n           }\\n         }\\n       } else {\\n@@ -229,11 +253,22 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {\\n   private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n       long trgtSize, long avgConditionSize, Task<?> mvTask,\\n       Task<?> mrTask, Task<?> mrAndMvTask, Path dirPath,\\n-      FileSystem inpFs, ConditionalResolverMergeFilesCtx ctx, MapWork work, int dpLbLevel)\\n+      FileSystem inpFs, ConditionalResolverMergeFilesCtx ctx, MapWork work, int dpLbLevel,\\n+      boolean manifestFilePresent)\\n       throws IOException {\\n     DynamicPartitionCtx dpCtx = ctx.getDPCtx();\\n-    // get list of dynamic partitions\\n-    List<FileStatus> statusList = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);\\n+    List<FileStatus> statusList;\\n+    Map<FileStatus, List<FileStatus>> manifestDirToFile = new HashMap<>();\\n+    if (manifestFilePresent) {\\n+      // Get the list of files from manifest file.\\n+      List<FileStatus> fileStatuses = getManifestFilePaths(conf, dirPath);\\n+      // Setup the work to include all the files present in the manifest.\\n+      setupWorkWhenUsingManifestFile(work, fileStatuses, dirPath, false);\\n+      manifestDirToFile = getManifestDirs(inpFs, fileStatuses);\\n+      statusList = new ArrayList<>(manifestDirToFile.keySet());\\n+    } else {\\n+      statusList = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);\\n+    }\\n     FileStatus[] status = statusList.toArray(new FileStatus[statusList.size()]);\\n \\n     // cleanup pathToPartitionInfo\\n@@ -253,15 +288,21 @@ private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n     work.removePathToAlias(path); // the root path is not useful anymore\\n \\n     // populate pathToPartitionInfo and pathToAliases w/ DP paths\\n-    long totalSz = 0;\\n+    long totalSize = 0;\\n     boolean doMerge = false;\\n     // list of paths that don\\'t need to merge but need to move to the dest location\\n-    List<Path> toMove = new ArrayList<Path>();\\n+    List<Path> toMove = new ArrayList<>();\\n+    List<Path> toMerge = new ArrayList<>();\\n     for (int i = 0; i < status.length; ++i) {\\n-      long len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);\\n+      long len;\\n+      if (manifestFilePresent) {\\n+        len = getMergeSize(manifestDirToFile.get(status[i]), avgConditionSize);\\n+      } else {\\n+        len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);\\n+      }\\n       if (len >= 0) {\\n         doMerge = true;\\n-        totalSz += len;\\n+        totalSize += len;\\n         PartitionDesc pDesc = (dpCtx != null) ? generateDPFullPartSpec(dpCtx, status, tblDesc, i)\\n             : partDesc;\\n         if (pDesc == null) {\\n@@ -271,6 +312,13 @@ private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n         Utilities.FILE_OP_LOGGER.debug(\"merge resolver will merge \" + status[i].getPath());\\n         work.resolveDynamicPartitionStoredAsSubDirsMerge(conf, status[i].getPath(), tblDesc,\\n             aliases, pDesc);\\n+        // Do not add input file since its already added when the manifest file is present.\\n+        if (manifestFilePresent) {\\n+          toMerge.addAll(manifestDirToFile.get(status[i])\\n+              .stream().map(FileStatus::getPath).collect(Collectors.toList()));\\n+        } else {\\n+          toMerge.add(status[i].getPath());\\n+        }\\n       } else {\\n         Utilities.FILE_OP_LOGGER.debug(\"merge resolver will move \" + status[i].getPath());\\n \\n@@ -278,8 +326,13 @@ private void generateActualTasks(HiveConf conf, List<Task<?>> resTsks,\\n       }\\n     }\\n     if (doMerge) {\\n+      // Set paths appropriately.\\n+      if (work.getInputPaths() != null && !work.getInputPaths().isEmpty()) {\\n+        toMerge.addAll(work.getInputPaths());\\n+      }\\n+      work.setInputPaths(toMerge);\\n       // add the merge MR job\\n-      setupMapRedWork(conf, work, trgtSize, totalSz);\\n+      setupMapRedWork(conf, work, trgtSize, totalSize);\\n \\n       // add the move task for those partitions that do not need merging\\n       if (toMove.size() > 0) {\\n@@ -359,11 +412,11 @@ private void setupMapRedWork(HiveConf conf, MapWork mWork, long targetSize, long\\n     mWork.setIsMergeFromResolver(true);\\n   }\\n \\n-  private static class AverageSize {\\n+  private static class FileSummary {\\n     private final long totalSize;\\n-    private final int numFiles;\\n+    private final long numFiles;\\n \\n-    public AverageSize(long totalSize, int numFiles) {\\n+    public FileSummary(long totalSize, long numFiles) {\\n       this.totalSize = totalSize;\\n       this.numFiles  = numFiles;\\n     }\\n@@ -372,64 +425,106 @@ public long getTotalSize() {\\n       return totalSize;\\n     }\\n \\n-    public int getNumFiles() {\\n+    public long getNumFiles() {\\n       return numFiles;\\n     }\\n   }\\n \\n-  private AverageSize getAverageSize(FileSystem inpFs, Path dirPath) {\\n-    AverageSize error = new AverageSize(-1, -1);\\n-    try {\\n-      FileStatus[] fStats = inpFs.listStatus(dirPath);\\n-\\n-      long totalSz = 0;\\n-      int numFiles = 0;\\n-      for (FileStatus fStat : fStats) {\\n-        Utilities.FILE_OP_LOGGER.debug(\"Resolver looking at \" + fStat.getPath());\\n-        if (fStat.isDir()) {\\n-          AverageSize avgSzDir = getAverageSize(inpFs, fStat.getPath());\\n-          if (avgSzDir.getTotalSize() < 0) {\\n-            return error;\\n-          }\\n-          totalSz += avgSzDir.getTotalSize();\\n-          numFiles += avgSzDir.getNumFiles();\\n-        }\\n-        else {\\n-          totalSz += fStat.getLen();\\n-          numFiles++;\\n-        }\\n-      }\\n+  private FileSummary getFileSummary(List<FileStatus> fileStatusList) {\\n+    LongSummaryStatistics stats = fileStatusList.stream().filter(FileStatus::isFile)\\n+        .mapToLong(FileStatus::getLen).summaryStatistics();\\n+    return new FileSummary(stats.getSum(), stats.getCount());\\n+  }\\n \\n-      return new AverageSize(totalSz, numFiles);\\n-    } catch (IOException e) {\\n-      return error;\\n+  private List<FileStatus> getManifestFilePaths(HiveConf conf, Path dirPath) throws IOException {\\n+    FileSystem manifestFs = dirPath.getFileSystem(conf);\\n+    List<String> filesKept;\\n+    List<FileStatus> pathsKept = new ArrayList<>();\\n+    try (FSDataInputStream inStream = manifestFs.open(new Path(dirPath, Utilities.BLOB_MANIFEST_FILE))) {\\n+      String paths = IOUtils.toString(inStream, Charset.defaultCharset());\\n+      filesKept = Lists.newArrayList(paths.split(System.lineSeparator()));\\n     }\\n+    // The first string contains the directory information. Not useful.\\n+    filesKept.remove(0);\\n+\\n+    for (String file : filesKept) {\\n+      pathsKept.add(manifestFs.getFileStatus(new Path(file)));\\n+    }\\n+    return pathsKept;\\n+  }\\n+\\n+  private long getMergeSize(FileSystem inpFs, Path dirPath, long avgSize) {\\n+    List<FileStatus> result = FileUtils.getFileStatusRecurse(dirPath, inpFs);\\n+    return getMergeSize(result, avgSize);\\n   }\\n \\n   /**\\n    * Whether to merge files inside directory given the threshold of the average file size.\\n    *\\n-   * @param inpFs input file system.\\n-   * @param dirPath input file directory.\\n+   * @param fileStatuses a list of FileStatus instances.\\n    * @param avgSize threshold of average file size.\\n    * @return -1 if not need to merge (either because of there is only 1 file or the\\n    * average size is larger than avgSize). Otherwise the size of the total size of files.\\n    * If return value is 0 that means there are multiple files each of which is an empty file.\\n    * This could be true when the table is bucketized and all buckets are empty.\\n    */\\n-  private long getMergeSize(FileSystem inpFs, Path dirPath, long avgSize) {\\n-    AverageSize averageSize = getAverageSize(inpFs, dirPath);\\n-    if (averageSize.getTotalSize() < 0) {\\n+  private long getMergeSize(List<FileStatus> fileStatuses, long avgSize) {\\n+    FileSummary fileSummary = getFileSummary(fileStatuses);\\n+    if (fileSummary.getTotalSize() <= 0) {\\n       return -1;\\n     }\\n \\n-    if (averageSize.getNumFiles() <= 1) {\\n+    if (fileSummary.getNumFiles() <= 1) {\\n       return -1;\\n     }\\n \\n-    if (averageSize.getTotalSize()/averageSize.getNumFiles() < avgSize) {\\n-      return averageSize.getTotalSize();\\n+    if (fileSummary.getTotalSize() / fileSummary.getNumFiles() < avgSize) {\\n+      return fileSummary.getTotalSize();\\n     }\\n     return -1;\\n   }\\n+\\n+  private void setupWorkWhenUsingManifestFile(MapWork mapWork, List<FileStatus> fileStatuses, Path dirPath,\\n+                                              boolean isTblLevel) {\\n+    Map<String, Operator<? extends OperatorDesc>> aliasToWork = mapWork.getAliasToWork();\\n+    Map<Path, PartitionDesc> pathToPartitionInfo = mapWork.getPathToPartitionInfo();\\n+    Operator<? extends OperatorDesc> op = aliasToWork.get(dirPath.toString());\\n+    PartitionDesc partitionDesc = pathToPartitionInfo.get(dirPath);\\n+    Path tmpDirPath = Utilities.toTempPath(dirPath);\\n+    if (op != null) {\\n+      aliasToWork.remove(dirPath.toString());\\n+      aliasToWork.put(tmpDirPath.toString(), op);\\n+      mapWork.setAliasToWork(aliasToWork);\\n+    }\\n+    if (partitionDesc != null) {\\n+      pathToPartitionInfo.remove(dirPath);\\n+      pathToPartitionInfo.put(tmpDirPath, partitionDesc);\\n+      mapWork.setPathToPartitionInfo(pathToPartitionInfo);\\n+    }\\n+    mapWork.removePathToAlias(dirPath);\\n+    mapWork.addPathToAlias(tmpDirPath, tmpDirPath.toString());\\n+    if (isTblLevel) {\\n+      List<Path> inputPaths = fileStatuses.stream()\\n+          .filter(FileStatus::isFile)\\n+          .map(FileStatus::getPath).collect(Collectors.toList());\\n+      mapWork.setInputPaths(inputPaths);\\n+    }\\n+    mapWork.setUseInputPathsDirectly(true);\\n+  }\\n+\\n+  private Map<FileStatus, List<FileStatus>> getManifestDirs(FileSystem inpFs, List<FileStatus> fileStatuses)\\n+      throws IOException {\\n+    Map<FileStatus, List<FileStatus>> manifestDirsToPaths = new HashMap<>();\\n+    for (FileStatus fileStatus : fileStatuses) {\\n+      if (!fileStatus.isDirectory()) {\\n+        FileStatus parentDir = inpFs.getFileStatus(fileStatus.getPath().getParent());\\n+        List<FileStatus> fileStatusList = Lists.newArrayList(fileStatus);\\n+        manifestDirsToPaths.merge(parentDir, fileStatusList, (oldValue, newValue) -> {\\n+          oldValue.addAll(newValue);\\n+          return oldValue;\\n+        });\\n+      }\\n+    }\\n+    return manifestDirsToPaths;\\n+  }\\n }\\ndiff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\\nindex 17e105310c..076ef0a99b 100644\\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\\n@@ -180,6 +180,10 @@ public enum LlapIODescriptor {\\n \\n   private ProbeDecodeContext probeDecodeContext = null;\\n \\n+  protected List<Path> inputPaths;\\n+\\n+  private boolean useInputPathsDirectly;\\n+\\n   public MapWork() {}\\n \\n   public MapWork(String name) {\\n@@ -934,4 +938,20 @@ public MapExplainVectorization getMapExplainVectorization() {\\n     }\\n     return new MapExplainVectorization(this);\\n   }\\n+\\n+  public List<Path> getInputPaths() {\\n+    return inputPaths;\\n+  }\\n+\\n+  public void setInputPaths(List<Path> inputPaths) {\\n+    this.inputPaths = inputPaths;\\n+  }\\n+\\n+  public void setUseInputPathsDirectly(boolean useInputPathsDirectly) {\\n+    this.useInputPathsDirectly = useInputPathsDirectly;\\n+  }\\n+\\n+  public boolean isUseInputPathsDirectly() {\\n+    return useInputPathsDirectly;\\n+  }\\n }\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: HIVE-28662\\nIssue Summary: Enable the dynamic leader election for HMS\\nIssue Type: Improvement\\nPriority: Major\\n\\nDescription:\\nIn the Hive ACID world, we must enable the metastore.compactor.initiator.on and metastore.compactor.cleaner.on on HMS to trigger the table compaction automatically. In a real warehouse, we might have multiple HMS instances behind, or even deploy for different purposes, running the tasks like these on all instances could be a waste of resources, make the HMS scale out not so easily.\\xa0\\n\\n\\n\\nThe HIVE-26509 introduces the dynamic leader election. Compared to the old one, the administrator doesn\\'t need to know the deployment beforehand, and a shared properties can be used across all the HMS instances, simple yet effective.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java\\nindex 87949e36b34..e60e645b3bb 100644\\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java\\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java\\n@@ -65,6 +65,8 @@ public enum AzureServiceErrorCode {\\n   COPY_BLOB_ABORTED(\"CopyBlobAborted\", HttpURLConnection.HTTP_INTERNAL_ERROR, null),\\n   BLOB_OPERATION_NOT_SUPPORTED(\"BlobOperationNotSupported\", HttpURLConnection.HTTP_CONFLICT, null),\\n   INVALID_APPEND_OPERATION(\"InvalidAppendOperation\", HttpURLConnection.HTTP_CONFLICT, null),\\n+  UNAUTHORIZED_BLOB_OVERWRITE(\"UnauthorizedBlobOverwrite\", HttpURLConnection.HTTP_FORBIDDEN,\\n+          \"This request is not authorized to perform blob overwrites.\"),\\n   UNKNOWN(null, -1, null);\\n \\n   private final String errorCode;\\ndiff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java\\nindex 8505b533ce4..f571cb86cca 100644\\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java\\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java\\n@@ -42,6 +42,7 @@\\n \\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\\n import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\\n import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore;\\n import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\\n@@ -132,6 +133,8 @@\\n import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_RETAIN_UNCOMMITTED_DATA;\\n import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.RENAME_DESTINATION_PARENT_PATH_NOT_FOUND;\\n import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.SOURCE_PATH_NOT_FOUND;\\n+import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.UNAUTHORIZED_BLOB_OVERWRITE;\\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_FILE_ALREADY_EXISTS;\\n \\n /**\\n  * AbfsClient interacting with the DFS Endpoint.\\n@@ -702,6 +705,14 @@ public AbfsClientRenameResult renamePath(\\n         throw e;\\n       }\\n \\n+      // ref: HADOOP-19393. Write permission checks can occur before validating\\n+      // rename operation\\'s validity. If there is an existing destination path, it may be rejected\\n+      // with an authorization error. Catching and throwing FileAlreadyExistsException instead.\\n+      if (op.getResult().getStorageErrorCode()\\n+          .equals(UNAUTHORIZED_BLOB_OVERWRITE.getErrorCode())){\\n+        throw new FileAlreadyExistsException(ERR_FILE_ALREADY_EXISTS);\\n+      }\\n+\\n       // ref: HADOOP-18242. Rename failure occurring due to a rare case of\\n       // tracking metadata being in incomplete state.\\n       if (op.getResult().getStorageErrorCode()\\ndiff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java\\nindex a5864290b13..87c8869d6c6 100644\\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java\\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java\\n@@ -28,6 +28,7 @@\\n @InterfaceAudience.Public\\n @InterfaceStability.Evolving\\n public final class AbfsErrors {\\n+  public static final String ERR_FILE_ALREADY_EXISTS = \"File already exists.\";\\n   public static final String ERR_WRITE_WITHOUT_LEASE = \"Attempted to write to file without lease\";\\n   public static final String ERR_LEASE_EXPIRED = \"A lease ID was specified, but the lease for the resource has expired.\";\\n   public static final String ERR_LEASE_EXPIRED_BLOB = \"A lease ID was specified, but the lease for the blob has expired.\";\\ndiff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java\\nindex d9b6f0f123f..70e5b23eadd 100644\\n--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java\\n+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java\\n@@ -34,6 +34,7 @@\\n \\n import org.apache.hadoop.fs.FSDataInputStream;\\n import org.apache.hadoop.fs.FSDataOutputStream;\\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\\n import org.apache.hadoop.fs.FileStatus;\\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n@@ -52,6 +53,7 @@\\n \\n import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_SAS_TOKEN_PROVIDER_TYPE;\\n import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.AUTHORIZATION_PERMISSION_MISS_MATCH;\\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_FILE_ALREADY_EXISTS;\\n import static org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers.aclEntry;\\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathDoesNotExist;\\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathExists;\\n@@ -213,6 +215,18 @@ public void testReadAndWrite() throws Exception {\\n     }\\n   }\\n \\n+  @Test\\n+  public void checkExceptionForRenameOverwrites() throws Exception {\\n+    final AzureBlobFileSystem fs = getFileSystem();\\n+\\n+    Path src = new Path(\"a/b/f1.txt\");\\n+    Path dest = new Path(\"a/b/f2.txt\");\\n+    touch(src);\\n+    touch(dest);\\n+\\n+    intercept(FileAlreadyExistsException.class, ERR_FILE_ALREADY_EXISTS, () -> fs.rename(src, dest));\\n+  }\\n+\\n   @Test\\n   // Test rename file and rename folder\\n   public void testRename() throws Exception {\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: HADOOP-19393\\nIssue Summary: ABFS: Returning FileAlreadyExists Exception for UnauthorizedBlobOverwrite Rename Errors\\nIssue Type: Bug\\nPriority: Minor\\n\\nDescription:\\nABFS driver adheres to Hadoop\\'s expectations which does not allow rename blob overwrites. Recently we came across the case where UnauthorizedBlobOverwrite error (HTTP 403- Access Denied Exception) is thrown for rename overwrites (with SAS authentication).\\n\\n\\n\\n\\n\\nRemapping this error to FileAlreadyExists exception for better understanding.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java\\nindex 87949e36b34..e60e645b3bb 100644\\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java\\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java\\n@@ -65,6 +65,8 @@ public enum AzureServiceErrorCode {\\n   COPY_BLOB_ABORTED(\"CopyBlobAborted\", HttpURLConnection.HTTP_INTERNAL_ERROR, null),\\n   BLOB_OPERATION_NOT_SUPPORTED(\"BlobOperationNotSupported\", HttpURLConnection.HTTP_CONFLICT, null),\\n   INVALID_APPEND_OPERATION(\"InvalidAppendOperation\", HttpURLConnection.HTTP_CONFLICT, null),\\n+  UNAUTHORIZED_BLOB_OVERWRITE(\"UnauthorizedBlobOverwrite\", HttpURLConnection.HTTP_FORBIDDEN,\\n+          \"This request is not authorized to perform blob overwrites.\"),\\n   UNKNOWN(null, -1, null);\\n \\n   private final String errorCode;\\ndiff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java\\nindex 8505b533ce4..f571cb86cca 100644\\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java\\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java\\n@@ -42,6 +42,7 @@\\n \\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\\n import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\\n import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore;\\n import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\\n@@ -132,6 +133,8 @@\\n import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_RETAIN_UNCOMMITTED_DATA;\\n import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.RENAME_DESTINATION_PARENT_PATH_NOT_FOUND;\\n import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.SOURCE_PATH_NOT_FOUND;\\n+import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.UNAUTHORIZED_BLOB_OVERWRITE;\\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_FILE_ALREADY_EXISTS;\\n \\n /**\\n  * AbfsClient interacting with the DFS Endpoint.\\n@@ -702,6 +705,14 @@ public AbfsClientRenameResult renamePath(\\n         throw e;\\n       }\\n \\n+      // ref: HADOOP-19393. Write permission checks can occur before validating\\n+      // rename operation\\'s validity. If there is an existing destination path, it may be rejected\\n+      // with an authorization error. Catching and throwing FileAlreadyExistsException instead.\\n+      if (op.getResult().getStorageErrorCode()\\n+          .equals(UNAUTHORIZED_BLOB_OVERWRITE.getErrorCode())){\\n+        throw new FileAlreadyExistsException(ERR_FILE_ALREADY_EXISTS);\\n+      }\\n+\\n       // ref: HADOOP-18242. Rename failure occurring due to a rare case of\\n       // tracking metadata being in incomplete state.\\n       if (op.getResult().getStorageErrorCode()\\ndiff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java\\nindex a5864290b13..87c8869d6c6 100644\\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java\\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java\\n@@ -28,6 +28,7 @@\\n @InterfaceAudience.Public\\n @InterfaceStability.Evolving\\n public final class AbfsErrors {\\n+  public static final String ERR_FILE_ALREADY_EXISTS = \"File already exists.\";\\n   public static final String ERR_WRITE_WITHOUT_LEASE = \"Attempted to write to file without lease\";\\n   public static final String ERR_LEASE_EXPIRED = \"A lease ID was specified, but the lease for the resource has expired.\";\\n   public static final String ERR_LEASE_EXPIRED_BLOB = \"A lease ID was specified, but the lease for the blob has expired.\";\\ndiff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java\\nindex d9b6f0f123f..70e5b23eadd 100644\\n--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java\\n+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java\\n@@ -34,6 +34,7 @@\\n \\n import org.apache.hadoop.fs.FSDataInputStream;\\n import org.apache.hadoop.fs.FSDataOutputStream;\\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\\n import org.apache.hadoop.fs.FileStatus;\\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n@@ -52,6 +53,7 @@\\n \\n import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_SAS_TOKEN_PROVIDER_TYPE;\\n import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.AUTHORIZATION_PERMISSION_MISS_MATCH;\\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_FILE_ALREADY_EXISTS;\\n import static org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers.aclEntry;\\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathDoesNotExist;\\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathExists;\\n@@ -213,6 +215,18 @@ public void testReadAndWrite() throws Exception {\\n     }\\n   }\\n \\n+  @Test\\n+  public void checkExceptionForRenameOverwrites() throws Exception {\\n+    final AzureBlobFileSystem fs = getFileSystem();\\n+\\n+    Path src = new Path(\"a/b/f1.txt\");\\n+    Path dest = new Path(\"a/b/f2.txt\");\\n+    touch(src);\\n+    touch(dest);\\n+\\n+    intercept(FileAlreadyExistsException.class, ERR_FILE_ALREADY_EXISTS, () -> fs.rename(src, dest));\\n+  }\\n+\\n   @Test\\n   // Test rename file and rename folder\\n   public void testRename() throws Exception {\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: HADOOP-19339\\nIssue Summary: OutofBounds Exception due to assumption about buffer size in BlockCompressorStream\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nh3. What Happened:\\xa0\\n\\n\\n\\nGot an OutofBounds exception when io.compression.codec.snappy.buffersize is set to 7. BlockCompressorStream assumes that the buffer size will always be greater than the compression overhead, and consequently MAX_INPUT_SIZE will always be greater than or equal to 0.\\xa0\\n\\nh3. Buggy Code:\\xa0\\n\\n\\n\\nWhen io.compression.codec.snappy.buffersize is set to 7, compressionOverhead is 33 and MAX_INPUT_SIZE is -26.\\xa0\\n\\n{code:java}\\n\\npublic BlockCompressorStream(OutputStream out, Compressor compressor, \\n\\n                             int bufferSize, int compressionOverhead) {\\n\\n  super(out, compressor, bufferSize);\\n\\n  MAX_INPUT_SIZE = bufferSize - compressionOverhead; // -> Assumes bufferSize is always greater than compressionOverhead and MAX_INPUT_SIZE is non-negative. \\n\\n} {code}\\n\\nh3. Stack Trace:\\xa0\\n\\n{code:java}\\n\\njava.lang.ArrayIndexOutOfBoundsException\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 at org.apache.hadoop.io.compress.snappy.SnappyCompressor.setInput(SnappyCompressor.java:86)\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 at org.apache.hadoop.io.compress.BlockCompressorStream.write(BlockCompressorStream.java:112) {code}\\n\\nh3. How to Reproduce:\\xa0\\n\\n\\n\\n(1) Set io.compression.codec.snappy.buffersize to 7\\n\\n\\n\\n(2) Run test: org.apache.hadoop.io.compress.TestCodec#testSnappyMapFile\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java\\nindex 87949e36b34..e60e645b3bb 100644\\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java\\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AzureServiceErrorCode.java\\n@@ -65,6 +65,8 @@ public enum AzureServiceErrorCode {\\n   COPY_BLOB_ABORTED(\"CopyBlobAborted\", HttpURLConnection.HTTP_INTERNAL_ERROR, null),\\n   BLOB_OPERATION_NOT_SUPPORTED(\"BlobOperationNotSupported\", HttpURLConnection.HTTP_CONFLICT, null),\\n   INVALID_APPEND_OPERATION(\"InvalidAppendOperation\", HttpURLConnection.HTTP_CONFLICT, null),\\n+  UNAUTHORIZED_BLOB_OVERWRITE(\"UnauthorizedBlobOverwrite\", HttpURLConnection.HTTP_FORBIDDEN,\\n+          \"This request is not authorized to perform blob overwrites.\"),\\n   UNKNOWN(null, -1, null);\\n \\n   private final String errorCode;\\ndiff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java\\nindex 8505b533ce4..f571cb86cca 100644\\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java\\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java\\n@@ -42,6 +42,7 @@\\n \\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\\n import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\\n import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore;\\n import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\\n@@ -132,6 +133,8 @@\\n import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_RETAIN_UNCOMMITTED_DATA;\\n import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.RENAME_DESTINATION_PARENT_PATH_NOT_FOUND;\\n import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.SOURCE_PATH_NOT_FOUND;\\n+import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.UNAUTHORIZED_BLOB_OVERWRITE;\\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_FILE_ALREADY_EXISTS;\\n \\n /**\\n  * AbfsClient interacting with the DFS Endpoint.\\n@@ -702,6 +705,14 @@ public AbfsClientRenameResult renamePath(\\n         throw e;\\n       }\\n \\n+      // ref: HADOOP-19393. Write permission checks can occur before validating\\n+      // rename operation\\'s validity. If there is an existing destination path, it may be rejected\\n+      // with an authorization error. Catching and throwing FileAlreadyExistsException instead.\\n+      if (op.getResult().getStorageErrorCode()\\n+          .equals(UNAUTHORIZED_BLOB_OVERWRITE.getErrorCode())){\\n+        throw new FileAlreadyExistsException(ERR_FILE_ALREADY_EXISTS);\\n+      }\\n+\\n       // ref: HADOOP-18242. Rename failure occurring due to a rare case of\\n       // tracking metadata being in incomplete state.\\n       if (op.getResult().getStorageErrorCode()\\ndiff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java\\nindex a5864290b13..87c8869d6c6 100644\\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java\\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsErrors.java\\n@@ -28,6 +28,7 @@\\n @InterfaceAudience.Public\\n @InterfaceStability.Evolving\\n public final class AbfsErrors {\\n+  public static final String ERR_FILE_ALREADY_EXISTS = \"File already exists.\";\\n   public static final String ERR_WRITE_WITHOUT_LEASE = \"Attempted to write to file without lease\";\\n   public static final String ERR_LEASE_EXPIRED = \"A lease ID was specified, but the lease for the resource has expired.\";\\n   public static final String ERR_LEASE_EXPIRED_BLOB = \"A lease ID was specified, but the lease for the blob has expired.\";\\ndiff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java\\nindex d9b6f0f123f..70e5b23eadd 100644\\n--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java\\n+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelegationSAS.java\\n@@ -34,6 +34,7 @@\\n \\n import org.apache.hadoop.fs.FSDataInputStream;\\n import org.apache.hadoop.fs.FSDataOutputStream;\\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\\n import org.apache.hadoop.fs.FileStatus;\\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n@@ -52,6 +53,7 @@\\n \\n import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_SAS_TOKEN_PROVIDER_TYPE;\\n import static org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode.AUTHORIZATION_PERMISSION_MISS_MATCH;\\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_FILE_ALREADY_EXISTS;\\n import static org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers.aclEntry;\\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathDoesNotExist;\\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathExists;\\n@@ -213,6 +215,18 @@ public void testReadAndWrite() throws Exception {\\n     }\\n   }\\n \\n+  @Test\\n+  public void checkExceptionForRenameOverwrites() throws Exception {\\n+    final AzureBlobFileSystem fs = getFileSystem();\\n+\\n+    Path src = new Path(\"a/b/f1.txt\");\\n+    Path dest = new Path(\"a/b/f2.txt\");\\n+    touch(src);\\n+    touch(dest);\\n+\\n+    intercept(FileAlreadyExistsException.class, ERR_FILE_ALREADY_EXISTS, () -> fs.rename(src, dest));\\n+  }\\n+\\n   @Test\\n   // Test rename file and rename folder\\n   public void testRename() throws Exception {\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: HADOOP-19031\\nIssue Summary: CVE-2024-23454: Apache Hadoop: Temporary File Local Information Disclosure\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nApache Hadoop’s RunJar.run() does not set permissions for temporary directory by default. If sensitive data will be present in this file, all the other local users may be able to view the content.\\n\\n\\n\\nThis is because, on unix-like systems, the system temporary directory is shared between all local users. As such, files written in this directory, without setting the correct posix permissions explicitly, may be viewable by all other local users.\\n\\n\\n\\nCredit: Andrea Cosentino (finder)\\n\\n\\n\\nSee: https://www.cve.org/CVERecord?id=CVE-2024-23454\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695D83C0>\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695D99A0>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695D9C20>\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B3700>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B3CF0>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A6959DD60>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688E90E0>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695DB4D0>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688E9130>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695DB1B0>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695DBC50>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695DBD40>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695DBE30>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695DBF20>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695DBB10>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'45'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-79b778868d-scrfv'), (b'x-envoy-upstream-service-time', b'31'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999942'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'req_349d2e2f35cdf6ebac935d0aab89fccd'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=OciuMlhfAR29dvylq.PHNTDdkW0HHmJLEvzOjtL4ba8-1740654836-1.0.1.1-1FsyegkdLh5qTd0BfW0reFqvPe_j1u65xeJXyAQ7edw9qySOZ5SOm0c.0KuaoZxeEcKfn85uyjT3SN29U9VCZQ; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=17Rd2rxlYeN5x4wyUFnTJWWfROYbbCtYnIaS.PrCq80-1740654836382-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf93b86dfd37-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'79'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-5b77d74cb4-pxhhz'), (b'x-envoy-upstream-service-time', b'41'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999931'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'req_06c3ee34d61cc967a403ffe0da7c5a7b'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=4_qFkdD4B1wdSPzcQfkhiBRHJDKBrJcdWxSHNFLbK_0-1740654836-1.0.1.1-vw7eMmI66FuDHmXy7CWL31GnxHy8cj.NG8DcCO1nLSIy8As1tuqMKvkgA3TBnsU9sLZla5WE8jl2qRAtQtX.iA; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=6AFkQq5maWQoeGhpD3HifsNRV5lHkbJdYi1TCX7MeB8-1740654836393-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf93ca62cdee-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688EBE80>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Thu, 27 Feb 2025 11:13:56 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '45'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-79b778868d-scrfv'), ('x-envoy-upstream-service-time', '31'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999942'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '3ms'), ('x-request-id', 'req_349d2e2f35cdf6ebac935d0aab89fccd'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=OciuMlhfAR29dvylq.PHNTDdkW0HHmJLEvzOjtL4ba8-1740654836-1.0.1.1-1FsyegkdLh5qTd0BfW0reFqvPe_j1u65xeJXyAQ7edw9qySOZ5SOm0c.0KuaoZxeEcKfn85uyjT3SN29U9VCZQ; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=17Rd2rxlYeN5x4wyUFnTJWWfROYbbCtYnIaS.PrCq80-1740654836382-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9187bf93b86dfd37-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_349d2e2f35cdf6ebac935d0aab89fccd\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'56'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-6cf9fb8d46-d9sth'), (b'x-envoy-upstream-service-time', b'33'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999959'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'req_42f90a2b93528fff427e694d655194be'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=VsQJGPYpftxIidj70KFt3jcgdNBkdFOu80bjCw5jz1o-1740654836-1.0.1.1-Fn9gSD8UyzGD2mSGGT8f9Jf6cMiAbUOmf0r.rYU5rzwJJcKOojdc8kjIwILJ0BaQIzhViKGBmrY83D.GhSKuAQ; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=DZoI16JQ4_EM_KbXzm._8unpmS7AoJZmG5xuhRL4gAY-1740654836414-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf93bde599a9-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/core/src/main/java/kafka/log/remote/RemoteLogManager.java b/core/src/main/java/kafka/log/remote/RemoteLogManager.java\\nindex c1c87d579e..3eacbea475 100644\\n--- a/core/src/main/java/kafka/log/remote/RemoteLogManager.java\\n+++ b/core/src/main/java/kafka/log/remote/RemoteLogManager.java\\n@@ -983,7 +983,9 @@ public class RemoteLogManager implements Closeable {\\n                     }\\n                 }\\n                 if (shouldDeleteSegment) {\\n-                    logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    if (!logStartOffset.isPresent() || logStartOffset.getAsLong() < metadata.endOffset() + 1) {\\n+                        logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    }\\n                     logger.info(\"About to delete remote log segment {} due to retention size {} breach. Log size after deletion will be {}.\",\\n                             metadata.remoteLogSegmentId(), retentionSizeData.get().retentionSize, remainingBreachedSize + retentionSizeData.get().retentionSize);\\n                 }\\n@@ -1000,7 +1002,9 @@ public class RemoteLogManager implements Closeable {\\n                     remainingBreachedSize = Math.max(0, remainingBreachedSize - metadata.segmentSizeInBytes());\\n                     // It is fine to have logStartOffset as `metadata.endOffset() + 1` as the segment offset intervals\\n                     // are ascending with in an epoch.\\n-                    logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    if (!logStartOffset.isPresent() || logStartOffset.getAsLong() < metadata.endOffset() + 1) {\\n+                        logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    }\\n                     logger.info(\"About to delete remote log segment {} due to retention time {}ms breach based on the largest record timestamp in the segment\",\\n                             metadata.remoteLogSegmentId(), retentionTimeData.get().retentionMs);\\n                 }\\ndiff --git a/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java b/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java\\nindex 4c4976f060..3c9b8a48e9 100644\\n--- a/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java\\n+++ b/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java\\n@@ -2055,6 +2055,75 @@ public class RemoteLogManagerTest {\\n         assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n     }\\n \\n+    @ParameterizedTest(name = \"testDeletionOnOverlappingRetentionBreachedSegments retentionSize={0} retentionMs={1}\")\\n+    @CsvSource(value = {\"0, -1\", \"-1, 0\"})\\n+    public void testDeletionOnOverlappingRetentionBreachedSegments(long retentionSize,\\n+                                                                   long retentionMs)\\n+            throws RemoteStorageException, ExecutionException, InterruptedException {\\n+        Map<String, Long> logProps = new HashMap<>();\\n+        logProps.put(\"retention.bytes\", retentionSize);\\n+        logProps.put(\"retention.ms\", retentionMs);\\n+        LogConfig mockLogConfig = new LogConfig(logProps);\\n+        when(mockLog.config()).thenReturn(mockLogConfig);\\n+\\n+        List<EpochEntry> epochEntries = Collections.singletonList(epochEntry0);\\n+        checkpoint.write(epochEntries);\\n+        LeaderEpochFileCache cache = new LeaderEpochFileCache(tp, checkpoint, scheduler);\\n+        when(mockLog.leaderEpochCache()).thenReturn(Option.apply(cache));\\n+\\n+        when(mockLog.topicPartition()).thenReturn(leaderTopicIdPartition.topicPartition());\\n+        when(mockLog.logEndOffset()).thenReturn(200L);\\n+\\n+        RemoteLogSegmentMetadata metadata1 = listRemoteLogSegmentMetadata(leaderTopicIdPartition, 1, 100, 1024,\\n+                epochEntries, RemoteLogSegmentState.COPY_SEGMENT_FINISHED)\\n+                .get(0);\\n+        // overlapping segment\\n+        RemoteLogSegmentMetadata metadata2 = new RemoteLogSegmentMetadata(new RemoteLogSegmentId(leaderTopicIdPartition, Uuid.randomUuid()),\\n+                metadata1.startOffset(), metadata1.endOffset() + 5, metadata1.maxTimestampMs(),\\n+                metadata1.brokerId() + 1, metadata1.eventTimestampMs(), metadata1.segmentSizeInBytes() + 128,\\n+                metadata1.customMetadata(), metadata1.state(), metadata1.segmentLeaderEpochs());\\n+\\n+        // When there are overlapping/duplicate segments, the RemoteLogMetadataManager#listRemoteLogSegments\\n+        // returns the segments in order of (valid ++ unreferenced) segments:\\n+        // (eg) B0 uploaded segment S0 with offsets 0-100 and B1 uploaded segment S1 with offsets 0-200.\\n+        //      We will mark the segment S0 as duplicate and add it to unreferencedSegmentIds.\\n+        //      The order of segments returned by listRemoteLogSegments will be S1, S0.\\n+        // While computing the next-log-start-offset, taking the max of deleted segment\\'s end-offset + 1.\\n+        List<RemoteLogSegmentMetadata> metadataList = new ArrayList<>();\\n+        metadataList.add(metadata2);\\n+        metadataList.add(metadata1);\\n+\\n+        when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition))\\n+                .thenReturn(metadataList.iterator());\\n+        when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition, 0))\\n+                .thenAnswer(ans -> metadataList.iterator());\\n+        when(remoteLogMetadataManager.updateRemoteLogSegmentMetadata(any(RemoteLogSegmentMetadataUpdate.class)))\\n+                .thenReturn(CompletableFuture.runAsync(() -> { }));\\n+\\n+        // Verify the metrics for remote deletes and for failures is zero before attempt to delete segments\\n+        assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n+        assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n+        // Verify aggregate metrics\\n+        assertEquals(0, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n+        assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n+\\n+        RemoteLogManager.RLMTask task = remoteLogManager.new RLMTask(leaderTopicIdPartition, 128);\\n+        task.convertToLeader(0);\\n+        task.cleanupExpiredRemoteLogSegments();\\n+\\n+        assertEquals(metadata2.endOffset() + 1, currentLogStartOffset.get());\\n+        verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(0));\\n+        verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(1));\\n+\\n+        // Verify the metric for remote delete is updated correctly\\n+        assertEquals(2, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n+        // Verify we did not report any failure for remote deletes\\n+        assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n+        // Verify aggregate metrics\\n+        assertEquals(2, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n+        assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n+    }\\n+\\n     @ParameterizedTest(name = \"testRemoteDeleteLagsOnRetentionBreachedSegments retentionSize={0} retentionMs={1}\")\\n     @CsvSource(value = {\"0, -1\", \"-1, 0\"})\\n     public void testRemoteDeleteLagsOnRetentionBreachedSegments(long retentionSize,\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: KAFKA-16890\\nIssue Summary: Failing to build aux state on broker failover\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nWe have clusters where we replace machines often falling into a state where we keep having \"Error building remote log auxiliary state for loadtest_topic-22\" and the partition being under-replicated until the leader is manually restarted.\\xa0\\n\\n\\n\\nLooking into a specific case, here is what we observed in __remote_log_metadata topic:\\n\\n\\n\\n\\n\\n{code:java}\\n\\n\\xa0\\n\\npartition: 29, offset: 183593, value: RemoteLogSegmentMetadata{remoteLogSegmentId=RemoteLogSegmentId{topicIdPartition=ClnIeN0MQsi_d4FAOFKaDA:loadtest_topic-22, id=GZeRTXLMSNe2BQjRXkg6hQ}, startOffset=10823, endOffset=11536, brokerId=10013, maxTimestampMs=1715774588597, eventTimestampMs=1715781657604, segmentLeaderEpochs={125=10823, 126=10968, 128=11047, 130=11048, 131=11324, 133=11442, 134=11443, 135=11445, 136=11521, 137=11533, 139=11535}, segmentSizeInBytes=704895, customMetadata=Optional.empty, state=COPY_SEGMENT_STARTED}\\n\\npartition: 29, offset: 183594, value: RemoteLogSegmentMetadataUpdate{remoteLogSegmentId=RemoteLogSegmentId{topicIdPartition=ClnIeN0MQsi_d4FAOFKaDA:loadtest_topic-22, id=GZeRTXLMSNe2BQjRXkg6hQ}, customMetadata=Optional.empty, state=COPY_SEGMENT_FINISHED, eventTimestampMs=1715781658183, brokerId=10013}\\n\\npartition: 29, offset: 183669, value: RemoteLogSegmentMetadata{remoteLogSegmentId=RemoteLogSegmentId{topicIdPartition=ClnIeN0MQsi_d4FAOFKaDA:loadtest_topic-22, id=L1TYzx0lQkagRIF86Kp0QQ}, startOffset=10823, endOffset=11544, brokerId=10008, maxTimestampMs=1715781445270, eventTimestampMs=1715782717593, segmentLeaderEpochs={125=10823, 126=10968, 128=11047, 130=11048, 131=11324, 133=11442, 134=11443, 135=11445, 136=11521, 137=11533, 139=11535, 140=11537, 142=11543}, segmentSizeInBytes=713088, customMetadata=Optional.empty, state=COPY_SEGMENT_STARTED}\\n\\npartition: 29, offset: 183670, value: RemoteLogSegmentMetadataUpdate{remoteLogSegmentId=RemoteLogSegmentId{topicIdPartition=ClnIeN0MQsi_d4FAOFKaDA:loadtest_topic-22, id=L1TYzx0lQkagRIF86Kp0QQ}, customMetadata=Optional.empty, state=COPY_SEGMENT_FINISHED, eventTimestampMs=1715782718370, brokerId=10008}\\n\\npartition: 29, offset: 186215, value: RemoteLogSegmentMetadataUpdate{remoteLogSegmentId=RemoteLogSegmentId{topicIdPartition=ClnIeN0MQsi_d4FAOFKaDA:loadtest_topic-22, id=L1TYzx0lQkagRIF86Kp0QQ}, customMetadata=Optional.empty, state=DELETE_SEGMENT_STARTED, eventTimestampMs=1715867874617, brokerId=10008}\\n\\npartition: 29, offset: 186216, value: RemoteLogSegmentMetadataUpdate{remoteLogSegmentId=RemoteLogSegmentId{topicIdPartition=ClnIeN0MQsi_d4FAOFKaDA:loadtest_topic-22, id=L1TYzx0lQkagRIF86Kp0QQ}, customMetadata=Optional.empty, state=DELETE_SEGMENT_FINISHED, eventTimestampMs=1715867874725, brokerId=10008}\\n\\npartition: 29, offset: 186217, value: RemoteLogSegmentMetadataUpdate{remoteLogSegmentId=RemoteLogSegmentId{topicIdPartition=ClnIeN0MQsi_d4FAOFKaDA:loadtest_topic-22, id=GZeRTXLMSNe2BQjRXkg6hQ}, customMetadata=Optional.empty, state=DELETE_SEGMENT_STARTED, eventTimestampMs=1715867874729, brokerId=10008}\\n\\npartition: 29, offset: 186218, value: RemoteLogSegmentMetadataUpdate{remoteLogSegmentId=RemoteLogSegmentId{topicIdPartition=ClnIeN0MQsi_d4FAOFKaDA:loadtest_topic-22, id=GZeRTXLMSNe2BQjRXkg6hQ}, customMetadata=Optional.empty, state=DELETE_SEGMENT_FINISHED, eventTimestampMs=1715867874817, brokerId=10008}\\n\\n{code}\\n\\n\\xa0\\n\\n\\n\\nIt seems that at the time the leader is restarted (10013), a second copy of the same segment is tiered by the new leader (10008). Interestingly the segment doesn\\'t have the same end offset, which is concerning.\\xa0\\n\\n\\n\\nThen the follower sees the following error repeatedly until the leader is restarted:\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n{code:java}\\n\\n[2024-05-17 20:46:42,133] DEBUG [ReplicaFetcher replicaId=10013, leaderId=10008, fetcherId=0] Handling errors in processFetchRequest for partitions HashSet(loadtest_topic-22) (kafka.server.ReplicaFetcherThread)\\n\\n[2024-05-17 20:46:43,174] DEBUG [ReplicaFetcher replicaId=10013, leaderId=10008, fetcherId=0] Received error OFFSET_MOVED_TO_TIERED_STORAGE, at fetch offset: 11537, topic-partition: loadtest_topic-22 (kafka.server.ReplicaFetcherThread)\\n\\n[2024-05-17 20:46:43,175] ERROR [ReplicaFetcher replicaId=10013, leaderId=10008, fetcherId=0] Error building remote log auxiliary state for loadtest_topic-22 (kafka.server.ReplicaFetcherThread)\\n\\norg.apache.kafka.server.log.remote.storage.RemoteStorageException: Couldn\\'t build the state from remote store for partition: loadtest_topic-22, currentLeaderEpoch: 153, leaderLocalLogStartOffset: 11545, leaderLogStartOffset: 11537, epoch: 142as the previous remote log segment metadata was not found\\n\\n{code}\\n\\nThe follower is trying to fetch from 11537 and gets OFFSET_MOVED_TO_TIERED_STORAGE . Then when the follower retries, it still thinks it needs to fetch from 11537 . There is no data in S3, so the correct leaderLogStartOffset should be 11545 .\\xa0 I\\'m not sure yet if its intentional that there can be two copies of the same segment that are different uploaded to S3 or if the segments were just deleted in the wrong order, but that is what ultimately caused the leaderLogStartOffset to be set incorrectly.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/core/src/main/java/kafka/log/remote/RemoteLogManager.java b/core/src/main/java/kafka/log/remote/RemoteLogManager.java\\nindex c1c87d579e..3eacbea475 100644\\n--- a/core/src/main/java/kafka/log/remote/RemoteLogManager.java\\n+++ b/core/src/main/java/kafka/log/remote/RemoteLogManager.java\\n@@ -983,7 +983,9 @@ public class RemoteLogManager implements Closeable {\\n                     }\\n                 }\\n                 if (shouldDeleteSegment) {\\n-                    logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    if (!logStartOffset.isPresent() || logStartOffset.getAsLong() < metadata.endOffset() + 1) {\\n+                        logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    }\\n                     logger.info(\"About to delete remote log segment {} due to retention size {} breach. Log size after deletion will be {}.\",\\n                             metadata.remoteLogSegmentId(), retentionSizeData.get().retentionSize, remainingBreachedSize + retentionSizeData.get().retentionSize);\\n                 }\\n@@ -1000,7 +1002,9 @@ public class RemoteLogManager implements Closeable {\\n                     remainingBreachedSize = Math.max(0, remainingBreachedSize - metadata.segmentSizeInBytes());\\n                     // It is fine to have logStartOffset as `metadata.endOffset() + 1` as the segment offset intervals\\n                     // are ascending with in an epoch.\\n-                    logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    if (!logStartOffset.isPresent() || logStartOffset.getAsLong() < metadata.endOffset() + 1) {\\n+                        logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    }\\n                     logger.info(\"About to delete remote log segment {} due to retention time {}ms breach based on the largest record timestamp in the segment\",\\n                             metadata.remoteLogSegmentId(), retentionTimeData.get().retentionMs);\\n                 }\\ndiff --git a/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java b/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java\\nindex 4c4976f060..3c9b8a48e9 100644\\n--- a/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java\\n+++ b/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java\\n@@ -2055,6 +2055,75 @@ public class RemoteLogManagerTest {\\n         assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n     }\\n \\n+    @ParameterizedTest(name = \"testDeletionOnOverlappingRetentionBreachedSegments retentionSize={0} retentionMs={1}\")\\n+    @CsvSource(value = {\"0, -1\", \"-1, 0\"})\\n+    public void testDeletionOnOverlappingRetentionBreachedSegments(long retentionSize,\\n+                                                                   long retentionMs)\\n+            throws RemoteStorageException, ExecutionException, InterruptedException {\\n+        Map<String, Long> logProps = new HashMap<>();\\n+        logProps.put(\"retention.bytes\", retentionSize);\\n+        logProps.put(\"retention.ms\", retentionMs);\\n+        LogConfig mockLogConfig = new LogConfig(logProps);\\n+        when(mockLog.config()).thenReturn(mockLogConfig);\\n+\\n+        List<EpochEntry> epochEntries = Collections.singletonList(epochEntry0);\\n+        checkpoint.write(epochEntries);\\n+        LeaderEpochFileCache cache = new LeaderEpochFileCache(tp, checkpoint, scheduler);\\n+        when(mockLog.leaderEpochCache()).thenReturn(Option.apply(cache));\\n+\\n+        when(mockLog.topicPartition()).thenReturn(leaderTopicIdPartition.topicPartition());\\n+        when(mockLog.logEndOffset()).thenReturn(200L);\\n+\\n+        RemoteLogSegmentMetadata metadata1 = listRemoteLogSegmentMetadata(leaderTopicIdPartition, 1, 100, 1024,\\n+                epochEntries, RemoteLogSegmentState.COPY_SEGMENT_FINISHED)\\n+                .get(0);\\n+        // overlapping segment\\n+        RemoteLogSegmentMetadata metadata2 = new RemoteLogSegmentMetadata(new RemoteLogSegmentId(leaderTopicIdPartition, Uuid.randomUuid()),\\n+                metadata1.startOffset(), metadata1.endOffset() + 5, metadata1.maxTimestampMs(),\\n+                metadata1.brokerId() + 1, metadata1.eventTimestampMs(), metadata1.segmentSizeInBytes() + 128,\\n+                metadata1.customMetadata(), metadata1.state(), metadata1.segmentLeaderEpochs());\\n+\\n+        // When there are overlapping/duplicate segments, the RemoteLogMetadataManager#listRemoteLogSegments\\n+        // returns the segments in order of (valid ++ unreferenced) segments:\\n+        // (eg) B0 uploaded segment S0 with offsets 0-100 and B1 uploaded segment S1 with offsets 0-200.\\n+        //      We will mark the segment S0 as duplicate and add it to unreferencedSegmentIds.\\n+        //      The order of segments returned by listRemoteLogSegments will be S1, S0.\\n+        // While computing the next-log-start-offset, taking the max of deleted segment\\'s end-offset + 1.\\n+        List<RemoteLogSegmentMetadata> metadataList = new ArrayList<>();\\n+        metadataList.add(metadata2);\\n+        metadataList.add(metadata1);\\n+\\n+        when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition))\\n+                .thenReturn(metadataList.iterator());\\n+        when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition, 0))\\n+                .thenAnswer(ans -> metadataList.iterator());\\n+        when(remoteLogMetadataManager.updateRemoteLogSegmentMetadata(any(RemoteLogSegmentMetadataUpdate.class)))\\n+                .thenReturn(CompletableFuture.runAsync(() -> { }));\\n+\\n+        // Verify the metrics for remote deletes and for failures is zero before attempt to delete segments\\n+        assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n+        assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n+        // Verify aggregate metrics\\n+        assertEquals(0, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n+        assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n+\\n+        RemoteLogManager.RLMTask task = remoteLogManager.new RLMTask(leaderTopicIdPartition, 128);\\n+        task.convertToLeader(0);\\n+        task.cleanupExpiredRemoteLogSegments();\\n+\\n+        assertEquals(metadata2.endOffset() + 1, currentLogStartOffset.get());\\n+        verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(0));\\n+        verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(1));\\n+\\n+        // Verify the metric for remote delete is updated correctly\\n+        assertEquals(2, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n+        // Verify we did not report any failure for remote deletes\\n+        assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n+        // Verify aggregate metrics\\n+        assertEquals(2, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n+        assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n+    }\\n+\\n     @ParameterizedTest(name = \"testRemoteDeleteLagsOnRetentionBreachedSegments retentionSize={0} retentionMs={1}\")\\n     @CsvSource(value = {\"0, -1\", \"-1, 0\"})\\n     public void testRemoteDeleteLagsOnRetentionBreachedSegments(long retentionSize,\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: KAFKA-18401\\nIssue Summary: Transaction version 2 does not support commit transaction without records\\nIssue Type: Bug\\nPriority: Blocker\\n\\nDescription:\\nThis issue was observed when implementing https://issues.apache.org/jira/browse/KAFKA-18206.\\n\\n\\n\\nIn short, under transaction version 2, it doesn\\'t support commit transaction without sending any records while transaction version 0 & 1 do support this kind of scenario.\\n\\n\\n\\nCommit transactions without sending any records is fine when using transaction versions 0 or 1 because the producer won\\'t send EndTxnRequest to the broker [0]. However, with transaction version 2, the producer still sends an EndTxnRequest to the broker while in transaction coordinator, the txn state is still in EMPTY, resulting in an error from the broker.\\n\\n\\n\\nThis issue can be reproduced with the test in below. I\\'m unsure if this behavior is expected. If it\\'s not, one potential fix could be to follow the approach used in TV_0 and TV_1, where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change.\\n\\n{code:java}\\n\\n    @ClusterTests({\\n\\n        @ClusterTest(brokers = 3, features = {\\n\\n            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 0)}),\\n\\n        @ClusterTest(brokers = 3, features = {\\n\\n            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 1)}),\\n\\n        @ClusterTest(brokers = 3, features = {\\n\\n            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 2)})\\n\\n    })\\n\\n    public void testProducerEndTransaction2(ClusterInstance cluster) throws InterruptedException {\\n\\n        Map<String, Object> properties = new HashMap<>();\\n\\n        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"foobar\");\\n\\n        properties.put(ProducerConfig.CLIENT_ID_CONFIG, \"test\");\\n\\n        properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\\n\\n        try (Producer<byte[], byte[]> producer1 = cluster.producer(properties)) {\\n\\n\\n\\n            producer1.initTransactions();\\n\\n            producer1.beginTransaction();\\n\\n            producer1.commitTransaction(); // In TV_2, we\\'ll get InvalidTxnStateException\\n\\n        }\\n\\n    }\\n\\n{code}\\n\\nAnother test case, which is essentially the same as the previous one, starts with a transaction that includes records, and then proceeds to start the next transaction. When using transaction version 2, we encounter an error, but this time it\\'s a different error from the one seen in the previous case.\\n\\n{code:java}\\n\\n    @ClusterTests({\\n\\n        @ClusterTest(brokers = 3, features = {\\n\\n            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 0)}),\\n\\n        @ClusterTest(brokers = 3, features = {\\n\\n            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 1)}),\\n\\n        @ClusterTest(brokers = 3, features = {\\n\\n            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 2)})\\n\\n    })\\n\\n    public void testProducerEndTransaction(ClusterInstance cluster) {\\n\\n        Map<String, Object> properties = new HashMap<>();\\n\\n        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"foobar\");\\n\\n        properties.put(ProducerConfig.CLIENT_ID_CONFIG, \"test\");\\n\\n        properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\\n\\n        try (Producer<byte[], byte[]> producer1 = cluster.producer(properties)) {\\n\\n\\n\\n            producer1.initTransactions();\\n\\n            producer1.beginTransaction();\\n\\n            producer1.send(new ProducerRecord<>(\"test\", \"key\".getBytes(), \"value\".getBytes()));\\n\\n            producer1.commitTransaction();\\n\\n\\n\\n            producer1.beginTransaction();\\n\\n            producer1.commitTransaction(); // In TV_2, we\\'ll get ProducerFencedException\\n\\n        }\\n\\n    }\\n\\n{code}\\n\\n\\xa0\\n\\n\\n\\n[0]: [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L857-L865]\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/core/src/main/java/kafka/log/remote/RemoteLogManager.java b/core/src/main/java/kafka/log/remote/RemoteLogManager.java\\nindex c1c87d579e..3eacbea475 100644\\n--- a/core/src/main/java/kafka/log/remote/RemoteLogManager.java\\n+++ b/core/src/main/java/kafka/log/remote/RemoteLogManager.java\\n@@ -983,7 +983,9 @@ public class RemoteLogManager implements Closeable {\\n                     }\\n                 }\\n                 if (shouldDeleteSegment) {\\n-                    logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    if (!logStartOffset.isPresent() || logStartOffset.getAsLong() < metadata.endOffset() + 1) {\\n+                        logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    }\\n                     logger.info(\"About to delete remote log segment {} due to retention size {} breach. Log size after deletion will be {}.\",\\n                             metadata.remoteLogSegmentId(), retentionSizeData.get().retentionSize, remainingBreachedSize + retentionSizeData.get().retentionSize);\\n                 }\\n@@ -1000,7 +1002,9 @@ public class RemoteLogManager implements Closeable {\\n                     remainingBreachedSize = Math.max(0, remainingBreachedSize - metadata.segmentSizeInBytes());\\n                     // It is fine to have logStartOffset as `metadata.endOffset() + 1` as the segment offset intervals\\n                     // are ascending with in an epoch.\\n-                    logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    if (!logStartOffset.isPresent() || logStartOffset.getAsLong() < metadata.endOffset() + 1) {\\n+                        logStartOffset = OptionalLong.of(metadata.endOffset() + 1);\\n+                    }\\n                     logger.info(\"About to delete remote log segment {} due to retention time {}ms breach based on the largest record timestamp in the segment\",\\n                             metadata.remoteLogSegmentId(), retentionTimeData.get().retentionMs);\\n                 }\\ndiff --git a/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java b/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java\\nindex 4c4976f060..3c9b8a48e9 100644\\n--- a/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java\\n+++ b/core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java\\n@@ -2055,6 +2055,75 @@ public class RemoteLogManagerTest {\\n         assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n     }\\n \\n+    @ParameterizedTest(name = \"testDeletionOnOverlappingRetentionBreachedSegments retentionSize={0} retentionMs={1}\")\\n+    @CsvSource(value = {\"0, -1\", \"-1, 0\"})\\n+    public void testDeletionOnOverlappingRetentionBreachedSegments(long retentionSize,\\n+                                                                   long retentionMs)\\n+            throws RemoteStorageException, ExecutionException, InterruptedException {\\n+        Map<String, Long> logProps = new HashMap<>();\\n+        logProps.put(\"retention.bytes\", retentionSize);\\n+        logProps.put(\"retention.ms\", retentionMs);\\n+        LogConfig mockLogConfig = new LogConfig(logProps);\\n+        when(mockLog.config()).thenReturn(mockLogConfig);\\n+\\n+        List<EpochEntry> epochEntries = Collections.singletonList(epochEntry0);\\n+        checkpoint.write(epochEntries);\\n+        LeaderEpochFileCache cache = new LeaderEpochFileCache(tp, checkpoint, scheduler);\\n+        when(mockLog.leaderEpochCache()).thenReturn(Option.apply(cache));\\n+\\n+        when(mockLog.topicPartition()).thenReturn(leaderTopicIdPartition.topicPartition());\\n+        when(mockLog.logEndOffset()).thenReturn(200L);\\n+\\n+        RemoteLogSegmentMetadata metadata1 = listRemoteLogSegmentMetadata(leaderTopicIdPartition, 1, 100, 1024,\\n+                epochEntries, RemoteLogSegmentState.COPY_SEGMENT_FINISHED)\\n+                .get(0);\\n+        // overlapping segment\\n+        RemoteLogSegmentMetadata metadata2 = new RemoteLogSegmentMetadata(new RemoteLogSegmentId(leaderTopicIdPartition, Uuid.randomUuid()),\\n+                metadata1.startOffset(), metadata1.endOffset() + 5, metadata1.maxTimestampMs(),\\n+                metadata1.brokerId() + 1, metadata1.eventTimestampMs(), metadata1.segmentSizeInBytes() + 128,\\n+                metadata1.customMetadata(), metadata1.state(), metadata1.segmentLeaderEpochs());\\n+\\n+        // When there are overlapping/duplicate segments, the RemoteLogMetadataManager#listRemoteLogSegments\\n+        // returns the segments in order of (valid ++ unreferenced) segments:\\n+        // (eg) B0 uploaded segment S0 with offsets 0-100 and B1 uploaded segment S1 with offsets 0-200.\\n+        //      We will mark the segment S0 as duplicate and add it to unreferencedSegmentIds.\\n+        //      The order of segments returned by listRemoteLogSegments will be S1, S0.\\n+        // While computing the next-log-start-offset, taking the max of deleted segment\\'s end-offset + 1.\\n+        List<RemoteLogSegmentMetadata> metadataList = new ArrayList<>();\\n+        metadataList.add(metadata2);\\n+        metadataList.add(metadata1);\\n+\\n+        when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition))\\n+                .thenReturn(metadataList.iterator());\\n+        when(remoteLogMetadataManager.listRemoteLogSegments(leaderTopicIdPartition, 0))\\n+                .thenAnswer(ans -> metadataList.iterator());\\n+        when(remoteLogMetadataManager.updateRemoteLogSegmentMetadata(any(RemoteLogSegmentMetadataUpdate.class)))\\n+                .thenReturn(CompletableFuture.runAsync(() -> { }));\\n+\\n+        // Verify the metrics for remote deletes and for failures is zero before attempt to delete segments\\n+        assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n+        assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n+        // Verify aggregate metrics\\n+        assertEquals(0, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n+        assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n+\\n+        RemoteLogManager.RLMTask task = remoteLogManager.new RLMTask(leaderTopicIdPartition, 128);\\n+        task.convertToLeader(0);\\n+        task.cleanupExpiredRemoteLogSegments();\\n+\\n+        assertEquals(metadata2.endOffset() + 1, currentLogStartOffset.get());\\n+        verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(0));\\n+        verify(remoteStorageManager).deleteLogSegmentData(metadataList.get(1));\\n+\\n+        // Verify the metric for remote delete is updated correctly\\n+        assertEquals(2, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).remoteDeleteRequestRate().count());\\n+        // Verify we did not report any failure for remote deletes\\n+        assertEquals(0, brokerTopicStats.topicStats(leaderTopicIdPartition.topic()).failedRemoteDeleteRequestRate().count());\\n+        // Verify aggregate metrics\\n+        assertEquals(2, brokerTopicStats.allTopicsStats().remoteDeleteRequestRate().count());\\n+        assertEquals(0, brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().count());\\n+    }\\n+\\n     @ParameterizedTest(name = \"testRemoteDeleteLagsOnRetentionBreachedSegments retentionSize={0} retentionMs={1}\")\\n     @CsvSource(value = {\"0, -1\", \"-1, 0\"})\\n     public void testRemoteDeleteLagsOnRetentionBreachedSegments(long retentionSize,\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: KAFKA-18211\\nIssue Summary: ClassGraph scanning does not correctly find isolated connect plugins\\nIssue Type: Bug\\nPriority: Blocker\\n\\nDescription:\\nConnect used to use reflections scanner for scanning and identifying connect plugins in its plugin.path. This would load said plugins in isolation via the use of a child first PluginClassloader, which is designed to load class from its set of URIs before delegating to parent, if not found. This effectively enforces that if a plugin and its dependencies are part of a plugin path it would not conflict with other plugins in the plugin path or plugins in classpath.\\xa0\\n\\n\\n\\n\\xa0\\n\\n\\n\\nGlassGraph was introduced as a replacement for the older reflections scanner in [KAFKA-15203 Use Classgraph since org.reflections is no longer under maintainence by PARADOXST · Pull Request #16604 · apache/kafka|https://github.com/apache/kafka/pull/16604]. It is used in place of reflections scanner for finding plugins during plugin scanning. The issue here is that it is missing any plugins present in isolated plugin paths if its already present in classpath.\\xa0 We can repro this by adding the json converter under an isolated plugin path and starting connect with debug logs. We can see the logs from ReflectionsScanner and find that the ClassGraph loader is always fetching the plugin from the classpath even though the PluginClassLoader is provided. This is causing [kafka/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/ReflectionScanner.java at 520681c38dbefe497181c4fd5dfc793d54233408 · apache/kafka|https://github.com/apache/kafka/blob/520681c38dbefe497181c4fd5dfc793d54233408/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/ReflectionScanner.java#L134] check to fail with the logs like.\\xa0\\n\\n{code:java}\\n\\n[2024-12-11 07:29:28,968] DEBUG class org.apache.kafka.connect.json.JsonConverter from other classloader jdk.internal.loader.ClassLoaders$AppClassLoader@c387f44 is visible from C:\\\\Users\\\\user\\\\Desktop\\\\confluent\\\\testing\\\\plugins3\\\\connect-file-1.2.1-T-0.9.0-P-3.1, excluding to prevent isolated loading (org.apache.kafka.connect.runtime.isolation.ReflectionScanner:135)\\n\\n[2024-12-11 07:29:28,969] DEBUG class org.apache.kafka.connect.json.JsonConverter from other classloader jdk.internal.loader.ClassLoaders$AppClassLoader@c387f44 is visible from C:\\\\Users\\\\user\\\\Desktop\\\\confluent\\\\testing\\\\plugins3\\\\connect-file-1.2.1-T-0.9.0-P-3.1, excluding to prevent isolated loading (org.apache.kafka.connect.runtime.isolation.ReflectionScanner:135) {code}\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Thu, 27 Feb 2025 11:13:56 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '79'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-5b77d74cb4-pxhhz'), ('x-envoy-upstream-service-time', '41'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999931'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '4ms'), ('x-request-id', 'req_06c3ee34d61cc967a403ffe0da7c5a7b'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=4_qFkdD4B1wdSPzcQfkhiBRHJDKBrJcdWxSHNFLbK_0-1740654836-1.0.1.1-vw7eMmI66FuDHmXy7CWL31GnxHy8cj.NG8DcCO1nLSIy8As1tuqMKvkgA3TBnsU9sLZla5WE8jl2qRAtQtX.iA; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=6AFkQq5maWQoeGhpD3HifsNRV5lHkbJdYi1TCX7MeB8-1740654836393-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9187bf93ca62cdee-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A6959C960>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A69605EF0>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A69606030>\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A69604050>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A696061C0>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A69606440>\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A69606670>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A69606710>\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:request_id: req_06c3ee34d61cc967a403ffe0da7c5a7b\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java b/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\\nindex 52d0efbb58..2856f131f1 100644\\n--- a/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\\n+++ b/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\\n@@ -36,6 +36,7 @@ import org.apache.cassandra.utils.ByteBufferUtil;\\n import org.apache.cassandra.utils.MurmurHash;\\n import org.apache.cassandra.utils.ObjectSizes;\\n \\n+import com.google.common.annotations.VisibleForTesting;\\n import com.google.common.primitives.Longs;\\n \\n /**\\n@@ -207,6 +208,18 @@ public class Murmur3Partitioner implements IPartitioner\\n         {\\n             return new LongToken(token + 1);\\n         }\\n+\\n+        /**\\n+         * Reverses murmur3 to find a possible 16 byte key that generates a given token\\n+         */\\n+        @VisibleForTesting\\n+        public static ByteBuffer keyForToken(LongToken token)\\n+        {\\n+            ByteBuffer result = ByteBuffer.allocate(16);\\n+            long[] inv = MurmurHash.inv_hash3_x64_128(new long[] {token.token, 0L});\\n+            result.putLong(inv[0]).putLong(inv[1]).position(0);\\n+            return result;\\n+        }\\n     }\\n \\n     /**\\ndiff --git a/src/java/org/apache/cassandra/utils/ByteBufferUtil.java b/src/java/org/apache/cassandra/utils/ByteBufferUtil.java\\nindex ff3fb3d0a8..5300d9de1f 100644\\n--- a/src/java/org/apache/cassandra/utils/ByteBufferUtil.java\\n+++ b/src/java/org/apache/cassandra/utils/ByteBufferUtil.java\\n@@ -535,6 +535,8 @@ public class ByteBufferUtil\\n             return ByteBufferUtil.bytes((InetAddress) obj);\\n         else if (obj instanceof String)\\n             return ByteBufferUtil.bytes((String) obj);\\n+        else if (obj instanceof ByteBuffer)\\n+            return (ByteBuffer) obj;\\n         else\\n             throw new IllegalArgumentException(String.format(\"Cannot convert value %s of type %s\",\\n                                                              obj,\\ndiff --git a/src/java/org/apache/cassandra/utils/MurmurHash.java b/src/java/org/apache/cassandra/utils/MurmurHash.java\\nindex c02fdcc6dc..80cf5cd39f 100644\\n--- a/src/java/org/apache/cassandra/utils/MurmurHash.java\\n+++ b/src/java/org/apache/cassandra/utils/MurmurHash.java\\n@@ -18,6 +18,9 @@\\n package org.apache.cassandra.utils;\\n \\n import java.nio.ByteBuffer;\\n+import java.util.BitSet;\\n+\\n+import com.google.common.primitives.Longs;\\n \\n /**\\n  * This is a very fast, non-cryptographic hash suitable for general hash-based\\n@@ -146,7 +149,7 @@ public class MurmurHash\\n         return h64;\\n     }\\n \\n-    protected static long getblock(ByteBuffer key, int offset, int index)\\n+    protected static long getBlock(ByteBuffer key, int offset, int index)\\n     {\\n         int i_8 = index << 3;\\n         int blockOffset = offset + i_8;\\n@@ -187,8 +190,8 @@ public class MurmurHash\\n \\n         for(int i = 0; i < nblocks; i++)\\n         {\\n-            long k1 = getblock(key, offset, i*2+0);\\n-            long k2 = getblock(key, offset, i*2+1);\\n+            long k1 = getBlock(key, offset, i * 2 + 0);\\n+            long k2 = getBlock(key, offset, i * 2 + 1);\\n \\n             k1 *= c1; k1 = rotl64(k1,31); k1 *= c2; h1 ^= k1;\\n \\n@@ -248,4 +251,108 @@ public class MurmurHash\\n         result[1] = h2;\\n     }\\n \\n+    protected static long invRotl64(long v, int n)\\n+    {\\n+        return ((v >>> n) | (v << (64 - n)));\\n+    }\\n+\\n+    protected static long invRShiftXor(long value, int shift)\\n+    {\\n+        long output = 0;\\n+        long i = 0;\\n+        while (i * shift < 64)\\n+        {\\n+            long c = (0xffffffffffffffffL << (64 - shift)) >>> (shift * i);\\n+            long partOutput = value & c;\\n+            value ^= partOutput >>> shift;\\n+            output |= partOutput;\\n+            i += 1;\\n+        }\\n+        return output;\\n+    }\\n+\\n+    protected static long invFmix(long k)\\n+    {\\n+        k = invRShiftXor(k, 33);\\n+        k *= 0x9cb4b2f8129337dbL;\\n+        k = invRShiftXor(k, 33);\\n+        k *= 0x4f74430c22a54005L;\\n+        k = invRShiftXor(k, 33);\\n+        return k;\\n+    }\\n+\\n+    /**\\n+     * This gives a correct reversal of the tail byte flip which is needed if want a non mod16==0 byte hash inv or to\\n+     * target a hash for a given schema.\\n+     */\\n+    public static long invTailReverse(long num)\\n+    {\\n+        byte[] v = Longs.toByteArray(Long.reverseBytes(num));\\n+        for (int i = 0; i < 8; i++)\\n+        {\\n+            if (v[i] < 0 && i < 7)\\n+            {\\n+                BitSet bits = BitSet.valueOf(v);\\n+                bits.flip(8 * (i + 1), 64);\\n+                v = bits.toByteArray();\\n+            }\\n+        }\\n+        return Longs.fromByteArray(v);\\n+    }\\n+\\n+    public static long[] inv_hash3_x64_128(long[] result)\\n+    {\\n+        long c1 = 0xa98409e882ce4d7dL;\\n+        long c2 = 0xa81e14edd9de2c7fL;\\n+\\n+        long k1 = 0;\\n+        long k2 = 0;\\n+        long h1 = result[0];\\n+        long h2 = result[1];\\n+\\n+        //----------\\n+        // reverse finalization\\n+        h2 -= h1;\\n+        h1 -= h2;\\n+\\n+        h1 = invFmix(h1);\\n+        h2 = invFmix(h2);\\n+\\n+        h2 -= h1;\\n+        h1 -= h2;\\n+\\n+        h1 ^= 16;\\n+        h2 ^= 16;\\n+\\n+        //----------\\n+        // reverse body\\n+        h2 -= 0x38495ab5;\\n+        h2 *= 0xcccccccccccccccdL;\\n+        h2 -= h1;\\n+        h2 = invRotl64(h2, 31);\\n+        k2 = h2;\\n+        h2 = 0;\\n+\\n+        k2 *= c1;\\n+        k2 = invRotl64(k2, 33);\\n+        k2 *= c2;\\n+\\n+        h1 -= 0x52dce729;\\n+        h1 *= 0xcccccccccccccccdL;\\n+        //h1 -= h2;\\n+        h1 = invRotl64(h1, 27);\\n+\\n+        k1 = h1;\\n+\\n+        k1 *= c2;\\n+        k1 = invRotl64(k1, 31);\\n+        k1 *= c1;\\n+\\n+        // note that while this works for body block reversing the tail reverse requires `invTailReverse`\\n+        k1 = Long.reverseBytes(k1);\\n+        k2 = Long.reverseBytes(k2);\\n+\\n+        return new long[] {k1, k2};\\n+    }\\n+\\n }\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: CASSANDRA-15542\\nIssue Summary: In JVM test for repairs on token boundaries \\nIssue Type: New Feature\\nPriority: Low\\n\\nDescription:\\nPutting partitions on each token range +-1 and making sure the logic end to end with repairs correctly handle inclusive and exclusivity of the bounds.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java b/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\\nindex 52d0efbb58..2856f131f1 100644\\n--- a/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\\n+++ b/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\\n@@ -36,6 +36,7 @@ import org.apache.cassandra.utils.ByteBufferUtil;\\n import org.apache.cassandra.utils.MurmurHash;\\n import org.apache.cassandra.utils.ObjectSizes;\\n \\n+import com.google.common.annotations.VisibleForTesting;\\n import com.google.common.primitives.Longs;\\n \\n /**\\n@@ -207,6 +208,18 @@ public class Murmur3Partitioner implements IPartitioner\\n         {\\n             return new LongToken(token + 1);\\n         }\\n+\\n+        /**\\n+         * Reverses murmur3 to find a possible 16 byte key that generates a given token\\n+         */\\n+        @VisibleForTesting\\n+        public static ByteBuffer keyForToken(LongToken token)\\n+        {\\n+            ByteBuffer result = ByteBuffer.allocate(16);\\n+            long[] inv = MurmurHash.inv_hash3_x64_128(new long[] {token.token, 0L});\\n+            result.putLong(inv[0]).putLong(inv[1]).position(0);\\n+            return result;\\n+        }\\n     }\\n \\n     /**\\ndiff --git a/src/java/org/apache/cassandra/utils/ByteBufferUtil.java b/src/java/org/apache/cassandra/utils/ByteBufferUtil.java\\nindex ff3fb3d0a8..5300d9de1f 100644\\n--- a/src/java/org/apache/cassandra/utils/ByteBufferUtil.java\\n+++ b/src/java/org/apache/cassandra/utils/ByteBufferUtil.java\\n@@ -535,6 +535,8 @@ public class ByteBufferUtil\\n             return ByteBufferUtil.bytes((InetAddress) obj);\\n         else if (obj instanceof String)\\n             return ByteBufferUtil.bytes((String) obj);\\n+        else if (obj instanceof ByteBuffer)\\n+            return (ByteBuffer) obj;\\n         else\\n             throw new IllegalArgumentException(String.format(\"Cannot convert value %s of type %s\",\\n                                                              obj,\\ndiff --git a/src/java/org/apache/cassandra/utils/MurmurHash.java b/src/java/org/apache/cassandra/utils/MurmurHash.java\\nindex c02fdcc6dc..80cf5cd39f 100644\\n--- a/src/java/org/apache/cassandra/utils/MurmurHash.java\\n+++ b/src/java/org/apache/cassandra/utils/MurmurHash.java\\n@@ -18,6 +18,9 @@\\n package org.apache.cassandra.utils;\\n \\n import java.nio.ByteBuffer;\\n+import java.util.BitSet;\\n+\\n+import com.google.common.primitives.Longs;\\n \\n /**\\n  * This is a very fast, non-cryptographic hash suitable for general hash-based\\n@@ -146,7 +149,7 @@ public class MurmurHash\\n         return h64;\\n     }\\n \\n-    protected static long getblock(ByteBuffer key, int offset, int index)\\n+    protected static long getBlock(ByteBuffer key, int offset, int index)\\n     {\\n         int i_8 = index << 3;\\n         int blockOffset = offset + i_8;\\n@@ -187,8 +190,8 @@ public class MurmurHash\\n \\n         for(int i = 0; i < nblocks; i++)\\n         {\\n-            long k1 = getblock(key, offset, i*2+0);\\n-            long k2 = getblock(key, offset, i*2+1);\\n+            long k1 = getBlock(key, offset, i * 2 + 0);\\n+            long k2 = getBlock(key, offset, i * 2 + 1);\\n \\n             k1 *= c1; k1 = rotl64(k1,31); k1 *= c2; h1 ^= k1;\\n \\n@@ -248,4 +251,108 @@ public class MurmurHash\\n         result[1] = h2;\\n     }\\n \\n+    protected static long invRotl64(long v, int n)\\n+    {\\n+        return ((v >>> n) | (v << (64 - n)));\\n+    }\\n+\\n+    protected static long invRShiftXor(long value, int shift)\\n+    {\\n+        long output = 0;\\n+        long i = 0;\\n+        while (i * shift < 64)\\n+        {\\n+            long c = (0xffffffffffffffffL << (64 - shift)) >>> (shift * i);\\n+            long partOutput = value & c;\\n+            value ^= partOutput >>> shift;\\n+            output |= partOutput;\\n+            i += 1;\\n+        }\\n+        return output;\\n+    }\\n+\\n+    protected static long invFmix(long k)\\n+    {\\n+        k = invRShiftXor(k, 33);\\n+        k *= 0x9cb4b2f8129337dbL;\\n+        k = invRShiftXor(k, 33);\\n+        k *= 0x4f74430c22a54005L;\\n+        k = invRShiftXor(k, 33);\\n+        return k;\\n+    }\\n+\\n+    /**\\n+     * This gives a correct reversal of the tail byte flip which is needed if want a non mod16==0 byte hash inv or to\\n+     * target a hash for a given schema.\\n+     */\\n+    public static long invTailReverse(long num)\\n+    {\\n+        byte[] v = Longs.toByteArray(Long.reverseBytes(num));\\n+        for (int i = 0; i < 8; i++)\\n+        {\\n+            if (v[i] < 0 && i < 7)\\n+            {\\n+                BitSet bits = BitSet.valueOf(v);\\n+                bits.flip(8 * (i + 1), 64);\\n+                v = bits.toByteArray();\\n+            }\\n+        }\\n+        return Longs.fromByteArray(v);\\n+    }\\n+\\n+    public static long[] inv_hash3_x64_128(long[] result)\\n+    {\\n+        long c1 = 0xa98409e882ce4d7dL;\\n+        long c2 = 0xa81e14edd9de2c7fL;\\n+\\n+        long k1 = 0;\\n+        long k2 = 0;\\n+        long h1 = result[0];\\n+        long h2 = result[1];\\n+\\n+        //----------\\n+        // reverse finalization\\n+        h2 -= h1;\\n+        h1 -= h2;\\n+\\n+        h1 = invFmix(h1);\\n+        h2 = invFmix(h2);\\n+\\n+        h2 -= h1;\\n+        h1 -= h2;\\n+\\n+        h1 ^= 16;\\n+        h2 ^= 16;\\n+\\n+        //----------\\n+        // reverse body\\n+        h2 -= 0x38495ab5;\\n+        h2 *= 0xcccccccccccccccdL;\\n+        h2 -= h1;\\n+        h2 = invRotl64(h2, 31);\\n+        k2 = h2;\\n+        h2 = 0;\\n+\\n+        k2 *= c1;\\n+        k2 = invRotl64(k2, 33);\\n+        k2 *= c2;\\n+\\n+        h1 -= 0x52dce729;\\n+        h1 *= 0xcccccccccccccccdL;\\n+        //h1 -= h2;\\n+        h1 = invRotl64(h1, 27);\\n+\\n+        k1 = h1;\\n+\\n+        k1 *= c2;\\n+        k1 = invRotl64(k1, 31);\\n+        k1 *= c1;\\n+\\n+        // note that while this works for body block reversing the tail reverse requires `invTailReverse`\\n+        k1 = Long.reverseBytes(k1);\\n+        k2 = Long.reverseBytes(k2);\\n+\\n+        return new long[] {k1, k2};\\n+    }\\n+\\n }\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: CASSANDRA-20135\\nIssue Summary: Assertion errors on CheckForAbort / QueryCancellationChecker on multiple calls of applyToPartition\\nIssue Type: Bug\\nPriority: Normal\\n\\nDescription:\\nWe see there are assertion errors thrown in 4.1 at least in StoppingTransformation like these:\\n\\n\\n\\n{code}\\n\\njava.lang.RuntimeException: java.lang.AssertionError\\n\\n        at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:108)\\n\\n        at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:45)\\n\\n        at org.apache.cassandra.net.InboundMessageHandler$ProcessMessage.run(InboundMessageHandler.java:430)\\n\\n        at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)\\n\\n        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:142)\\n\\n        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\n        at java.base/java.lang.Thread.run(Thread.java:829)\\n\\nCaused by: java.lang.AssertionError: null\\n\\n        at org.apache.cassandra.db.transform.StoppingTransformation.attachTo(StoppingTransformation.java:72)\\n\\n        at org.apache.cassandra.db.transform.BaseRows.add(BaseRows.java:104)\\n\\n        at org.apache.cassandra.db.transform.UnfilteredRows.add(UnfilteredRows.java:49)\\n\\n        at org.apache.cassandra.db.transform.Transformation.add(Transformation.java:198)\\n\\n        at org.apache.cassandra.db.transform.Transformation.apply(Transformation.java:140)\\n\\n        at org.apache.cassandra.db.ReadCommand$CheckForAbort.applyToPartition(ReadCommand.java:616)\\n\\n        at org.apache.cassandra.db.ReadCommand$CheckForAbort.applyToPartition(ReadCommand.java:604)\\n\\n        at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:97)\\n\\n        at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:303)\\n\\n        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:201)\\n\\n        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:186)\\n\\n        at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:48)\\n\\n        at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:337)\\n\\n        at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:63)\\n\\n        at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:78)\\n\\n        at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:97)\\n\\n        ... 6 common frames omitted\\n\\n{code}\\n\\n\\n\\nThis does not make sense at first sight and it is quite a rabbit hole to go through. If you follow the stacktrace, you see that \\n\\n\\n\\n{code}\\n\\nCaused by: java.lang.AssertionError: null\\n\\n        at org.apache.cassandra.db.transform.StoppingTransformation.attachTo(StoppingTransformation.java:72)\\n\\n{code}\\n\\n\\n\\nbut ... why? It means that this (1) was called twice because that is the only place where \"this.rows\" are ever updated in that class (and this.rows is private) which means that _something_ has to call this twice in a row. Once it sets it just fine and another time it goes to set it again but it fails as it is not null anymore. Hence, the question is why is that set twice?\\n\\n\\n\\nThe reason is quite elaborative. \"attachTo\" which throws is ever called in BaseRows#add(Transformation) (2) and just on the next line it calls \"super.add(transformation);\" which adds that transformation at the end of a stack in Stack class which BaseRows extends.\\n\\n\\n\\n{code}\\n\\n    void add(Transformation add)\\n\\n    {\\n\\n        if (length == stack.length)\\n\\n            stack = resize(stack);\\n\\n        stack[length++] = add;\\n\\n    }\\n\\n{code}\\n\\n\\n\\nNext thing we see from the stacktrace is that CheckForAbort.applyToPartition is calling Transformation.apply (3) and what that ultimately does is that it will add itself, again, at the end of the stack (4).\\n\\n\\n\\nWhen we look at that stacktrace as a whole, what it does is that it is iterating over Unfiliteredpartition while building a local data response on a read and as it does so, it calls \"BasePartitions.hasNext\". Now we are getting to that ... (5). What \"hasNext\" is doing is that while this.next is null, it will take the stack and it loops in while by taking \"next\" from \"input\" and it applies all the transformations by calling \"fs[i].applyToParition(next)\".\\n\\n\\n\\nSo, there is a stack of transformations and they are called just one after another until some result of \"applyToPartition\" returns null or we iterated over all transformations. The chain of transformations also include \"CheckForAbort\" transformation which we added here (6) so what happens is that when we call \"applyToPartitions\" for the first time on CheckForAbort, it will run just fine, but when that while loop / for loop in BasePartitions is called _again_ (e.g. we are calling \"hasNext\" upon iterating in UnfilteredPartitionIterators), then \"applyToPartition\" for \"CheckForAbort\" will be called again as well. But CheckForAbort is doing this (7).\\n\\n\\n\\n{code}\\n\\n        protected UnfilteredRowIterator applyToPartition(UnfilteredRowIterator partition)\\n\\n        {\\n\\n            if (maybeAbort())\\n\\n            {\\n\\n                partition.close();\\n\\n                return null;\\n\\n            }\\n\\n\\n\\n            return Transformation.apply(partition, this);\\n\\n        }\\n\\n{code}\\n\\n\\n\\nCheck the last line where it applies itself when it is not aborted:\\n\\n\\n\\n{code}\\n\\nTransformation.apply(partition, this)\\n\\n{code}\\n\\n\\n\\nThe application of this stopping transformation to given partition means that it will add that transformation at the end of the stack as we already showed. Then, we will iterate over that stack again upon iterating in BasePartitions, which eventually calls \"attachTo\" for the second time, hence the assertion error.\\n\\n\\n\\nThe stack might look like\\n\\n\\n\\n{code}\\n\\nstack[0] = transformation1\\n\\nstack[1] = transformation2\\n\\nstack[2] = CheckForAbort\\n\\n{code}\\n\\n\\n\\nthen we call \"fs[i].applyToParition(next)\" which will modify the stack like this:\\n\\n\\n\\n{code}\\n\\nstack[0] = transformation1\\n\\nstack[1] = transformation2\\n\\nstack[2] = CheckForAbort\\n\\nstack[3] = CheckForAbort \\n\\n{code}\\n\\n\\n\\nThen we will loop over that again and if I am not mistaken, when we hit stack[2], it will call applyToPartition on that and it will do \\n\\n\\n\\n{code}\\n\\nstack[0] = transformation1\\n\\nstack[1] = transformation2\\n\\nstack[2] = CheckForAbort // this basically adds itself at the end again\\n\\nstack[3] = CheckForAbort \\n\\nstack[4] = CheckForAbort \\n\\n{code}\\n\\n\\n\\nbut we actually never get this far because on adding itself to the stack, we will hit that assertion error. \\n\\n\\n\\nI also see that CheckForAbort was replaced by CASSANDRA-17810 (8) by QueryCancellationChecker but except some \"cosmetic\" changes, the logic remains the same. That stopping transformation is applying itself in applyToPartition so I think that this problem is present in 5.0+ too. No transformation is applying itself like that but this one. \\n\\n\\n\\nI am not completely sure what we should do about this but two ideas are obvious (with non-zero probability that both of them are wrong)\\n\\n\\n\\n1) We apply _new instance_ of QueryCancellationChecker / CheckForAbort so we will never call attachTo on the _same_ instance (but then we would end up with a bunch of QueryCancellationChecker / CheckForAbort instances in the stack, brrr)\\n\\n2) We will remove \"assert\" in attachTo in StoppingTransformation so we will enable this to be called twice so we will not throw at least ... \\n\\n\\n\\n(1) https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/transform/StoppingTransformation.java#L72\\n\\n(2) https://github.com/apache/cassandra/blob/cassandra-4.1/src/java/org/apache/cassandra/db/transform/BaseRows.java#L104\\n\\n(3) https://github.com/apache/cassandra/blob/cassandra-4.1/src/java/org/apache/cassandra/db/ReadCommand.java#L627\\n\\n(4) https://github.com/apache/cassandra/blob/cassandra-4.1/src/java/org/apache/cassandra/db/transform/Transformation.java#L198\\n\\n(5) https://github.com/apache/cassandra/blob/cassandra-4.1/src/java/org/apache/cassandra/db/transform/BasePartitions.java#L87-L109\\n\\n(6) https://github.com/apache/cassandra/blob/cassandra-4.1/src/java/org/apache/cassandra/db/ReadCommand.java#L436\\n\\n(7) https://github.com/apache/cassandra/blob/cassandra-4.1/src/java/org/apache/cassandra/db/ReadCommand.java#L619-L628\\n\\n(8) https://github.com/apache/cassandra/commit/f4b69ba0e82bb051e56a92d792142034d9f617f0#diff-554e7dff38b500f5eaed0b9b651c7098c3f8a1bd4f6aca12063eab352e685b9fR690\\n\\n\\n\\ncc [~jmckenzie] [~marcuse] [~aleksey]\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java b/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\\nindex 52d0efbb58..2856f131f1 100644\\n--- a/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\\n+++ b/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\\n@@ -36,6 +36,7 @@ import org.apache.cassandra.utils.ByteBufferUtil;\\n import org.apache.cassandra.utils.MurmurHash;\\n import org.apache.cassandra.utils.ObjectSizes;\\n \\n+import com.google.common.annotations.VisibleForTesting;\\n import com.google.common.primitives.Longs;\\n \\n /**\\n@@ -207,6 +208,18 @@ public class Murmur3Partitioner implements IPartitioner\\n         {\\n             return new LongToken(token + 1);\\n         }\\n+\\n+        /**\\n+         * Reverses murmur3 to find a possible 16 byte key that generates a given token\\n+         */\\n+        @VisibleForTesting\\n+        public static ByteBuffer keyForToken(LongToken token)\\n+        {\\n+            ByteBuffer result = ByteBuffer.allocate(16);\\n+            long[] inv = MurmurHash.inv_hash3_x64_128(new long[] {token.token, 0L});\\n+            result.putLong(inv[0]).putLong(inv[1]).position(0);\\n+            return result;\\n+        }\\n     }\\n \\n     /**\\ndiff --git a/src/java/org/apache/cassandra/utils/ByteBufferUtil.java b/src/java/org/apache/cassandra/utils/ByteBufferUtil.java\\nindex ff3fb3d0a8..5300d9de1f 100644\\n--- a/src/java/org/apache/cassandra/utils/ByteBufferUtil.java\\n+++ b/src/java/org/apache/cassandra/utils/ByteBufferUtil.java\\n@@ -535,6 +535,8 @@ public class ByteBufferUtil\\n             return ByteBufferUtil.bytes((InetAddress) obj);\\n         else if (obj instanceof String)\\n             return ByteBufferUtil.bytes((String) obj);\\n+        else if (obj instanceof ByteBuffer)\\n+            return (ByteBuffer) obj;\\n         else\\n             throw new IllegalArgumentException(String.format(\"Cannot convert value %s of type %s\",\\n                                                              obj,\\ndiff --git a/src/java/org/apache/cassandra/utils/MurmurHash.java b/src/java/org/apache/cassandra/utils/MurmurHash.java\\nindex c02fdcc6dc..80cf5cd39f 100644\\n--- a/src/java/org/apache/cassandra/utils/MurmurHash.java\\n+++ b/src/java/org/apache/cassandra/utils/MurmurHash.java\\n@@ -18,6 +18,9 @@\\n package org.apache.cassandra.utils;\\n \\n import java.nio.ByteBuffer;\\n+import java.util.BitSet;\\n+\\n+import com.google.common.primitives.Longs;\\n \\n /**\\n  * This is a very fast, non-cryptographic hash suitable for general hash-based\\n@@ -146,7 +149,7 @@ public class MurmurHash\\n         return h64;\\n     }\\n \\n-    protected static long getblock(ByteBuffer key, int offset, int index)\\n+    protected static long getBlock(ByteBuffer key, int offset, int index)\\n     {\\n         int i_8 = index << 3;\\n         int blockOffset = offset + i_8;\\n@@ -187,8 +190,8 @@ public class MurmurHash\\n \\n         for(int i = 0; i < nblocks; i++)\\n         {\\n-            long k1 = getblock(key, offset, i*2+0);\\n-            long k2 = getblock(key, offset, i*2+1);\\n+            long k1 = getBlock(key, offset, i * 2 + 0);\\n+            long k2 = getBlock(key, offset, i * 2 + 1);\\n \\n             k1 *= c1; k1 = rotl64(k1,31); k1 *= c2; h1 ^= k1;\\n \\n@@ -248,4 +251,108 @@ public class MurmurHash\\n         result[1] = h2;\\n     }\\n \\n+    protected static long invRotl64(long v, int n)\\n+    {\\n+        return ((v >>> n) | (v << (64 - n)));\\n+    }\\n+\\n+    protected static long invRShiftXor(long value, int shift)\\n+    {\\n+        long output = 0;\\n+        long i = 0;\\n+        while (i * shift < 64)\\n+        {\\n+            long c = (0xffffffffffffffffL << (64 - shift)) >>> (shift * i);\\n+            long partOutput = value & c;\\n+            value ^= partOutput >>> shift;\\n+            output |= partOutput;\\n+            i += 1;\\n+        }\\n+        return output;\\n+    }\\n+\\n+    protected static long invFmix(long k)\\n+    {\\n+        k = invRShiftXor(k, 33);\\n+        k *= 0x9cb4b2f8129337dbL;\\n+        k = invRShiftXor(k, 33);\\n+        k *= 0x4f74430c22a54005L;\\n+        k = invRShiftXor(k, 33);\\n+        return k;\\n+    }\\n+\\n+    /**\\n+     * This gives a correct reversal of the tail byte flip which is needed if want a non mod16==0 byte hash inv or to\\n+     * target a hash for a given schema.\\n+     */\\n+    public static long invTailReverse(long num)\\n+    {\\n+        byte[] v = Longs.toByteArray(Long.reverseBytes(num));\\n+        for (int i = 0; i < 8; i++)\\n+        {\\n+            if (v[i] < 0 && i < 7)\\n+            {\\n+                BitSet bits = BitSet.valueOf(v);\\n+                bits.flip(8 * (i + 1), 64);\\n+                v = bits.toByteArray();\\n+            }\\n+        }\\n+        return Longs.fromByteArray(v);\\n+    }\\n+\\n+    public static long[] inv_hash3_x64_128(long[] result)\\n+    {\\n+        long c1 = 0xa98409e882ce4d7dL;\\n+        long c2 = 0xa81e14edd9de2c7fL;\\n+\\n+        long k1 = 0;\\n+        long k2 = 0;\\n+        long h1 = result[0];\\n+        long h2 = result[1];\\n+\\n+        //----------\\n+        // reverse finalization\\n+        h2 -= h1;\\n+        h1 -= h2;\\n+\\n+        h1 = invFmix(h1);\\n+        h2 = invFmix(h2);\\n+\\n+        h2 -= h1;\\n+        h1 -= h2;\\n+\\n+        h1 ^= 16;\\n+        h2 ^= 16;\\n+\\n+        //----------\\n+        // reverse body\\n+        h2 -= 0x38495ab5;\\n+        h2 *= 0xcccccccccccccccdL;\\n+        h2 -= h1;\\n+        h2 = invRotl64(h2, 31);\\n+        k2 = h2;\\n+        h2 = 0;\\n+\\n+        k2 *= c1;\\n+        k2 = invRotl64(k2, 33);\\n+        k2 *= c2;\\n+\\n+        h1 -= 0x52dce729;\\n+        h1 *= 0xcccccccccccccccdL;\\n+        //h1 -= h2;\\n+        h1 = invRotl64(h1, 27);\\n+\\n+        k1 = h1;\\n+\\n+        k1 *= c2;\\n+        k1 = invRotl64(k1, 31);\\n+        k1 *= c1;\\n+\\n+        // note that while this works for body block reversing the tail reverse requires `invTailReverse`\\n+        k1 = Long.reverseBytes(k1);\\n+        k2 = Long.reverseBytes(k2);\\n+\\n+        return new long[] {k1, k2};\\n+    }\\n+\\n }\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: CASSANDRA-20108\\nIssue Summary: IndexOutOfBoundsException when accessing partition where the column was deleted\\nIssue Type: Bug\\nPriority: Normal\\n\\nDescription:\\n{code}\\n\\nCaused by: java.lang.IndexOutOfBoundsException\\n\\n\\tat java.base/java.nio.Buffer.checkIndex(Buffer.java:687)\\n\\n\\tat java.base/java.nio.HeapByteBuffer.get(HeapByteBuffer.java:169)\\n\\n\\tat org.apache.cassandra.db.marshal.ByteBufferAccessor.getByte(ByteBufferAccessor.java:184)\\n\\n\\tat org.apache.cassandra.db.marshal.ByteBufferAccessor.getByte(ByteBufferAccessor.java:42)\\n\\n\\tat org.apache.cassandra.db.marshal.ByteType.compareCustom(ByteType.java:51)\\n\\n\\tat org.apache.cassandra.db.marshal.AbstractType.compare(AbstractType.java:216)\\n\\n\\tat org.apache.cassandra.db.marshal.AbstractType.compare(AbstractType.java:211)\\n\\n\\tat org.apache.cassandra.db.marshal.AbstractType.compareForCQL(AbstractType.java:269)\\n\\n\\tat org.apache.cassandra.cql3.Operator$1.isSatisfiedBy(Operator.java:73)\\n\\n\\tat org.apache.cassandra.db.filter.RowFilter$SimpleExpression.isSatisfiedBy(RowFilter.java:725)\\n\\n\\tat org.apache.cassandra.db.filter.RowFilter$1.applyToPartition(RowFilter.java:227)\\n\\n\\tat org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:94)\\n\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:1045)\\n\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:629)\\n\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:665)\\n\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.executeLocally(SelectStatement.java:635)\\n\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.executeLocally(SelectStatement.java:151)\\n\\n{code}\\n\\n\\n\\nTable\\n\\n\\n\\n{code}\\n\\nCREATE TABLE keyspace_test_00.\"3W56TBuMmC11vPVxalpse84eS\" (\\n\\n\\t\\t    pk0 date,\\n\\n\\t\\t    pk1 double,\\n\\n\\t\\t    ck0 int,\\n\\n\\t\\t    ck1 inet,\\n\\n\\t\\t    s0 tinyint static,\\n\\n\\t\\t    v0 int,\\n\\n\\t\\t    v1 varint,\\n\\n\\t\\t    v2 varint,\\n\\n\\t\\t    v3 timestamp,\\n\\n\\t\\t    PRIMARY KEY ((pk0, pk1), ck0, ck1)\\n\\n\\t\\t) WITH CLUSTERING ORDER BY (ck0 DESC, ck1 ASC)\\n\\n{code}\\n\\n\\n\\nThe query\\n\\n\\n\\n{code}\\n\\nSELECT *\\n\\nFROM keyspace_test_00.\"3W56TBuMmC11vPVxalpse84eS\"\\n\\nWHERE s0 = ? —- value is \"(byte) -113\"\\n\\nALLOW FILTERING\\n\\n{code}\\n\\n\\n\\nThe issue is that we see the delete, but don’t properly handle null data\\n\\n\\n\\n{code}\\n\\nByteBuffer foundValue = getValue(metadata, partitionKey, row);\\n\\n// value is java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]; aka null (empty)\\n\\n{code}\\n\\n\\n\\nHistory of operations on this partition\\n\\n\\n\\n{code}\\n\\n\\tHistory:\\n\\n\\t\\t1: UPDATE pd0\\n\\n\\t\\t2: Select Whole Partition pd0\\n\\n\\t\\t4: Select Whole Partition pd0\\n\\n\\t\\t6: Delete COLUMN [s0, v0, v1, v2, v3] pd0\\n\\n\\t\\t7: Select Whole Partition pd0\\n\\n\\t\\t10: Select Whole Partition pd0\\n\\n\\t\\t12: Select Row pd0\\n\\n\\t\\t17: Delete COLUMN [s0, v0, v1, v2, v3] pd0\\n\\n\\t\\t20: INSERT pd0\\n\\n\\t\\t27: UPDATE pd0\\n\\n\\t\\t38: INSERT pd0\\n\\n\\t\\t41: Select Row pd0\\n\\n\\t\\t56: Select Row pd0\\n\\n\\t\\t66: Delete COLUMN [s0, v0, v1, v2, v3] pd0\\n\\n\\t\\t67: Search on column s0\\n\\n{code}\\n\\n\\n\\nHere we see an insert was done so liveness info is generated, but we do delete on all columns leaving only the partition/clustering keys around...\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'63'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-5fd58fb6cb-sr9j6'), (b'x-envoy-upstream-service-time', b'43'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999918'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'req_35292105d99dc55727f6e63421b5ec83'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=fOayALf8zfigPGPeLVYG.aa3q4KAq_.Xw.oVmlkZvAo-1740654836-1.0.1.1-LEPBC2KcjYB1.E64PHrBnKRZFCeWoebypdCCY14UmA5Mi5HBaN2a8ybzlaE_nDhtGQCXPd3zWZiHulwEmiY6Cw; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=k1UHgwUmNOV4G_HOBmUcDYq5L11ycFO2AWBSfLYT_4s-1740654836503-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf93b86afd37-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Thu, 27 Feb 2025 11:13:56 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '56'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-6cf9fb8d46-d9sth'), ('x-envoy-upstream-service-time', '33'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999959'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '2ms'), ('x-request-id', 'req_42f90a2b93528fff427e694d655194be'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=VsQJGPYpftxIidj70KFt3jcgdNBkdFOu80bjCw5jz1o-1740654836-1.0.1.1-Fn9gSD8UyzGD2mSGGT8f9Jf6cMiAbUOmf0r.rYU5rzwJJcKOojdc8kjIwILJ0BaQIzhViKGBmrY83D.GhSKuAQ; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=DZoI16JQ4_EM_KbXzm._8unpmS7AoJZmG5xuhRL4gAY-1740654836414-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9187bf93bde599a9-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:request_id: req_42f90a2b93528fff427e694d655194be\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Thu, 27 Feb 2025 11:13:56 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '63'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-5fd58fb6cb-sr9j6'), ('x-envoy-upstream-service-time', '43'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999918'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '4ms'), ('x-request-id', 'req_35292105d99dc55727f6e63421b5ec83'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=fOayALf8zfigPGPeLVYG.aa3q4KAq_.Xw.oVmlkZvAo-1740654836-1.0.1.1-LEPBC2KcjYB1.E64PHrBnKRZFCeWoebypdCCY14UmA5Mi5HBaN2a8ybzlaE_nDhtGQCXPd3zWZiHulwEmiY6Cw; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=k1UHgwUmNOV4G_HOBmUcDYq5L11ycFO2AWBSfLYT_4s-1740654836503-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9187bf93b86afd37-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_35292105d99dc55727f6e63421b5ec83\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java b/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java\\nindex 3ca787fc3..f8b275a01 100644\\n--- a/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java\\n+++ b/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java\\n@@ -17,10 +17,9 @@\\n  */\\n package org.apache.zookeeper.common;\\n \\n-\\n-import java.io.ByteArrayInputStream;\\n import java.io.Closeable;\\n import java.io.IOException;\\n+import java.lang.reflect.InvocationTargetException;\\n import java.net.Socket;\\n import java.nio.file.Path;\\n import java.nio.file.Paths;\\n@@ -33,15 +32,14 @@\\n import java.security.Security;\\n import java.security.cert.PKIXBuilderParameters;\\n import java.security.cert.X509CertSelector;\\n-import java.util.Arrays;\\n import java.util.Objects;\\n import java.util.concurrent.atomic.AtomicReference;\\n+import java.util.function.Supplier;\\n \\n import javax.net.ssl.CertPathTrustManagerParameters;\\n import javax.net.ssl.KeyManager;\\n import javax.net.ssl.KeyManagerFactory;\\n import javax.net.ssl.SSLContext;\\n-import javax.net.ssl.SSLParameters;\\n import javax.net.ssl.SSLServerSocket;\\n import javax.net.ssl.SSLSocket;\\n import javax.net.ssl.TrustManager;\\n@@ -137,6 +135,7 @@ public static ClientAuth fromPropertyValue(String prop) {\\n     private String sslTruststoreLocationProperty = getConfigPrefix() + \"trustStore.location\";\\n     private String sslTruststorePasswdProperty = getConfigPrefix() + \"trustStore.password\";\\n     private String sslTruststoreTypeProperty = getConfigPrefix() + \"trustStore.type\";\\n+    private String sslContextSupplierClassProperty = getConfigPrefix() + \"context.supplier.class\";\\n     private String sslHostnameVerificationEnabledProperty = getConfigPrefix() + \"hostnameVerification\";\\n     private String sslCrlEnabledProperty = getConfigPrefix() + \"crl\";\\n     private String sslOcspEnabledProperty = getConfigPrefix() + \"ocsp\";\\n@@ -202,6 +201,10 @@ public String getSslTruststoreTypeProperty() {\\n         return sslTruststoreTypeProperty;\\n     }\\n \\n+    public String getSslContextSupplierClassProperty() {\\n+        return sslContextSupplierClassProperty;\\n+    }\\n+\\n     public String getSslHostnameVerificationEnabledProperty() {\\n         return sslHostnameVerificationEnabledProperty;\\n     }\\n@@ -282,7 +285,28 @@ public int getSslHandshakeTimeoutMillis() {\\n         }\\n     }\\n \\n+    @SuppressWarnings(\"unchecked\")\\n     public SSLContextAndOptions createSSLContextAndOptions(ZKConfig config) throws SSLContextException {\\n+        final String supplierContextClassName = config.getProperty(sslContextSupplierClassProperty);\\n+        if (supplierContextClassName != null) {\\n+            if (LOG.isDebugEnabled()) {\\n+                LOG.debug(\"Loading SSLContext supplier from property \\'{}\\'\", sslContextSupplierClassProperty);\\n+            }\\n+            try {\\n+                Class<?> sslContextClass = Class.forName(supplierContextClassName);\\n+                Supplier<SSLContext> sslContextSupplier = (Supplier<SSLContext>) sslContextClass.getConstructor().newInstance();\\n+                return new SSLContextAndOptions(this, config, sslContextSupplier.get());\\n+            } catch (ClassNotFoundException | ClassCastException | NoSuchMethodException | InvocationTargetException |\\n+                    InstantiationException | IllegalAccessException e) {\\n+                throw new SSLContextException(\"Could not retrieve the SSLContext from supplier source \\'\" + supplierContextClassName +\\n+                        \"\\' provided in the property \\'\" + sslContextSupplierClassProperty + \"\\'\", e);\\n+            }\\n+        } else {\\n+            return createSSLContextAndOptionsFromConfig(config);\\n+        }\\n+    }\\n+\\n+    public SSLContextAndOptions createSSLContextAndOptionsFromConfig(ZKConfig config) throws SSLContextException {\\n         KeyManager[] keyManagers = null;\\n         TrustManager[] trustManagers = null;\\n \\ndiff --git a/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java b/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java\\nindex 43bc2d8e9..76bdd2e20 100644\\n--- a/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java\\n+++ b/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java\\n@@ -133,6 +133,8 @@ private void putSSLProperties(X509Util x509Util) {\\n                 System.getProperty(x509Util.getSslTruststorePasswdProperty()));\\n         properties.put(x509Util.getSslTruststoreTypeProperty(),\\n                 System.getProperty(x509Util.getSslTruststoreTypeProperty()));\\n+        properties.put(x509Util.getSslContextSupplierClassProperty(),\\n+                System.getProperty(x509Util.getSslContextSupplierClassProperty()));\\n         properties.put(x509Util.getSslHostnameVerificationEnabledProperty(),\\n                 System.getProperty(x509Util.getSslHostnameVerificationEnabledProperty()));\\n         properties.put(x509Util.getSslCrlEnabledProperty(),\\ndiff --git a/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java b/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java\\nindex 2a6bb3246..1fecd808d 100644\\n--- a/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java\\n+++ b/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java\\n@@ -22,6 +22,7 @@\\n import java.net.InetSocketAddress;\\n import java.net.ServerSocket;\\n import java.net.Socket;\\n+import java.security.NoSuchAlgorithmException;\\n import java.security.Security;\\n import java.util.Collection;\\n import java.util.concurrent.Callable;\\n@@ -30,6 +31,7 @@\\n import java.util.concurrent.Executors;\\n import java.util.concurrent.Future;\\n import java.util.concurrent.atomic.AtomicInteger;\\n+import java.util.function.Supplier;\\n \\n import javax.net.ssl.HandshakeCompletedEvent;\\n import javax.net.ssl.HandshakeCompletedListener;\\n@@ -403,6 +405,23 @@ public void testGetSslHandshakeDetectionTimeoutMillisProperty() {\\n         }\\n     }\\n \\n+    @Test(expected = X509Exception.SSLContextException.class)\\n+    public void testCreateSSLContext_invalidCustomSSLContextClass() throws Exception {\\n+        ZKConfig zkConfig = new ZKConfig();\\n+        ClientX509Util clientX509Util = new ClientX509Util();\\n+        zkConfig.setProperty(clientX509Util.getSslContextSupplierClassProperty(), String.class.getCanonicalName());\\n+        clientX509Util.createSSLContext(zkConfig);\\n+    }\\n+\\n+    @Test\\n+    public void testCreateSSLContext_validCustomSSLContextClass() throws Exception {\\n+        ZKConfig zkConfig = new ZKConfig();\\n+        ClientX509Util clientX509Util = new ClientX509Util();\\n+        zkConfig.setProperty(clientX509Util.getSslContextSupplierClassProperty(), SslContextSupplier.class.getName());\\n+        final SSLContext sslContext = clientX509Util.createSSLContext(zkConfig);\\n+        Assert.assertEquals(SSLContext.getDefault(), sslContext);\\n+    }\\n+\\n     private static void forceClose(Socket s) {\\n         if (s == null || s.isClosed()) {\\n             return;\\n@@ -528,4 +547,18 @@ private void setCustomCipherSuites() {\\n         x509Util.close(); // remember to close old instance before replacing it\\n         x509Util = new ClientX509Util();\\n     }\\n+\\n+    public static class SslContextSupplier implements Supplier<SSLContext> {\\n+\\n+        @Override\\n+        public SSLContext get() {\\n+            try {\\n+                return SSLContext.getDefault();\\n+            } catch (NoSuchAlgorithmException e) {\\n+                throw new RuntimeException(e);\\n+            }\\n+        }\\n+\\n+    }\\n+\\n }\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: ZOOKEEPER-3160\\nIssue Summary: Custom User SSLContext\\nIssue Type: New Feature\\nPriority: Minor\\n\\nDescription:\\nThe Zookeeper libraries currently allow you to set up your SSL Context via system properties such as \"zookeeper.ssl.keyStore.location\" in the X509Util. This covers most simple use cases, where users have software keystores on their harddrive.\\n\\n\\n\\nThere are, however, a few additional scenarios that this doesn\\'t cover. Two possible ones would be:\\n\\n # The user has a hardware keystore, loaded in using PKCS11 or something similar.\\n\\n # The user has no access to the software keystore, but can retrieve an already-constructed SSLContext from their container.\\n\\n\\n\\nFor this, I would propose that the X509Util be extended to allow a user to set a property such as \"zookeeper.ssl.client.context\" to provide a class which supplies a custom SSL context. This gives a lot more flexibility to the ZK client, and allows the user to construct the SSLContext in whatever way they please (which also future proofs the implementation somewhat).\\n\\n\\n\\nI\\'ve already completed this feature, and will put in a PR soon for it.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java b/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java\\nindex 3ca787fc3..f8b275a01 100644\\n--- a/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java\\n+++ b/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java\\n@@ -17,10 +17,9 @@\\n  */\\n package org.apache.zookeeper.common;\\n \\n-\\n-import java.io.ByteArrayInputStream;\\n import java.io.Closeable;\\n import java.io.IOException;\\n+import java.lang.reflect.InvocationTargetException;\\n import java.net.Socket;\\n import java.nio.file.Path;\\n import java.nio.file.Paths;\\n@@ -33,15 +32,14 @@\\n import java.security.Security;\\n import java.security.cert.PKIXBuilderParameters;\\n import java.security.cert.X509CertSelector;\\n-import java.util.Arrays;\\n import java.util.Objects;\\n import java.util.concurrent.atomic.AtomicReference;\\n+import java.util.function.Supplier;\\n \\n import javax.net.ssl.CertPathTrustManagerParameters;\\n import javax.net.ssl.KeyManager;\\n import javax.net.ssl.KeyManagerFactory;\\n import javax.net.ssl.SSLContext;\\n-import javax.net.ssl.SSLParameters;\\n import javax.net.ssl.SSLServerSocket;\\n import javax.net.ssl.SSLSocket;\\n import javax.net.ssl.TrustManager;\\n@@ -137,6 +135,7 @@ public static ClientAuth fromPropertyValue(String prop) {\\n     private String sslTruststoreLocationProperty = getConfigPrefix() + \"trustStore.location\";\\n     private String sslTruststorePasswdProperty = getConfigPrefix() + \"trustStore.password\";\\n     private String sslTruststoreTypeProperty = getConfigPrefix() + \"trustStore.type\";\\n+    private String sslContextSupplierClassProperty = getConfigPrefix() + \"context.supplier.class\";\\n     private String sslHostnameVerificationEnabledProperty = getConfigPrefix() + \"hostnameVerification\";\\n     private String sslCrlEnabledProperty = getConfigPrefix() + \"crl\";\\n     private String sslOcspEnabledProperty = getConfigPrefix() + \"ocsp\";\\n@@ -202,6 +201,10 @@ public String getSslTruststoreTypeProperty() {\\n         return sslTruststoreTypeProperty;\\n     }\\n \\n+    public String getSslContextSupplierClassProperty() {\\n+        return sslContextSupplierClassProperty;\\n+    }\\n+\\n     public String getSslHostnameVerificationEnabledProperty() {\\n         return sslHostnameVerificationEnabledProperty;\\n     }\\n@@ -282,7 +285,28 @@ public int getSslHandshakeTimeoutMillis() {\\n         }\\n     }\\n \\n+    @SuppressWarnings(\"unchecked\")\\n     public SSLContextAndOptions createSSLContextAndOptions(ZKConfig config) throws SSLContextException {\\n+        final String supplierContextClassName = config.getProperty(sslContextSupplierClassProperty);\\n+        if (supplierContextClassName != null) {\\n+            if (LOG.isDebugEnabled()) {\\n+                LOG.debug(\"Loading SSLContext supplier from property \\'{}\\'\", sslContextSupplierClassProperty);\\n+            }\\n+            try {\\n+                Class<?> sslContextClass = Class.forName(supplierContextClassName);\\n+                Supplier<SSLContext> sslContextSupplier = (Supplier<SSLContext>) sslContextClass.getConstructor().newInstance();\\n+                return new SSLContextAndOptions(this, config, sslContextSupplier.get());\\n+            } catch (ClassNotFoundException | ClassCastException | NoSuchMethodException | InvocationTargetException |\\n+                    InstantiationException | IllegalAccessException e) {\\n+                throw new SSLContextException(\"Could not retrieve the SSLContext from supplier source \\'\" + supplierContextClassName +\\n+                        \"\\' provided in the property \\'\" + sslContextSupplierClassProperty + \"\\'\", e);\\n+            }\\n+        } else {\\n+            return createSSLContextAndOptionsFromConfig(config);\\n+        }\\n+    }\\n+\\n+    public SSLContextAndOptions createSSLContextAndOptionsFromConfig(ZKConfig config) throws SSLContextException {\\n         KeyManager[] keyManagers = null;\\n         TrustManager[] trustManagers = null;\\n \\ndiff --git a/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java b/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java\\nindex 43bc2d8e9..76bdd2e20 100644\\n--- a/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java\\n+++ b/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java\\n@@ -133,6 +133,8 @@ private void putSSLProperties(X509Util x509Util) {\\n                 System.getProperty(x509Util.getSslTruststorePasswdProperty()));\\n         properties.put(x509Util.getSslTruststoreTypeProperty(),\\n                 System.getProperty(x509Util.getSslTruststoreTypeProperty()));\\n+        properties.put(x509Util.getSslContextSupplierClassProperty(),\\n+                System.getProperty(x509Util.getSslContextSupplierClassProperty()));\\n         properties.put(x509Util.getSslHostnameVerificationEnabledProperty(),\\n                 System.getProperty(x509Util.getSslHostnameVerificationEnabledProperty()));\\n         properties.put(x509Util.getSslCrlEnabledProperty(),\\ndiff --git a/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java b/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java\\nindex 2a6bb3246..1fecd808d 100644\\n--- a/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java\\n+++ b/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java\\n@@ -22,6 +22,7 @@\\n import java.net.InetSocketAddress;\\n import java.net.ServerSocket;\\n import java.net.Socket;\\n+import java.security.NoSuchAlgorithmException;\\n import java.security.Security;\\n import java.util.Collection;\\n import java.util.concurrent.Callable;\\n@@ -30,6 +31,7 @@\\n import java.util.concurrent.Executors;\\n import java.util.concurrent.Future;\\n import java.util.concurrent.atomic.AtomicInteger;\\n+import java.util.function.Supplier;\\n \\n import javax.net.ssl.HandshakeCompletedEvent;\\n import javax.net.ssl.HandshakeCompletedListener;\\n@@ -403,6 +405,23 @@ public void testGetSslHandshakeDetectionTimeoutMillisProperty() {\\n         }\\n     }\\n \\n+    @Test(expected = X509Exception.SSLContextException.class)\\n+    public void testCreateSSLContext_invalidCustomSSLContextClass() throws Exception {\\n+        ZKConfig zkConfig = new ZKConfig();\\n+        ClientX509Util clientX509Util = new ClientX509Util();\\n+        zkConfig.setProperty(clientX509Util.getSslContextSupplierClassProperty(), String.class.getCanonicalName());\\n+        clientX509Util.createSSLContext(zkConfig);\\n+    }\\n+\\n+    @Test\\n+    public void testCreateSSLContext_validCustomSSLContextClass() throws Exception {\\n+        ZKConfig zkConfig = new ZKConfig();\\n+        ClientX509Util clientX509Util = new ClientX509Util();\\n+        zkConfig.setProperty(clientX509Util.getSslContextSupplierClassProperty(), SslContextSupplier.class.getName());\\n+        final SSLContext sslContext = clientX509Util.createSSLContext(zkConfig);\\n+        Assert.assertEquals(SSLContext.getDefault(), sslContext);\\n+    }\\n+\\n     private static void forceClose(Socket s) {\\n         if (s == null || s.isClosed()) {\\n             return;\\n@@ -528,4 +547,18 @@ private void setCustomCipherSuites() {\\n         x509Util.close(); // remember to close old instance before replacing it\\n         x509Util = new ClientX509Util();\\n     }\\n+\\n+    public static class SslContextSupplier implements Supplier<SSLContext> {\\n+\\n+        @Override\\n+        public SSLContext get() {\\n+            try {\\n+                return SSLContext.getDefault();\\n+            } catch (NoSuchAlgorithmException e) {\\n+                throw new RuntimeException(e);\\n+            }\\n+        }\\n+\\n+    }\\n+\\n }\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: ZOOKEEPER-4790\\nIssue Summary: TLS Quorum hostname verification breaks in some scenarios\\nIssue Type: Improvement\\nPriority: Minor\\n\\nDescription:\\nCurrently, enabling Quorum TLS will make the server validate SANs client certificates of connecting quorum peers against their reverse DNS address.Â\\xa0\\n\\n\\n\\nÂ\\xa0We have seen this cause issues when running in Kubernetes, due to ip addresses resolving to multiple dns names, when ZooKeeper pods participate in multiple services. \\n\\n\\n\\nSince `InetAddress.getHostAddress()` returns a String, it basically becomes a game of chance which dns name is checked against the cert. \\n\\nThis usually shakes itself loose after a few minutes, when the hostname that gets returned by the reverse lookup randomly changes and all of a sudden matches the certificate... but this is less than ideal.\\n\\n\\n\\nThis has also caused issues in the Strimzi operator as well (see [this issue|https://github.com/strimzi/strimzi-kafka-operator/issues/3099]) - they solved this by pretty much adding anything they can find that might be relevant to the SAN, and a few wildcards on top of that.\\n\\n\\n\\nThis is both, error prone and doesn\\'t really add any relevant extra amount of security, since \"This certificate matches the connecting peer\" shouldn\\'t automatically mean \"this peer should be allowed to connect\".\\n\\nÂ\\xa0\\n\\nÂ\\xa0There are two (probably more) ways to fix this:\\n\\n\\n\\n# Retrieve _all_  reverse entries and check against all of them\\n\\n# The ZK server could verify the SAN against the list of servers ({{{}servers.N{}}}Â\\xa0in the config). A peer should be able to connect on the quorum port if and only if at least one SAN matches at least one of the listed servers.\\n\\n\\n\\nI\\'d argue that the second option is the better one, especially since the java api doesn\\'t even seem to have the option of retrieving all dns entries, but also because it better matches the expressed intent of the ZK admin.\\n\\n\\n\\nAdditionally, it would be nice to have a \"disable client hostname verification\" option that still leaves server hostname verification enabled. Strictly speaking this is a separate issue though, I\\'d be happy to spin that out into a ticket of its own..\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java b/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java\\nindex 3ca787fc3..f8b275a01 100644\\n--- a/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java\\n+++ b/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java\\n@@ -17,10 +17,9 @@\\n  */\\n package org.apache.zookeeper.common;\\n \\n-\\n-import java.io.ByteArrayInputStream;\\n import java.io.Closeable;\\n import java.io.IOException;\\n+import java.lang.reflect.InvocationTargetException;\\n import java.net.Socket;\\n import java.nio.file.Path;\\n import java.nio.file.Paths;\\n@@ -33,15 +32,14 @@\\n import java.security.Security;\\n import java.security.cert.PKIXBuilderParameters;\\n import java.security.cert.X509CertSelector;\\n-import java.util.Arrays;\\n import java.util.Objects;\\n import java.util.concurrent.atomic.AtomicReference;\\n+import java.util.function.Supplier;\\n \\n import javax.net.ssl.CertPathTrustManagerParameters;\\n import javax.net.ssl.KeyManager;\\n import javax.net.ssl.KeyManagerFactory;\\n import javax.net.ssl.SSLContext;\\n-import javax.net.ssl.SSLParameters;\\n import javax.net.ssl.SSLServerSocket;\\n import javax.net.ssl.SSLSocket;\\n import javax.net.ssl.TrustManager;\\n@@ -137,6 +135,7 @@ public static ClientAuth fromPropertyValue(String prop) {\\n     private String sslTruststoreLocationProperty = getConfigPrefix() + \"trustStore.location\";\\n     private String sslTruststorePasswdProperty = getConfigPrefix() + \"trustStore.password\";\\n     private String sslTruststoreTypeProperty = getConfigPrefix() + \"trustStore.type\";\\n+    private String sslContextSupplierClassProperty = getConfigPrefix() + \"context.supplier.class\";\\n     private String sslHostnameVerificationEnabledProperty = getConfigPrefix() + \"hostnameVerification\";\\n     private String sslCrlEnabledProperty = getConfigPrefix() + \"crl\";\\n     private String sslOcspEnabledProperty = getConfigPrefix() + \"ocsp\";\\n@@ -202,6 +201,10 @@ public String getSslTruststoreTypeProperty() {\\n         return sslTruststoreTypeProperty;\\n     }\\n \\n+    public String getSslContextSupplierClassProperty() {\\n+        return sslContextSupplierClassProperty;\\n+    }\\n+\\n     public String getSslHostnameVerificationEnabledProperty() {\\n         return sslHostnameVerificationEnabledProperty;\\n     }\\n@@ -282,7 +285,28 @@ public int getSslHandshakeTimeoutMillis() {\\n         }\\n     }\\n \\n+    @SuppressWarnings(\"unchecked\")\\n     public SSLContextAndOptions createSSLContextAndOptions(ZKConfig config) throws SSLContextException {\\n+        final String supplierContextClassName = config.getProperty(sslContextSupplierClassProperty);\\n+        if (supplierContextClassName != null) {\\n+            if (LOG.isDebugEnabled()) {\\n+                LOG.debug(\"Loading SSLContext supplier from property \\'{}\\'\", sslContextSupplierClassProperty);\\n+            }\\n+            try {\\n+                Class<?> sslContextClass = Class.forName(supplierContextClassName);\\n+                Supplier<SSLContext> sslContextSupplier = (Supplier<SSLContext>) sslContextClass.getConstructor().newInstance();\\n+                return new SSLContextAndOptions(this, config, sslContextSupplier.get());\\n+            } catch (ClassNotFoundException | ClassCastException | NoSuchMethodException | InvocationTargetException |\\n+                    InstantiationException | IllegalAccessException e) {\\n+                throw new SSLContextException(\"Could not retrieve the SSLContext from supplier source \\'\" + supplierContextClassName +\\n+                        \"\\' provided in the property \\'\" + sslContextSupplierClassProperty + \"\\'\", e);\\n+            }\\n+        } else {\\n+            return createSSLContextAndOptionsFromConfig(config);\\n+        }\\n+    }\\n+\\n+    public SSLContextAndOptions createSSLContextAndOptionsFromConfig(ZKConfig config) throws SSLContextException {\\n         KeyManager[] keyManagers = null;\\n         TrustManager[] trustManagers = null;\\n \\ndiff --git a/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java b/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java\\nindex 43bc2d8e9..76bdd2e20 100644\\n--- a/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java\\n+++ b/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKConfig.java\\n@@ -133,6 +133,8 @@ private void putSSLProperties(X509Util x509Util) {\\n                 System.getProperty(x509Util.getSslTruststorePasswdProperty()));\\n         properties.put(x509Util.getSslTruststoreTypeProperty(),\\n                 System.getProperty(x509Util.getSslTruststoreTypeProperty()));\\n+        properties.put(x509Util.getSslContextSupplierClassProperty(),\\n+                System.getProperty(x509Util.getSslContextSupplierClassProperty()));\\n         properties.put(x509Util.getSslHostnameVerificationEnabledProperty(),\\n                 System.getProperty(x509Util.getSslHostnameVerificationEnabledProperty()));\\n         properties.put(x509Util.getSslCrlEnabledProperty(),\\ndiff --git a/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java b/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java\\nindex 2a6bb3246..1fecd808d 100644\\n--- a/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java\\n+++ b/zookeeper-server/src/test/java/org/apache/zookeeper/common/X509UtilTest.java\\n@@ -22,6 +22,7 @@\\n import java.net.InetSocketAddress;\\n import java.net.ServerSocket;\\n import java.net.Socket;\\n+import java.security.NoSuchAlgorithmException;\\n import java.security.Security;\\n import java.util.Collection;\\n import java.util.concurrent.Callable;\\n@@ -30,6 +31,7 @@\\n import java.util.concurrent.Executors;\\n import java.util.concurrent.Future;\\n import java.util.concurrent.atomic.AtomicInteger;\\n+import java.util.function.Supplier;\\n \\n import javax.net.ssl.HandshakeCompletedEvent;\\n import javax.net.ssl.HandshakeCompletedListener;\\n@@ -403,6 +405,23 @@ public void testGetSslHandshakeDetectionTimeoutMillisProperty() {\\n         }\\n     }\\n \\n+    @Test(expected = X509Exception.SSLContextException.class)\\n+    public void testCreateSSLContext_invalidCustomSSLContextClass() throws Exception {\\n+        ZKConfig zkConfig = new ZKConfig();\\n+        ClientX509Util clientX509Util = new ClientX509Util();\\n+        zkConfig.setProperty(clientX509Util.getSslContextSupplierClassProperty(), String.class.getCanonicalName());\\n+        clientX509Util.createSSLContext(zkConfig);\\n+    }\\n+\\n+    @Test\\n+    public void testCreateSSLContext_validCustomSSLContextClass() throws Exception {\\n+        ZKConfig zkConfig = new ZKConfig();\\n+        ClientX509Util clientX509Util = new ClientX509Util();\\n+        zkConfig.setProperty(clientX509Util.getSslContextSupplierClassProperty(), SslContextSupplier.class.getName());\\n+        final SSLContext sslContext = clientX509Util.createSSLContext(zkConfig);\\n+        Assert.assertEquals(SSLContext.getDefault(), sslContext);\\n+    }\\n+\\n     private static void forceClose(Socket s) {\\n         if (s == null || s.isClosed()) {\\n             return;\\n@@ -528,4 +547,18 @@ private void setCustomCipherSuites() {\\n         x509Util.close(); // remember to close old instance before replacing it\\n         x509Util = new ClientX509Util();\\n     }\\n+\\n+    public static class SslContextSupplier implements Supplier<SSLContext> {\\n+\\n+        @Override\\n+        public SSLContext get() {\\n+            try {\\n+                return SSLContext.getDefault();\\n+            } catch (NoSuchAlgorithmException e) {\\n+                throw new RuntimeException(e);\\n+            }\\n+        }\\n+\\n+    }\\n+\\n }\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: ZOOKEEPER-4753\\nIssue Summary: Explicit handling of DIGEST-MD5 vs GSSAPI in quorum auth\\nIssue Type: Improvement\\nPriority: Major\\n\\nDescription:\\nThe SASL-based quorum authorizer does not explicitly distinguish between the DIGEST-MD5 and GSSAPI mechanisms: it is simply relying on {{NameCallback}} and {{PasswordCallback}} for authentication with the former and examining Kerberos principals in {{AuthorizeCallback}} for the latter.\\n\\n\\n\\nIt turns out that some SASL/DIGEST-MD5 configurations cause authentication and authorization IDs not to match the expected format, and the DIGEST-MD5-based portions of the quorum test suite to fail with obscure errors. (They can be traced to failures to join the quorum, but only by looking into detailed logs.)\\n\\n\\n\\nWe can use the login module name to determine whether DIGEST-MD5 or GSSAPI is used, and relax the authentication ID check for the former.  As a cleanup, we can keep the password-based credential map empty when Kerberos principals are expected.  Finally, we can adapt tests to ensure \"weirdly-shaped\" credentials only cause authentication failures in the GSSAPI case.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68EEF110>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68EEF4D0>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68EEF570>\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': \"Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java b/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java\\nindex 52419ddf..b567f1ea 100644\\n--- a/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java\\n+++ b/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java\\n@@ -23,9 +23,18 @@ import opennlp.tools.util.Sequence;\\n /**\\n  * A thread-safe version of the POSTaggerME. Using it is completely transparent. You can use it in\\n  * a single-threaded context as well, it only incurs a minimal overhead.\\n+ * <p>\\n+ * Note, however, that this implementation uses a {@link ThreadLocal}. Although the implementation is\\n+ * lightweight because the model is not duplicated, if you have many long-running threads,\\n+ * you may run into memory problems.\\n+ * </p>\\n+ * <p>\\n+ * Be careful when using this in a Jakarta EE application, for example.\\n+ * </p>\\n+ * The user is responsible for clearing the {@link ThreadLocal}.\\n  */\\n @ThreadSafe\\n-public class ThreadSafePOSTaggerME implements POSTagger {\\n+public class ThreadSafePOSTaggerME implements POSTagger, AutoCloseable {\\n \\n   private final POSModel model;\\n \\n@@ -64,4 +73,9 @@ public class ThreadSafePOSTaggerME implements POSTagger {\\n   public Sequence[] topKSequences(String[] sentence, Object[] additionaContext) {\\n     return getTagger().topKSequences(sentence, additionaContext);\\n   }\\n+\\n+  @Override\\n+  public void close() {\\n+    threadLocal.remove();\\n+  }\\n }\\ndiff --git a/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java b/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java\\nindex 99abc6fb..17ea14e8 100644\\n--- a/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java\\n+++ b/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java\\n@@ -24,16 +24,21 @@ import opennlp.tools.util.Span;\\n  * A thread-safe version of SentenceDetectorME. Using it is completely transparent. You can use it in\\n  * a single-threaded context as well, it only incurs a minimal overhead.\\n  * <p>\\n- * Note, however, that this implementation uses a ThreadLocal. Although the implementation is\\n- * lightweight as the model is not duplicated, if you have many long-running threads, you may run\\n- * into memory issues. Be careful when you use this in a JEE application, for example.\\n+ * Note, however, that this implementation uses a {@link ThreadLocal}. Although the implementation is\\n+ * lightweight because the model is not duplicated, if you have many long-running threads,\\n+ * you may run into memory problems.\\n+ * </p>\\n+ * <p>\\n+ * Be careful when using this in a Jakarta EE application, for example.\\n+ * </p>\\n+ * The user is responsible for clearing the {@link ThreadLocal}.\\n  */\\n @ThreadSafe\\n-public class ThreadSafeSentenceDetectorME implements SentenceDetector {\\n+public class ThreadSafeSentenceDetectorME implements SentenceDetector, AutoCloseable {\\n \\n   private final SentenceModel model;\\n \\n-  private final ThreadLocal<SentenceDetectorME> sentenceDetectorThreadLocal =\\n+  private final ThreadLocal<SentenceDetectorME> threadLocal =\\n       new ThreadLocal<>();\\n \\n   public ThreadSafeSentenceDetectorME(SentenceModel model) {\\n@@ -43,10 +48,10 @@ public class ThreadSafeSentenceDetectorME implements SentenceDetector {\\n \\n   // If a thread-local version exists, return it. Otherwise, create, then return.\\n   private SentenceDetectorME getSD() {\\n-    SentenceDetectorME sd = sentenceDetectorThreadLocal.get();\\n+    SentenceDetectorME sd = threadLocal.get();\\n     if (sd == null) {\\n       sd = new SentenceDetectorME(model);\\n-      sentenceDetectorThreadLocal.set(sd);\\n+      threadLocal.set(sd);\\n     }\\n     return sd;\\n   }\\n@@ -64,4 +69,9 @@ public class ThreadSafeSentenceDetectorME implements SentenceDetector {\\n   public Span[] sentPosDetect(CharSequence s) {\\n     return getSD().sentPosDetect(s);\\n   }\\n+\\n+  @Override\\n+  public void close() {\\n+    threadLocal.remove();\\n+  }\\n }\\ndiff --git a/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java b/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java\\nindex b92dd5e0..3ebbd1e3 100644\\n--- a/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java\\n+++ b/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java\\n@@ -23,13 +23,22 @@ import opennlp.tools.util.Span;\\n /**\\n  * A thread-safe version of TokenizerME. Using it is completely transparent. You can use it in\\n  * a single-threaded context as well, it only incurs a minimal overhead.\\n+ * <p>\\n+ * Note, however, that this implementation uses a {@link ThreadLocal}. Although the implementation is\\n+ * lightweight because the model is not duplicated, if you have many long-running threads,\\n+ * you may run into memory problems.\\n+ * </p>\\n+ * <p>\\n+ * Be careful when using this in a Jakarta EE application, for example.\\n+ * </p>\\n+ * The user is responsible for clearing the {@link ThreadLocal}.\\n  */\\n @ThreadSafe\\n-public class ThreadSafeTokenizerME implements Tokenizer {\\n+public class ThreadSafeTokenizerME implements Tokenizer, AutoCloseable {\\n \\n   private final TokenizerModel model;\\n \\n-  private final ThreadLocal<TokenizerME> tokenizerThreadLocal = new ThreadLocal<>();\\n+  private final ThreadLocal<TokenizerME> threadLocal = new ThreadLocal<>();\\n \\n   public ThreadSafeTokenizerME(TokenizerModel model) {\\n     super();\\n@@ -37,10 +46,10 @@ public class ThreadSafeTokenizerME implements Tokenizer {\\n   }\\n \\n   private TokenizerME getTokenizer() {\\n-    TokenizerME tokenizer = tokenizerThreadLocal.get();\\n+    TokenizerME tokenizer = threadLocal.get();\\n     if (tokenizer == null) {\\n       tokenizer = new TokenizerME(model);\\n-      tokenizerThreadLocal.set(tokenizer);\\n+      threadLocal.set(tokenizer);\\n     }\\n     return tokenizer;\\n   }\\n@@ -58,4 +67,9 @@ public class ThreadSafeTokenizerME implements Tokenizer {\\n   public double[] getProbabilities() {\\n     return getTokenizer().getTokenProbabilities();\\n   }\\n+\\n+  @Override\\n+  public void close() {\\n+    threadLocal.remove();\\n+  }\\n }\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: OPENNLP-1667\\nIssue Summary: Add thread-safe version of ChunkerME\\nIssue Type: New Feature\\nPriority: Major\\n\\nDescription:\\nCurrently, ChunkerME is not thread-safe. With OPENNLP-936, a thread-safe version for several related classes was introduced.\\n\\n\\n\\nHowever, this was not done for the Chunker case.\\n\\nLet's introduce and provide ThreadSafeChunkerME.\\n>>>\\n> Relevant (YES / NO):\", 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java b/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java\\nindex 52419ddf..b567f1ea 100644\\n--- a/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java\\n+++ b/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java\\n@@ -23,9 +23,18 @@ import opennlp.tools.util.Sequence;\\n /**\\n  * A thread-safe version of the POSTaggerME. Using it is completely transparent. You can use it in\\n  * a single-threaded context as well, it only incurs a minimal overhead.\\n+ * <p>\\n+ * Note, however, that this implementation uses a {@link ThreadLocal}. Although the implementation is\\n+ * lightweight because the model is not duplicated, if you have many long-running threads,\\n+ * you may run into memory problems.\\n+ * </p>\\n+ * <p>\\n+ * Be careful when using this in a Jakarta EE application, for example.\\n+ * </p>\\n+ * The user is responsible for clearing the {@link ThreadLocal}.\\n  */\\n @ThreadSafe\\n-public class ThreadSafePOSTaggerME implements POSTagger {\\n+public class ThreadSafePOSTaggerME implements POSTagger, AutoCloseable {\\n \\n   private final POSModel model;\\n \\n@@ -64,4 +73,9 @@ public class ThreadSafePOSTaggerME implements POSTagger {\\n   public Sequence[] topKSequences(String[] sentence, Object[] additionaContext) {\\n     return getTagger().topKSequences(sentence, additionaContext);\\n   }\\n+\\n+  @Override\\n+  public void close() {\\n+    threadLocal.remove();\\n+  }\\n }\\ndiff --git a/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java b/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java\\nindex 99abc6fb..17ea14e8 100644\\n--- a/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java\\n+++ b/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java\\n@@ -24,16 +24,21 @@ import opennlp.tools.util.Span;\\n  * A thread-safe version of SentenceDetectorME. Using it is completely transparent. You can use it in\\n  * a single-threaded context as well, it only incurs a minimal overhead.\\n  * <p>\\n- * Note, however, that this implementation uses a ThreadLocal. Although the implementation is\\n- * lightweight as the model is not duplicated, if you have many long-running threads, you may run\\n- * into memory issues. Be careful when you use this in a JEE application, for example.\\n+ * Note, however, that this implementation uses a {@link ThreadLocal}. Although the implementation is\\n+ * lightweight because the model is not duplicated, if you have many long-running threads,\\n+ * you may run into memory problems.\\n+ * </p>\\n+ * <p>\\n+ * Be careful when using this in a Jakarta EE application, for example.\\n+ * </p>\\n+ * The user is responsible for clearing the {@link ThreadLocal}.\\n  */\\n @ThreadSafe\\n-public class ThreadSafeSentenceDetectorME implements SentenceDetector {\\n+public class ThreadSafeSentenceDetectorME implements SentenceDetector, AutoCloseable {\\n \\n   private final SentenceModel model;\\n \\n-  private final ThreadLocal<SentenceDetectorME> sentenceDetectorThreadLocal =\\n+  private final ThreadLocal<SentenceDetectorME> threadLocal =\\n       new ThreadLocal<>();\\n \\n   public ThreadSafeSentenceDetectorME(SentenceModel model) {\\n@@ -43,10 +48,10 @@ public class ThreadSafeSentenceDetectorME implements SentenceDetector {\\n \\n   // If a thread-local version exists, return it. Otherwise, create, then return.\\n   private SentenceDetectorME getSD() {\\n-    SentenceDetectorME sd = sentenceDetectorThreadLocal.get();\\n+    SentenceDetectorME sd = threadLocal.get();\\n     if (sd == null) {\\n       sd = new SentenceDetectorME(model);\\n-      sentenceDetectorThreadLocal.set(sd);\\n+      threadLocal.set(sd);\\n     }\\n     return sd;\\n   }\\n@@ -64,4 +69,9 @@ public class ThreadSafeSentenceDetectorME implements SentenceDetector {\\n   public Span[] sentPosDetect(CharSequence s) {\\n     return getSD().sentPosDetect(s);\\n   }\\n+\\n+  @Override\\n+  public void close() {\\n+    threadLocal.remove();\\n+  }\\n }\\ndiff --git a/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java b/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java\\nindex b92dd5e0..3ebbd1e3 100644\\n--- a/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java\\n+++ b/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java\\n@@ -23,13 +23,22 @@ import opennlp.tools.util.Span;\\n /**\\n  * A thread-safe version of TokenizerME. Using it is completely transparent. You can use it in\\n  * a single-threaded context as well, it only incurs a minimal overhead.\\n+ * <p>\\n+ * Note, however, that this implementation uses a {@link ThreadLocal}. Although the implementation is\\n+ * lightweight because the model is not duplicated, if you have many long-running threads,\\n+ * you may run into memory problems.\\n+ * </p>\\n+ * <p>\\n+ * Be careful when using this in a Jakarta EE application, for example.\\n+ * </p>\\n+ * The user is responsible for clearing the {@link ThreadLocal}.\\n  */\\n @ThreadSafe\\n-public class ThreadSafeTokenizerME implements Tokenizer {\\n+public class ThreadSafeTokenizerME implements Tokenizer, AutoCloseable {\\n \\n   private final TokenizerModel model;\\n \\n-  private final ThreadLocal<TokenizerME> tokenizerThreadLocal = new ThreadLocal<>();\\n+  private final ThreadLocal<TokenizerME> threadLocal = new ThreadLocal<>();\\n \\n   public ThreadSafeTokenizerME(TokenizerModel model) {\\n     super();\\n@@ -37,10 +46,10 @@ public class ThreadSafeTokenizerME implements Tokenizer {\\n   }\\n \\n   private TokenizerME getTokenizer() {\\n-    TokenizerME tokenizer = tokenizerThreadLocal.get();\\n+    TokenizerME tokenizer = threadLocal.get();\\n     if (tokenizer == null) {\\n       tokenizer = new TokenizerME(model);\\n-      tokenizerThreadLocal.set(tokenizer);\\n+      threadLocal.set(tokenizer);\\n     }\\n     return tokenizer;\\n   }\\n@@ -58,4 +67,9 @@ public class ThreadSafeTokenizerME implements Tokenizer {\\n   public double[] getProbabilities() {\\n     return getTokenizer().getTokenProbabilities();\\n   }\\n+\\n+  @Override\\n+  public void close() {\\n+    threadLocal.remove();\\n+  }\\n }\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: OPENNLP-1620\\nIssue Summary: It should be possible to remove the allocated ThreadLocal\\nIssue Type: Improvement\\nPriority: Major\\n\\nDescription:\\nIt should be possible to remove the allocated thread locals, if needed by the user as it is tied to the lifetime of the thread using it.\\n\\n\\n\\nÂ\\xa0\\n\\n\\n\\n!image-2024-10-08-11-55-15-901.png!\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': \"Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java b/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java\\nindex 52419ddf..b567f1ea 100644\\n--- a/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java\\n+++ b/opennlp-tools/src/main/java/opennlp/tools/postag/ThreadSafePOSTaggerME.java\\n@@ -23,9 +23,18 @@ import opennlp.tools.util.Sequence;\\n /**\\n  * A thread-safe version of the POSTaggerME. Using it is completely transparent. You can use it in\\n  * a single-threaded context as well, it only incurs a minimal overhead.\\n+ * <p>\\n+ * Note, however, that this implementation uses a {@link ThreadLocal}. Although the implementation is\\n+ * lightweight because the model is not duplicated, if you have many long-running threads,\\n+ * you may run into memory problems.\\n+ * </p>\\n+ * <p>\\n+ * Be careful when using this in a Jakarta EE application, for example.\\n+ * </p>\\n+ * The user is responsible for clearing the {@link ThreadLocal}.\\n  */\\n @ThreadSafe\\n-public class ThreadSafePOSTaggerME implements POSTagger {\\n+public class ThreadSafePOSTaggerME implements POSTagger, AutoCloseable {\\n \\n   private final POSModel model;\\n \\n@@ -64,4 +73,9 @@ public class ThreadSafePOSTaggerME implements POSTagger {\\n   public Sequence[] topKSequences(String[] sentence, Object[] additionaContext) {\\n     return getTagger().topKSequences(sentence, additionaContext);\\n   }\\n+\\n+  @Override\\n+  public void close() {\\n+    threadLocal.remove();\\n+  }\\n }\\ndiff --git a/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java b/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java\\nindex 99abc6fb..17ea14e8 100644\\n--- a/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java\\n+++ b/opennlp-tools/src/main/java/opennlp/tools/sentdetect/ThreadSafeSentenceDetectorME.java\\n@@ -24,16 +24,21 @@ import opennlp.tools.util.Span;\\n  * A thread-safe version of SentenceDetectorME. Using it is completely transparent. You can use it in\\n  * a single-threaded context as well, it only incurs a minimal overhead.\\n  * <p>\\n- * Note, however, that this implementation uses a ThreadLocal. Although the implementation is\\n- * lightweight as the model is not duplicated, if you have many long-running threads, you may run\\n- * into memory issues. Be careful when you use this in a JEE application, for example.\\n+ * Note, however, that this implementation uses a {@link ThreadLocal}. Although the implementation is\\n+ * lightweight because the model is not duplicated, if you have many long-running threads,\\n+ * you may run into memory problems.\\n+ * </p>\\n+ * <p>\\n+ * Be careful when using this in a Jakarta EE application, for example.\\n+ * </p>\\n+ * The user is responsible for clearing the {@link ThreadLocal}.\\n  */\\n @ThreadSafe\\n-public class ThreadSafeSentenceDetectorME implements SentenceDetector {\\n+public class ThreadSafeSentenceDetectorME implements SentenceDetector, AutoCloseable {\\n \\n   private final SentenceModel model;\\n \\n-  private final ThreadLocal<SentenceDetectorME> sentenceDetectorThreadLocal =\\n+  private final ThreadLocal<SentenceDetectorME> threadLocal =\\n       new ThreadLocal<>();\\n \\n   public ThreadSafeSentenceDetectorME(SentenceModel model) {\\n@@ -43,10 +48,10 @@ public class ThreadSafeSentenceDetectorME implements SentenceDetector {\\n \\n   // If a thread-local version exists, return it. Otherwise, create, then return.\\n   private SentenceDetectorME getSD() {\\n-    SentenceDetectorME sd = sentenceDetectorThreadLocal.get();\\n+    SentenceDetectorME sd = threadLocal.get();\\n     if (sd == null) {\\n       sd = new SentenceDetectorME(model);\\n-      sentenceDetectorThreadLocal.set(sd);\\n+      threadLocal.set(sd);\\n     }\\n     return sd;\\n   }\\n@@ -64,4 +69,9 @@ public class ThreadSafeSentenceDetectorME implements SentenceDetector {\\n   public Span[] sentPosDetect(CharSequence s) {\\n     return getSD().sentPosDetect(s);\\n   }\\n+\\n+  @Override\\n+  public void close() {\\n+    threadLocal.remove();\\n+  }\\n }\\ndiff --git a/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java b/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java\\nindex b92dd5e0..3ebbd1e3 100644\\n--- a/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java\\n+++ b/opennlp-tools/src/main/java/opennlp/tools/tokenize/ThreadSafeTokenizerME.java\\n@@ -23,13 +23,22 @@ import opennlp.tools.util.Span;\\n /**\\n  * A thread-safe version of TokenizerME. Using it is completely transparent. You can use it in\\n  * a single-threaded context as well, it only incurs a minimal overhead.\\n+ * <p>\\n+ * Note, however, that this implementation uses a {@link ThreadLocal}. Although the implementation is\\n+ * lightweight because the model is not duplicated, if you have many long-running threads,\\n+ * you may run into memory problems.\\n+ * </p>\\n+ * <p>\\n+ * Be careful when using this in a Jakarta EE application, for example.\\n+ * </p>\\n+ * The user is responsible for clearing the {@link ThreadLocal}.\\n  */\\n @ThreadSafe\\n-public class ThreadSafeTokenizerME implements Tokenizer {\\n+public class ThreadSafeTokenizerME implements Tokenizer, AutoCloseable {\\n \\n   private final TokenizerModel model;\\n \\n-  private final ThreadLocal<TokenizerME> tokenizerThreadLocal = new ThreadLocal<>();\\n+  private final ThreadLocal<TokenizerME> threadLocal = new ThreadLocal<>();\\n \\n   public ThreadSafeTokenizerME(TokenizerModel model) {\\n     super();\\n@@ -37,10 +46,10 @@ public class ThreadSafeTokenizerME implements Tokenizer {\\n   }\\n \\n   private TokenizerME getTokenizer() {\\n-    TokenizerME tokenizer = tokenizerThreadLocal.get();\\n+    TokenizerME tokenizer = threadLocal.get();\\n     if (tokenizer == null) {\\n       tokenizer = new TokenizerME(model);\\n-      tokenizerThreadLocal.set(tokenizer);\\n+      threadLocal.set(tokenizer);\\n     }\\n     return tokenizer;\\n   }\\n@@ -58,4 +67,9 @@ public class ThreadSafeTokenizerME implements Tokenizer {\\n   public double[] getProbabilities() {\\n     return getTokenizer().getTokenProbabilities();\\n   }\\n+\\n+  @Override\\n+  public void close() {\\n+    threadLocal.remove();\\n+  }\\n }\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: OPENNLP-1655\\nIssue Summary: Add constructors in SentenceDetectorME and TokenizerME to inject custom abbreviation dictionary\\nIssue Type: Improvement\\nPriority: Major\\n\\nDescription:\\nUsers ofÂ\\xa0{{TokenizerME}} and/or {{SentenceDetectorME}} may want to load an additional or custom {{Dictionary}} for abbreviations used in a certain language or domain.\\n\\n\\n\\nHowever, this is not possible right now, at construction time of those classes.\\n\\n\\n\\nLet's fix this by adding an additional constructor providing more flexibility.\\n>>>\\n> Relevant (YES / NO):\", 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68F30E10>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68F30F00>\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68F304B0>\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695DBF70>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688B3700>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A69606B70>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68F31630>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688EB4D0>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688EBDE0>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A69606210>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68EEF4D0>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68F31950>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A69605450>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A69607250>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68F30F00>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'user-yxw5duua92nmh24amrdiwkpm'), (b'openai-processing-ms', b'172'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-5456d57cf4-n46fp'), (b'x-envoy-upstream-service-time', b'95'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999952'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'req_2bdfa3432243a5389b20780b5d5f293d'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=QePk5x9qZfzVulm1tMKG130g4BIjmkSN7zCnwP3hlwg-1740654836-1.0.1.1-lkc_MLp_bW23GY.OL1avOvX8SN7wBCCyL.ZXbH_U_y0GOHut5sbF5W99Zy5j6uh_QpjqXGIhB6gk8AMkH3gyPg; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=OaoN28J6DHHAittC3S78wwrHTAhYrNCFdGahTSX4Hio-1740654836689-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf93cb484c11-SIN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Thu, 27 Feb 2025 11:13:56 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-yxw5duua92nmh24amrdiwkpm'), ('openai-processing-ms', '172'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-5456d57cf4-n46fp'), ('x-envoy-upstream-service-time', '95'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999952'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '2ms'), ('x-request-id', 'req_2bdfa3432243a5389b20780b5d5f293d'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=QePk5x9qZfzVulm1tMKG130g4BIjmkSN7zCnwP3hlwg-1740654836-1.0.1.1-lkc_MLp_bW23GY.OL1avOvX8SN7wBCCyL.ZXbH_U_y0GOHut5sbF5W99Zy5j6uh_QpjqXGIhB6gk8AMkH3gyPg; path=/; expires=Thu, 27-Feb-25 11:43:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=OaoN28J6DHHAittC3S78wwrHTAhYrNCFdGahTSX4Hio-1740654836689-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9187bf93cb484c11-SIN'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_2bdfa3432243a5389b20780b5d5f293d\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java b/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java\\nindex 6b96c07a966..aa502a2b47b 100644\\n--- a/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java\\n+++ b/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java\\n@@ -349,7 +349,9 @@ public class BeanExpression implements Expression, Predicate {\\n     private static Object invokeBean(BeanHolder beanHolder, String beanName, String methodName, Exchange exchange) {\\n         Object result;\\n \\n-        try (BeanExpressionProcessor processor = new BeanExpressionProcessor(beanHolder)) {\\n+        try {\\n+            // do not close BeanExpressionProcessor as beanHolder should not be closed\\n+            BeanExpressionProcessor processor = new BeanExpressionProcessor(beanHolder);\\n \\n             if (methodName != null) {\\n                 processor.setMethod(methodName);\\ndiff --git a/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java b/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java\\nindex fc076c25d4b..4cc2fc9c335 100644\\n--- a/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java\\n+++ b/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java\\n@@ -692,12 +692,12 @@ public class DefaultRoute extends ServiceSupport implements Route {\\n             services.add(service);\\n         }\\n         for (Processor p : onCompletions.values()) {\\n-            if (processor instanceof Service service) {\\n+            if (p instanceof Service service) {\\n                 services.add(service);\\n             }\\n         }\\n         for (Processor p : onExceptions.values()) {\\n-            if (processor instanceof Service service) {\\n+            if (p instanceof Service service) {\\n                 services.add(service);\\n             }\\n         }\\ndiff --git a/core/camel-core/src/test/java/org/apache/camel/processor/SupervisingRouteControllerSplitOnExceptionTest.java b/core/camel-core/src/test/java/org/apache/camel/processor/SupervisingRouteControllerSplitOnExceptionTest.java\\nnew file mode 100644\\nindex 00000000000..710f56f00d4\\n--- /dev/null\\n+++ b/core/camel-core/src/test/java/org/apache/camel/processor/SupervisingRouteControllerSplitOnExceptionTest.java\\n@@ -0,0 +1,87 @@\\n+/*\\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\\n+ * contributor license agreements.  See the NOTICE file distributed with\\n+ * this work for additional information regarding copyright ownership.\\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\\n+ * (the \"License\"); you may not use this file except in compliance with\\n+ * the License.  You may obtain a copy of the License at\\n+ *\\n+ *      http://www.apache.org/licenses/LICENSE-2.0\\n+ *\\n+ * Unless required by applicable law or agreed to in writing, software\\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+ * See the License for the specific language governing permissions and\\n+ * limitations under the License.\\n+ */\\n+package org.apache.camel.processor;\\n+\\n+import java.util.ArrayList;\\n+import java.util.List;\\n+\\n+import org.apache.camel.Body;\\n+import org.apache.camel.CamelContext;\\n+import org.apache.camel.ContextTestSupport;\\n+import org.apache.camel.Message;\\n+import org.apache.camel.RoutesBuilder;\\n+import org.apache.camel.builder.RouteBuilder;\\n+import org.apache.camel.spi.SupervisingRouteController;\\n+import org.junit.jupiter.api.Test;\\n+\\n+public class SupervisingRouteControllerSplitOnExceptionTest extends ContextTestSupport {\\n+\\n+    @Override\\n+    protected CamelContext createCamelContext() throws Exception {\\n+        CamelContext context = super.createCamelContext();\\n+\\n+        SupervisingRouteController src = context.getRouteController().supervising();\\n+        src.setBackOffDelay(25);\\n+        src.setBackOffMaxAttempts(3);\\n+        src.setInitialDelay(100);\\n+        src.setThreadPoolSize(1);\\n+\\n+        return context;\\n+    }\\n+\\n+    @Test\\n+    public void testSupervising() throws Exception {\\n+        getMockEndpoint(\"mock:error\").expectedMessageCount(1);\\n+        getMockEndpoint(\"mock:uk\").expectedMessageCount(0);\\n+        getMockEndpoint(\"mock:other\").expectedMessageCount(0);\\n+\\n+        template.sendBody(\"direct:start\", \"<hello>World\");\\n+\\n+        assertMockEndpointsSatisfied();\\n+    }\\n+\\n+    @Override\\n+    protected RoutesBuilder createRouteBuilder() throws Exception {\\n+        return new RouteBuilder() {\\n+            @Override\\n+            public void configure() throws Exception {\\n+                onException().handled(true).split().method(SupervisingRouteControllerSplitOnExceptionTest.class, \"mySplit\")\\n+                        .streaming().log(\"Exception occurred\").to(\"mock:error\");\\n+\\n+                from(\"direct:start\")\\n+                        .choice()\\n+                        .when(xpath(\"/person/city = \\'London\\'\"))\\n+                        .log(\"UK message\")\\n+                        .to(\"mock:uk\")\\n+                        .otherwise()\\n+                        .log(\"Other message\")\\n+                        .to(\"mock:other\");\\n+            }\\n+        };\\n+    }\\n+\\n+    public static List<Message> mySplit(@Body Message inputMessage) {\\n+        List<Message> outputMessages = new ArrayList<>();\\n+\\n+        Message outputMessage = inputMessage.copy();\\n+        outputMessage.setBody(inputMessage.getBody());\\n+        outputMessages.add(outputMessage);\\n+\\n+        return outputMessages;\\n+    }\\n+\\n+}\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: CAMEL-21775\\nIssue Summary: Split or Multicast in onException route cannot be started using Supervised route controller\\nIssue Type: Bug\\nPriority: Minor\\n\\nDescription:\\nUsing the supervised route controller,\\n\\n\\n\\nwhen a split is defined in a onException of a route, the route cannot\\xa0 be started.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nI suspect 2 issues:\\n\\n\\n\\n* One in the route initialization process:\\n\\n\\n\\n[https://github.com/apache/camel/blob/fa02c9e4879fac119a7f2b4f1b4ff46f646c48eb/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java#L700]\\n\\n\\n\\n\\xa0\\n\\n\\n\\n* One in the split expression evaluation. Because the try-with-resource closes the BeanProcessor Service and that processor will need to be reinitialized and will fail during initialization at MulticastProcessor.wrapInErrorHandler because the route does not contain exceptionHandlerFactory anymore.\\n\\n\\n\\n[https://github.com/apache/camel/blob/2192884d371377423dbada7f2870057cabdef8b4/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java#L352]\\n\\n\\n\\n\\xa0\\n\\n\\n\\nSee included reproducer.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A6959E580>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695DA5D0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68F32B20>\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java b/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java\\nindex 6b96c07a966..aa502a2b47b 100644\\n--- a/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java\\n+++ b/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java\\n@@ -349,7 +349,9 @@ public class BeanExpression implements Expression, Predicate {\\n     private static Object invokeBean(BeanHolder beanHolder, String beanName, String methodName, Exchange exchange) {\\n         Object result;\\n \\n-        try (BeanExpressionProcessor processor = new BeanExpressionProcessor(beanHolder)) {\\n+        try {\\n+            // do not close BeanExpressionProcessor as beanHolder should not be closed\\n+            BeanExpressionProcessor processor = new BeanExpressionProcessor(beanHolder);\\n \\n             if (methodName != null) {\\n                 processor.setMethod(methodName);\\ndiff --git a/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java b/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java\\nindex fc076c25d4b..4cc2fc9c335 100644\\n--- a/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java\\n+++ b/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java\\n@@ -692,12 +692,12 @@ public class DefaultRoute extends ServiceSupport implements Route {\\n             services.add(service);\\n         }\\n         for (Processor p : onCompletions.values()) {\\n-            if (processor instanceof Service service) {\\n+            if (p instanceof Service service) {\\n                 services.add(service);\\n             }\\n         }\\n         for (Processor p : onExceptions.values()) {\\n-            if (processor instanceof Service service) {\\n+            if (p instanceof Service service) {\\n                 services.add(service);\\n             }\\n         }\\ndiff --git a/core/camel-core/src/test/java/org/apache/camel/processor/SupervisingRouteControllerSplitOnExceptionTest.java b/core/camel-core/src/test/java/org/apache/camel/processor/SupervisingRouteControllerSplitOnExceptionTest.java\\nnew file mode 100644\\nindex 00000000000..710f56f00d4\\n--- /dev/null\\n+++ b/core/camel-core/src/test/java/org/apache/camel/processor/SupervisingRouteControllerSplitOnExceptionTest.java\\n@@ -0,0 +1,87 @@\\n+/*\\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\\n+ * contributor license agreements.  See the NOTICE file distributed with\\n+ * this work for additional information regarding copyright ownership.\\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\\n+ * (the \"License\"); you may not use this file except in compliance with\\n+ * the License.  You may obtain a copy of the License at\\n+ *\\n+ *      http://www.apache.org/licenses/LICENSE-2.0\\n+ *\\n+ * Unless required by applicable law or agreed to in writing, software\\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+ * See the License for the specific language governing permissions and\\n+ * limitations under the License.\\n+ */\\n+package org.apache.camel.processor;\\n+\\n+import java.util.ArrayList;\\n+import java.util.List;\\n+\\n+import org.apache.camel.Body;\\n+import org.apache.camel.CamelContext;\\n+import org.apache.camel.ContextTestSupport;\\n+import org.apache.camel.Message;\\n+import org.apache.camel.RoutesBuilder;\\n+import org.apache.camel.builder.RouteBuilder;\\n+import org.apache.camel.spi.SupervisingRouteController;\\n+import org.junit.jupiter.api.Test;\\n+\\n+public class SupervisingRouteControllerSplitOnExceptionTest extends ContextTestSupport {\\n+\\n+    @Override\\n+    protected CamelContext createCamelContext() throws Exception {\\n+        CamelContext context = super.createCamelContext();\\n+\\n+        SupervisingRouteController src = context.getRouteController().supervising();\\n+        src.setBackOffDelay(25);\\n+        src.setBackOffMaxAttempts(3);\\n+        src.setInitialDelay(100);\\n+        src.setThreadPoolSize(1);\\n+\\n+        return context;\\n+    }\\n+\\n+    @Test\\n+    public void testSupervising() throws Exception {\\n+        getMockEndpoint(\"mock:error\").expectedMessageCount(1);\\n+        getMockEndpoint(\"mock:uk\").expectedMessageCount(0);\\n+        getMockEndpoint(\"mock:other\").expectedMessageCount(0);\\n+\\n+        template.sendBody(\"direct:start\", \"<hello>World\");\\n+\\n+        assertMockEndpointsSatisfied();\\n+    }\\n+\\n+    @Override\\n+    protected RoutesBuilder createRouteBuilder() throws Exception {\\n+        return new RouteBuilder() {\\n+            @Override\\n+            public void configure() throws Exception {\\n+                onException().handled(true).split().method(SupervisingRouteControllerSplitOnExceptionTest.class, \"mySplit\")\\n+                        .streaming().log(\"Exception occurred\").to(\"mock:error\");\\n+\\n+                from(\"direct:start\")\\n+                        .choice()\\n+                        .when(xpath(\"/person/city = \\'London\\'\"))\\n+                        .log(\"UK message\")\\n+                        .to(\"mock:uk\")\\n+                        .otherwise()\\n+                        .log(\"Other message\")\\n+                        .to(\"mock:other\");\\n+            }\\n+        };\\n+    }\\n+\\n+    public static List<Message> mySplit(@Body Message inputMessage) {\\n+        List<Message> outputMessages = new ArrayList<>();\\n+\\n+        Message outputMessage = inputMessage.copy();\\n+        outputMessage.setBody(inputMessage.getBody());\\n+        outputMessages.add(outputMessage);\\n+\\n+        return outputMessages;\\n+    }\\n+\\n+}\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: CAMEL-21621\\nIssue Summary: camel-jbang - k8s-httpclient-vertx fails in shutdown hook\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\n{code}\\n\\nException in thread \"Camel Thread #7 - CamelShutdownInterceptor\" java.lang.RuntimeException: java.lang.IllegalStateException: Client is closed\\n\\n\\tat org.apache.camel.dsl.jbang.core.commands.kubernetes.KubernetesRun.lambda$installShutdownHook$10(KubernetesRun.java:514)\\n\\n\\tat java.base/java.lang.Thread.run(Thread.java:833)\\n\\nCaused by: java.lang.IllegalStateException: Client is closed\\n\\n\\tat io.vertx.core.http.impl.HttpClientImpl.checkClosed(HttpClientImpl.java:405)\\n\\n\\tat io.vertx.core.http.impl.HttpClientImpl.doRequest(HttpClientImpl.java:281)\\n\\n\\tat io.vertx.core.http.impl.HttpClientImpl.request(HttpClientImpl.java:191)\\n\\n\\tat io.fabric8.kubernetes.client.vertx.VertxHttpRequest.consumeBytes(VertxHttpRequest.java:124)\\n\\n{code}\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Evaluate the performance of a document retriever. Given the Git diff and retrieved context, return YES if the context is relevant with the changes in the Git diff. Otherwise, return NO.\\n\\nAvoid adding any additional comments or annotations to the classification. Return only YES or NO. Any additional words will break the system.\\n\\n> Git diff: \\n>>>\\ndiff --git a/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java b/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java\\nindex 6b96c07a966..aa502a2b47b 100644\\n--- a/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java\\n+++ b/components/camel-bean/src/main/java/org/apache/camel/language/bean/BeanExpression.java\\n@@ -349,7 +349,9 @@ public class BeanExpression implements Expression, Predicate {\\n     private static Object invokeBean(BeanHolder beanHolder, String beanName, String methodName, Exchange exchange) {\\n         Object result;\\n \\n-        try (BeanExpressionProcessor processor = new BeanExpressionProcessor(beanHolder)) {\\n+        try {\\n+            // do not close BeanExpressionProcessor as beanHolder should not be closed\\n+            BeanExpressionProcessor processor = new BeanExpressionProcessor(beanHolder);\\n \\n             if (methodName != null) {\\n                 processor.setMethod(methodName);\\ndiff --git a/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java b/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java\\nindex fc076c25d4b..4cc2fc9c335 100644\\n--- a/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java\\n+++ b/core/camel-base-engine/src/main/java/org/apache/camel/impl/engine/DefaultRoute.java\\n@@ -692,12 +692,12 @@ public class DefaultRoute extends ServiceSupport implements Route {\\n             services.add(service);\\n         }\\n         for (Processor p : onCompletions.values()) {\\n-            if (processor instanceof Service service) {\\n+            if (p instanceof Service service) {\\n                 services.add(service);\\n             }\\n         }\\n         for (Processor p : onExceptions.values()) {\\n-            if (processor instanceof Service service) {\\n+            if (p instanceof Service service) {\\n                 services.add(service);\\n             }\\n         }\\ndiff --git a/core/camel-core/src/test/java/org/apache/camel/processor/SupervisingRouteControllerSplitOnExceptionTest.java b/core/camel-core/src/test/java/org/apache/camel/processor/SupervisingRouteControllerSplitOnExceptionTest.java\\nnew file mode 100644\\nindex 00000000000..710f56f00d4\\n--- /dev/null\\n+++ b/core/camel-core/src/test/java/org/apache/camel/processor/SupervisingRouteControllerSplitOnExceptionTest.java\\n@@ -0,0 +1,87 @@\\n+/*\\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\\n+ * contributor license agreements.  See the NOTICE file distributed with\\n+ * this work for additional information regarding copyright ownership.\\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\\n+ * (the \"License\"); you may not use this file except in compliance with\\n+ * the License.  You may obtain a copy of the License at\\n+ *\\n+ *      http://www.apache.org/licenses/LICENSE-2.0\\n+ *\\n+ * Unless required by applicable law or agreed to in writing, software\\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+ * See the License for the specific language governing permissions and\\n+ * limitations under the License.\\n+ */\\n+package org.apache.camel.processor;\\n+\\n+import java.util.ArrayList;\\n+import java.util.List;\\n+\\n+import org.apache.camel.Body;\\n+import org.apache.camel.CamelContext;\\n+import org.apache.camel.ContextTestSupport;\\n+import org.apache.camel.Message;\\n+import org.apache.camel.RoutesBuilder;\\n+import org.apache.camel.builder.RouteBuilder;\\n+import org.apache.camel.spi.SupervisingRouteController;\\n+import org.junit.jupiter.api.Test;\\n+\\n+public class SupervisingRouteControllerSplitOnExceptionTest extends ContextTestSupport {\\n+\\n+    @Override\\n+    protected CamelContext createCamelContext() throws Exception {\\n+        CamelContext context = super.createCamelContext();\\n+\\n+        SupervisingRouteController src = context.getRouteController().supervising();\\n+        src.setBackOffDelay(25);\\n+        src.setBackOffMaxAttempts(3);\\n+        src.setInitialDelay(100);\\n+        src.setThreadPoolSize(1);\\n+\\n+        return context;\\n+    }\\n+\\n+    @Test\\n+    public void testSupervising() throws Exception {\\n+        getMockEndpoint(\"mock:error\").expectedMessageCount(1);\\n+        getMockEndpoint(\"mock:uk\").expectedMessageCount(0);\\n+        getMockEndpoint(\"mock:other\").expectedMessageCount(0);\\n+\\n+        template.sendBody(\"direct:start\", \"<hello>World\");\\n+\\n+        assertMockEndpointsSatisfied();\\n+    }\\n+\\n+    @Override\\n+    protected RoutesBuilder createRouteBuilder() throws Exception {\\n+        return new RouteBuilder() {\\n+            @Override\\n+            public void configure() throws Exception {\\n+                onException().handled(true).split().method(SupervisingRouteControllerSplitOnExceptionTest.class, \"mySplit\")\\n+                        .streaming().log(\"Exception occurred\").to(\"mock:error\");\\n+\\n+                from(\"direct:start\")\\n+                        .choice()\\n+                        .when(xpath(\"/person/city = \\'London\\'\"))\\n+                        .log(\"UK message\")\\n+                        .to(\"mock:uk\")\\n+                        .otherwise()\\n+                        .log(\"Other message\")\\n+                        .to(\"mock:other\");\\n+            }\\n+        };\\n+    }\\n+\\n+    public static List<Message> mySplit(@Body Message inputMessage) {\\n+        List<Message> outputMessages = new ArrayList<>();\\n+\\n+        Message outputMessage = inputMessage.copy();\\n+        outputMessage.setBody(inputMessage.getBody());\\n+        outputMessages.add(outputMessage);\\n+\\n+        return outputMessages;\\n+    }\\n+\\n+}\\n\\n>>>\\n> Retrieved context:\\n>>>\\nTicket ID: CAMEL-21614\\nIssue Summary: Simple expressions execute forever. Thread is RUNNABLE for ever. Issue appears with bean expressions inside simple expressions on SimpleLRUCache\\nIssue Type: Bug\\nPriority: Major\\n\\nDescription:\\nThis is a Follow-Up on this issue:\\xa0CAMEL-21467\\n\\n\\n\\nUnfortunately we are still experiencing the same issue with threads getting stuck in the state RUNNABLE. It seems like the frequency of the issue has greatly decreased though. We have had it twice in about a month.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nThreaddump:\\n\\n{noformat}\\n\\n\"Camel (camel-1) thread #4 - timer://my-timer\" - Thread t@65\\n\\n\\xa0 \\xa0java.lang.Thread.State: RUNNABLE\\n\\n\\xa0 \\xa0 at org.apache.camel.support.cache.SimpleLRUCache$OperationContext.close(SimpleLRUCache.java:309)\\n\\n\\xa0 \\xa0 at org.apache.camel.support.cache.SimpleLRUCache.put(SimpleLRUCache.java:104)\\n\\n\\xa0 \\xa0 at org.apache.camel.support.cache.SimpleSoftCache.put(SimpleSoftCache.java:114)\\n\\n\\xa0 \\xa0 at org.apache.camel.component.bean.BeanComponent.addBeanInfoToCache(BeanComponent.java:96)\\n\\n\\xa0 \\xa0 at org.apache.camel.component.bean.BeanInfo.<init>(BeanInfo.java:169)\\n\\n\\xa0 \\xa0 at org.apache.camel.component.bean.ConstantBeanHolder.<init>(ConstantBeanHolder.java:50)\\n\\n\\xa0 \\xa0 at org.apache.camel.language.bean.BeanExpression.createBeanHolder(BeanExpression.java:303)\\n\\n\\xa0 \\xa0 at org.apache.camel.language.bean.BeanExpression.init(BeanExpression.java:175)\\n\\n\\xa0 \\xa0 at org.apache.camel.language.bean.BeanLanguage.createExpression(BeanLanguage.java:148)\\n\\n\\xa0 \\xa0 at org.apache.camel.language.simple.SimpleExpressionBuilder$KeyedOgnlExpressionAdapter.evaluate(SimpleExpressionBuilder.java:1251)\\n\\n\\xa0 \\xa0 at org.apache.camel.support.ExpressionAdapter.evaluate(ExpressionAdapter.java:45)\\n\\n\\xa0 \\xa0 at org.apache.camel.support.builder.ExpressionBuilder$62.evaluate(ExpressionBuilder.java:2105)\\n\\n\\xa0 \\xa0 at org.apache.camel.support.ExpressionAdapter.evaluate(ExpressionAdapter.java:45)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.LogProcessor.process(LogProcessor.java:71)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.errorhandler.RedeliveryErrorHandler$RedeliveryTask.doRun(RedeliveryErrorHandler.java:808)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.errorhandler.RedeliveryErrorHandler$RedeliveryTask.run(RedeliveryErrorHandler.java:714)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor$Worker.executeFromQueue(DefaultReactiveExecutor.java:240)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor.executeFromQueue(DefaultReactiveExecutor.java:77)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.await(DefaultAsyncProcessorAwaitManager.java:95)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.process(DefaultAsyncProcessorAwaitManager.java:84)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.errorhandler.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:178)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.CamelInternalProcessor.processTransacted(CamelInternalProcessor.java:390)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.CamelInternalProcessor.process(CamelInternalProcessor.java:320)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.Pipeline$PipelineTask.run(Pipeline.java:102)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor$Worker.executeFromQueue(DefaultReactiveExecutor.java:240)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor.executeFromQueue(DefaultReactiveExecutor.java:77)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.await(DefaultAsyncProcessorAwaitManager.java:95)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.process(DefaultAsyncProcessorAwaitManager.java:84)\\n\\n\\xa0 \\xa0 at org.apache.camel.support.AsyncProcessorSupport.process(AsyncProcessorSupport.java:32)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.CamelInternalProcessor.processTransacted(CamelInternalProcessor.java:390)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.CamelInternalProcessor.process(CamelInternalProcessor.java:320)\\n\\n\\xa0 \\xa0 at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:102)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:208)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.errorhandler.RedeliveryErrorHandler$RedeliveryTask.doRun(RedeliveryErrorHandler.java:808)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.errorhandler.RedeliveryErrorHandler$RedeliveryTask.run(RedeliveryErrorHandler.java:714)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor$Worker.executeFromQueue(DefaultReactiveExecutor.java:240)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor.executeFromQueue(DefaultReactiveExecutor.java:77)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.await(DefaultAsyncProcessorAwaitManager.java:95)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.process(DefaultAsyncProcessorAwaitManager.java:84)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.errorhandler.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:178)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.CamelInternalProcessor.processTransacted(CamelInternalProcessor.java:390)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.CamelInternalProcessor.process(CamelInternalProcessor.java:320)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.Pipeline$PipelineTask.run(Pipeline.java:102)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor$Worker.executeFromQueue(DefaultReactiveExecutor.java:240)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor.executeFromQueue(DefaultReactiveExecutor.java:77)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.await(DefaultAsyncProcessorAwaitManager.java:95)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.process(DefaultAsyncProcessorAwaitManager.java:84)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.errorhandler.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:178)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.CamelInternalProcessor.processTransacted(CamelInternalProcessor.java:390)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.CamelInternalProcessor.process(CamelInternalProcessor.java:320)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.Pipeline$PipelineTask.run(Pipeline.java:102)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor$Worker.executeFromQueue(DefaultReactiveExecutor.java:240)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor.executeFromQueue(DefaultReactiveExecutor.java:77)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.await(DefaultAsyncProcessorAwaitManager.java:95)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.process(DefaultAsyncProcessorAwaitManager.java:84)\\n\\n\\xa0 \\xa0 at org.apache.camel.spring.spi.TransactionErrorHandler.processByErrorHandler(TransactionErrorHandler.java:244)\\n\\n\\xa0 \\xa0 at org.apache.camel.spring.spi.TransactionErrorHandler.process(TransactionErrorHandler.java:119)\\n\\n\\xa0 \\xa0 at org.apache.camel.spring.spi.TransactionErrorHandler.process(TransactionErrorHandler.java:132)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.errorhandler.RedeliveryErrorHandler$RedeliveryTask.doRun(RedeliveryErrorHandler.java:808)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.errorhandler.RedeliveryErrorHandler$RedeliveryTask.run(RedeliveryErrorHandler.java:714)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor$Worker.executeFromQueue(DefaultReactiveExecutor.java:240)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor.executeFromQueue(DefaultReactiveExecutor.java:77)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.await(DefaultAsyncProcessorAwaitManager.java:95)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.process(DefaultAsyncProcessorAwaitManager.java:84)\\n\\n\\xa0 \\xa0 at org.apache.camel.spring.spi.TransactionErrorHandler.processByErrorHandler(TransactionErrorHandler.java:244)\\n\\n\\xa0 \\xa0 at org.apache.camel.spring.spi.TransactionErrorHandler$1.doInTransactionWithoutResult(TransactionErrorHandler.java:207)\\n\\n\\xa0 \\xa0 at org.springframework.transaction.support.TransactionCallbackWithoutResult.doInTransaction(TransactionCallbackWithoutResult.java:36)\\n\\n\\xa0 \\xa0 at org.springframework.transaction.support.TransactionTemplate.execute(TransactionTemplate.java:140)\\n\\n\\xa0 \\xa0 at org.apache.camel.spring.spi.TransactionErrorHandler.doInTransactionTemplate(TransactionErrorHandler.java:200)\\n\\n\\xa0 \\xa0 at org.apache.camel.spring.spi.TransactionErrorHandler.processInTransaction(TransactionErrorHandler.java:155)\\n\\n\\xa0 \\xa0 at org.apache.camel.spring.spi.TransactionErrorHandler.process(TransactionErrorHandler.java:123)\\n\\n\\xa0 \\xa0 at org.apache.camel.spring.spi.TransactionErrorHandler.process(TransactionErrorHandler.java:132)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.CamelInternalProcessor.processNonTransacted(CamelInternalProcessor.java:347)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.CamelInternalProcessor.process(CamelInternalProcessor.java:323)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.Pipeline$PipelineTask.run(Pipeline.java:102)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor$Worker.doRun(DefaultReactiveExecutor.java:199)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor$Worker.executeReactiveWork(DefaultReactiveExecutor.java:189)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor$Worker.tryExecuteReactiveWork(DefaultReactiveExecutor.java:166)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor$Worker.schedule(DefaultReactiveExecutor.java:148)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.DefaultReactiveExecutor.scheduleMain(DefaultReactiveExecutor.java:59)\\n\\n\\xa0 \\xa0 at org.apache.camel.processor.Pipeline.process(Pipeline.java:163)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.CamelInternalProcessor.processNonTransacted(CamelInternalProcessor.java:347)\\n\\n\\xa0 \\xa0 at org.apache.camel.impl.engine.CamelInternalProcessor.process(CamelInternalProcessor.java:323)\\n\\n\\xa0 \\xa0 at org.apache.camel.component.timer.TimerConsumer.sendTimerExchange(TimerConsumer.java:293)\\n\\n\\xa0 \\xa0 at org.apache.camel.component.timer.TimerConsumer$1.doRun(TimerConsumer.java:164)\\n\\n\\xa0 \\xa0 at org.apache.camel.component.timer.TimerConsumer$1.run(TimerConsumer.java:136)\\n\\n\\xa0 \\xa0 at java.base@21.0.2/java.util.TimerThread.mainLoop(Timer.java:566)\\n\\n\\xa0 \\xa0 at java.base@21.0.2/java.util.TimerThread.run(Timer.java:516){noformat}\\n\\n\\n\\nThis is basically exactly the second case from my previous issue and seems to occur in this route:\\n\\n\\n\\n\\n\\n{code:java}\\n\\nfrom(direct(getDeliverRecipientRouteId()))\\n\\n        .routeId(getDeliverRecipientRouteId())\\n\\n        .startupOrder(1)\\n\\n        .policy(txJmsPropagationRequiresNew)\\n\\n        .choice()\\n\\n            .when(not(isPageable))\\n\\n                .log(LoggingLevel.INFO, log, \"--- Sending data message to queue=\" + getRecipientQueueName())\\n\\n            .when(currentStatusIs(StockRequestStatus.DATA))\\n\\n                .log(LoggingLevel.INFO, log, \"--- Sending data message for page=${exchangeProperty.\"\\n\\n                        + Exchange.LOOP_INDEX + \"} to queue=\" + getRecipientQueueName())\\n\\n            .otherwise()\\n\\n                .log(LoggingLevel.INFO, log, \"--- Sending control message for status=${exchangeProperty.\"\\n\\n                        + PROPERTY_REQUEST + \".status} to queue=\" + getRecipientQueueName())\\n\\n        .end()\\n\\n        .convertBodyTo(String.class)\\n\\n        .to(getJmsUriToRecipientQueue()); {code}\\n\\n\\n\\nRegarding [~davsclaus]\\'s question in the last issue (which apparently I have overseen):\\n\\n\\n\\nThe bean referenced via PROPERTY_REQUEST looks something like this:\\n\\n{code:java}\\n\\n@Data\\n\\n@RequiredArgsConstructor\\n\\npublic class MyRequest implements BaseRequest<Long, MyRequestDataType> {\\n\\n\\n\\n    public static final String COLUMN_FROM_REVISION = \"FROM_REVISION\";\\n\\n    public static final String COLUMN_TO_REVISION = \"TO_REVISION\";\\n\\n\\n\\n    private Long id;\\n\\n    private RequestStatus status;\\n\\n    private RequestDataType dataType;\\n\\n    private Instant timestamp;\\n\\n    private Long fromRevision;\\n\\n    private Long toRevision;\\n\\n    private String errorMessage;\\n\\n}{code}\\n\\n\\n\\n// the implemented interface\\n\\n{code:java}\\n\\npublic interface BaseRequest<ID, D extends RequestDataType> extends Serializable {\\n\\n\\n\\n    String COLUMN_ID = \"ID\";\\n\\n    String COLUMN_STATUS = \"STATUS\";\\n\\n    String COLUMN_DATATYPE = \"DATATYPE\";\\n\\n    String COLUMN_FROM = \"\\\\\"FROM\\\\\"\";\\n\\n    String COLUMN_TO = \"\\\\\"TO\\\\\"\";\\n\\n    String COLUMN_TIMESTAMP = \"\\\\\"TIMESTAMP\\\\\"\";\\n\\n    String COLUMN_ERROR_MESSAGE = \"ERROR_MESSAGE\";\\n\\n\\n\\n    ID getId();\\n\\n\\n\\n    void setId(ID id);\\n\\n\\n\\n    RequestStatus getStatus();\\n\\n\\n\\n    void setStatus(RequestStatus status);\\n\\n\\n\\n    D getDataType();\\n\\n\\n\\n    void setDataType(D dataType);\\n\\n\\n\\n    default Instant getFrom() {\\n\\n        return null;\\n\\n    }\\n\\n\\n\\n    default void setFrom(Instant from) {\\n\\n        // noop\\n\\n    }\\n\\n\\n\\n    default Instant getTo() {\\n\\n        return null;\\n\\n    }\\n\\n\\n\\n    default void setTo(Instant to) {\\n\\n        // noop\\n\\n    }\\n\\n\\n\\n    Instant getTimestamp();\\n\\n\\n\\n    void setTimestamp(Instant instant);\\n\\n\\n\\n    String getErrorMessage();\\n\\n\\n\\n    void setErrorMessage(String errorMessage);\\n\\n\\n\\n    default Map<String, Object> toMap(Exchange exchange) {\\n\\n        Map<String, Object> map = new LinkedHashMap<>();\\n\\n        map.put(\"id\", getId());\\n\\n        map.put(\"status\", getStatus());\\n\\n        map.put(\"dataType\", getDataType().getName());\\n\\n        if (getDataType().isDeltaDeliverable()) {\\n\\n            map.put(\"from\", getFrom().toString());\\n\\n            map.put(\"to\", getTo().toString());\\n\\n        }\\n\\n        map.put(\"timestamp\", getTimestamp().toString());\\n\\n        map.put(\"errorMessage\", getErrorMessage());\\n\\n        return map;\\n\\n    }\\n\\n} {code}\\n\\n\\n\\nAre the classes of the contained fields (RequestStatus, RequestDataType) also relevant for your question?\\n\\n\\n\\n\\xa0\\n\\n\\n\\nRegarding clear steps to reproduce, I am unsure whether I can provide. The issue appeared twice in a month within thousands of invocations of the timer route and the constellation causing it is not clear to me.\\n>>>\\n> Relevant (YES / NO):', 'role': 'user'}], 'model': 'qwen/qwen-2.5-coder-32b-instruct', 'max_tokens': 4096, 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68F32530>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68F33700>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A68F33BB0>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf97b85e3f74-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf97ccb2fdf3-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695D8370>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf97a89ff92e-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf97ee1244ab-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A695DBD90>\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688E8A00>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024A7FC01C70> server_hostname='openrouter.ai' timeout=5.0\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf987a28ff85-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A7FE893B0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf9878fb5f8a-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A688EB700>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024A69606CB0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf975ef5ce01-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf986b57ff8f-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf986c6dfd30-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf97598b48e2-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf9749639fbb-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf998bdf44b2-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf998af03d93-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf996b0f9fbb-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf997b88a12f-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf99f9c3ff86-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf99bc519d1d-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf9a2907fd3a-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf99fba2926c-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf99fc59ce6a-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf985f7e9fdd-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf986ab88369-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf9a2a92449d-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf987efe6b9e-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf986a24a8ee-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf9b3d24a3cd-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf99ba7f407c-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf9b3bd1fda8-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf9a084a9c9b-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf97ccb2fdf3-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 27 Feb 2025 11:13:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'x-clerk-auth-message', b'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)'), (b'x-clerk-auth-reason', b'token-invalid'), (b'x-clerk-auth-status', b'signed-out'), (b'Vary', b'Accept-Encoding'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9187bf9afb614092-SIN'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf97a89ff92e-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf975ef5ce01-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf99fba2926c-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf987a28ff85-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf998af03d93-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf986ab88369-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf99f9c3ff86-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf9a2907fd3a-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf985f7e9fdd-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf998bdf44b2-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf9a2a92449d-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf986c6dfd30-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf97ee1244ab-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf99bc519d1d-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf9a084a9c9b-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf9878fb5f8a-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf987efe6b9e-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf986b57ff8f-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf9749639fbb-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf996b0f9fbb-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf97598b48e2-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf99fc59ce6a-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf9b3d24a3cd-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf9afb614092-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf97b85e3f74-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf99ba7f407c-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf9b3bd1fda8-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf986a24a8ee-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions \"200 OK\" Headers({'date': 'Thu, 27 Feb 2025 11:13:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'x-clerk-auth-message': 'Invalid JWT form. A JWT consists of three parts separated by dots. (reason=token-invalid, token-carrier=header)', 'x-clerk-auth-reason': 'token-invalid', 'x-clerk-auth-status': 'signed-out', 'vary': 'Accept-Encoding', 'server': 'cloudflare', 'cf-ray': '9187bf997b88a12f-SIN', 'content-encoding': 'gzip'})\n",
      "DEBUG:openai._base_client:request_id: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"POST /runs/multipart HTTP/11\" 202 34\n"
     ]
    }
   ],
   "source": [
    "INCLUDED_HIGH_LEVEL_CONTEXT_CHAIN_INDICES = [0]\n",
    "\n",
    "for index in INCLUDED_HIGH_LEVEL_CONTEXT_CHAIN_INDICES:\n",
    "    evaluator.get_high_level_contexts(\n",
    "        HIGH_LEVEL_CONTEXT_CHAINS[index],\n",
    "        COMMITS, \n",
    "        CONTEXT_DATA_PATH, \n",
    "        HIGH_LEVEL_CONTEXT_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Commit Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDED_GENERATOR_INDICES = [1, 2, 3]\n",
    "INCLUDED_GENERATOR_INDICES = [2]\n",
    "\n",
    "filtered_generators = [GENERATORS[i] for i in INCLUDED_GENERATOR_INDICES]\n",
    "evaluator.evaluate(filtered_generators, COMMITS, CONTEXT_DATA_PATH, CMG_OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
