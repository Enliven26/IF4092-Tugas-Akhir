Ticket ID: KAFKA-18401
Issue Summary: Transaction version 2 does not support commit transaction without records
Issue Type: Bug
Priority: Blocker

Description:
This issue was observed when implementing https://issues.apache.org/jira/browse/KAFKA-18206.

In short, under transaction version 2, it doesn't support commit transaction without sending any records while transaction version 0 & 1 do support this kind of scenario.

Commit transactions without sending any records is fine when using transaction versions 0 or 1 because the producer won't send EndTxnRequest to the broker [0]. However, with transaction version 2, the producer still sends an EndTxnRequest to the broker while in transaction coordinator, the txn state is still in EMPTY, resulting in an error from the broker.

This issue can be reproduced with the test in below. I'm unsure if this behavior is expected. If it's not, one potential fix could be to follow the approach used in TV_0 and TV_1, where the EndTxnRequest is not sent if no partitions or offsets have been successfully added to the transaction. If this behavior is expected, we should document it and let user know this change.
{code:java}
    @ClusterTests({
        @ClusterTest(brokers = 3, features = {
            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 0)}),
        @ClusterTest(brokers = 3, features = {
            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 1)}),
        @ClusterTest(brokers = 3, features = {
            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 2)})
    })
    public void testProducerEndTransaction2(ClusterInstance cluster) throws InterruptedException {
        Map<String, Object> properties = new HashMap<>();
        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "foobar");
        properties.put(ProducerConfig.CLIENT_ID_CONFIG, "test");
        properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        try (Producer<byte[], byte[]> producer1 = cluster.producer(properties)) {

            producer1.initTransactions();
            producer1.beginTransaction();
            producer1.commitTransaction(); // In TV_2, we'll get InvalidTxnStateException
        }
    }
{code}
Another test case, which is essentially the same as the previous one, starts with a transaction that includes records, and then proceeds to start the next transaction. When using transaction version 2, we encounter an error, but this time it's a different error from the one seen in the previous case.
{code:java}
    @ClusterTests({
        @ClusterTest(brokers = 3, features = {
            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 0)}),
        @ClusterTest(brokers = 3, features = {
            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 1)}),
        @ClusterTest(brokers = 3, features = {
            @ClusterFeature(feature = Feature.TRANSACTION_VERSION, version = 2)})
    })
    public void testProducerEndTransaction(ClusterInstance cluster) {
        Map<String, Object> properties = new HashMap<>();
        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "foobar");
        properties.put(ProducerConfig.CLIENT_ID_CONFIG, "test");
        properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        try (Producer<byte[], byte[]> producer1 = cluster.producer(properties)) {

            producer1.initTransactions();
            producer1.beginTransaction();
            producer1.send(new ProducerRecord<>("test", "key".getBytes(), "value".getBytes()));
            producer1.commitTransaction();

            producer1.beginTransaction();
            producer1.commitTransaction(); // In TV_2, we'll get ProducerFencedException
        }
    }
{code}
 

[0]: [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L857-L865]--- RETRIEVED DOCUMENT SPLIT END ---Ticket ID: KAFKA-14020
Issue Summary: Performance regression in Producer
Issue Type: Bug
Priority: Blocker

Description:
[https://github.com/apache/kafka/commit/f7db6031b84a136ad0e257df722b20faa7c37b8a] introduced a 10% performance regression in the KafkaProducer under a default config.

 

The context for this result is a benchmark that we run for Kafka Streams. The benchmark provisions 5 independent AWS clusters, including one broker node on an i3.large and one client node on an i3.large. During a benchmark run, we first run the Producer for 10 minutes to generate test data, and then we run Kafka Streams under a number of configurations to measure its performance.

Our observation was a 10% regression in throughput under the simplest configuration, in which Streams simply consumes from a topic and does nothing else. That benchmark actually runs faster than the producer that generates the test data, so its thoughput is bounded by the data generator's throughput. After investigation, we realized that the regression was in the data generator, not the consumer or Streams.

We have numerous benchmark runs leading up to the commit in question, and they all show a throughput in the neighborhood of 115,000 records per second. We also have 40 runs including and after that commit, and they all show a throughput in the neighborhood of 105,000 records per second. A test on [trunk with the commit reverted |https://github.com/apache/kafka/pull/12342] shows a return to around 115,000 records per second.

Config:
{code:java}
final Properties properties = new Properties();
properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, broker);
properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
{code}
Here's the producer code in the data generator. Our tests were running with three produceThreads.
{code:java}
 for (int t = 0; t < produceThreads; t++) {
    futures.add(executorService.submit(() -> {
        int threadTotal = 0;
        long lastPrint = start;
        final long printInterval = Duration.ofSeconds(10).toMillis();
        long now;
        try (final org.apache.kafka.clients.producer.Producer<String, String> producer = new KafkaProducer<>(producerConfig(broker))) {
            while (limit > (now = System.currentTimeMillis()) - start) {
                for (int i = 0; i < 1000; i++) {
                    final String key = keys.next();
                    final String data = dataGen.generate();

                    producer.send(new ProducerRecord<>(topic, key, valueBuilder.apply(key, data)));

                    threadTotal++;
                }

                if ((now - lastPrint) > printInterval) {
                    System.out.println(Thread.currentThread().getName() + " produced " + numberFormat.format(threadTotal) + " to " + topic + " in " + Duration.ofMillis(now - start));
                    lastPrint = now;
                }
            }
        }
        total.addAndGet(threadTotal);
        System.out.println(Thread.currentThread().getName() + " finished (" + numberFormat.format(threadTotal) + ") in " + Duration.ofMillis(now - start));
    }));
}{code}
As you can see, this is a very basic usage.--- RETRIEVED DOCUMENT SPLIT END ---Ticket ID: KAFKA-17561
Issue Summary: Operator Metrics for Kafka Streams
Issue Type: Improvement
Priority: Major

Description:
This task for adding 3 new metrics to Kafka Streams 
 # client-state the current state of the Kafka Streams client
 # thread-state a client-level metric for the state of each `StreamThread`
 # recording-level a metric that reflects the current recording level of KafkaStreams metrics--- RETRIEVED DOCUMENT SPLIT END ---Ticket ID: KAFKA-18211
Issue Summary: ClassGraph scanning does not correctly find isolated connect plugins
Issue Type: Bug
Priority: Blocker

Description:
Connect used to use reflections scanner for scanning and identifying connect plugins in its plugin.path. This would load said plugins in isolation via the use of a child first PluginClassloader, which is designed to load class from its set of URIs before delegating to parent, if not found. This effectively enforces that if a plugin and its dependencies are part of a plugin path it would not conflict with other plugins in the plugin path or plugins in classpath. 

 

GlassGraph was introduced as a replacement for the older reflections scanner in [KAFKA-15203 Use Classgraph since org.reflections is no longer under maintainence by PARADOXST · Pull Request #16604 · apache/kafka|https://github.com/apache/kafka/pull/16604]. It is used in place of reflections scanner for finding plugins during plugin scanning. The issue here is that it is missing any plugins present in isolated plugin paths if its already present in classpath.  We can repro this by adding the json converter under an isolated plugin path and starting connect with debug logs. We can see the logs from ReflectionsScanner and find that the ClassGraph loader is always fetching the plugin from the classpath even though the PluginClassLoader is provided. This is causing [kafka/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/ReflectionScanner.java at 520681c38dbefe497181c4fd5dfc793d54233408 · apache/kafka|https://github.com/apache/kafka/blob/520681c38dbefe497181c4fd5dfc793d54233408/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/ReflectionScanner.java#L134] check to fail with the logs like. 
{code:java}
[2024-12-11 07:29:28,968] DEBUG class org.apache.kafka.connect.json.JsonConverter from other classloader jdk.internal.loader.ClassLoaders$AppClassLoader@c387f44 is visible from C:\Users\user\Desktop\confluent\testing\plugins3\connect-file-1.2.1-T-0.9.0-P-3.1, excluding to prevent isolated loading (org.apache.kafka.connect.runtime.isolation.ReflectionScanner:135)
[2024-12-11 07:29:28,969] DEBUG class org.apache.kafka.connect.json.JsonConverter from other classloader jdk.internal.loader.ClassLoaders$AppClassLoader@c387f44 is visible from C:\Users\user\Desktop\confluent\testing\plugins3\connect-file-1.2.1-T-0.9.0-P-3.1, excluding to prevent isolated loading (org.apache.kafka.connect.runtime.isolation.ReflectionScanner:135) {code}--- RETRIEVED DOCUMENT SPLIT END ---Ticket ID: KAFKA-9192
Issue Summary: NullPointerException if field in schema not present in value
Issue Type: Bug
Priority: Major

Description:
Given a message:
{code:java}
{
   "schema":{
      "type":"struct",
      "fields":[
         {
            "type":"string",
            "optional":true,
            "field":"abc"
         }
      ],
      "optional":false,
      "name":"foobar"
   },
   "payload":{
   }
}
{code}


I would expect, given the field is optional, for the JsonConverter to still process this value. 

What happens is I get a null pointer exception, the stacktrace points to this line: https://github.com/apache/kafka/blob/2.1/connect/json/src/main/java/org/apache/kafka/connect/json/JsonConverter.java#L701 called by https://github.com/apache/kafka/blob/2.1/connect/json/src/main/java/org/apache/kafka/connect/json/JsonConverter.java#L181

Issue seems to be that we need to check and see if the jsonValue is null before checking if the jsonValue has a null value.